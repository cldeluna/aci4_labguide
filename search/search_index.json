{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to CloudMyLab ACI 4 Lab Guide \u00b6 This is a companion guide to the CloudMyLab Cisco Data Center CCIE Rental Rack. This lab guide will go through a typical ACI deployment workflow.","title":"Home"},{"location":"#welcome-to-cloudmylab-aci-4-lab-guide","text":"This is a companion guide to the CloudMyLab Cisco Data Center CCIE Rental Rack. This lab guide will go through a typical ACI deployment workflow.","title":"Welcome to CloudMyLab ACI 4 Lab Guide"},{"location":"01_lab_fabric_discovery/","text":"Lab 01 - Fabric Inventory and Discovery \u00b6 Physical Build \u00b6 Its important to be familiar with the physical configuration of each node in your ACI fabric. A leaf, spine, or apic is a \"Node\" in your ACI fabric and the node numbering is important. In our lab we have a Nexus 9336 Spine, two Nexus 9396 Leafs, and a single APIC-Server-M1. Hardware Overview \u00b6 Model Name View Information N9K-C9336PQ Spine-1 Hardware Overview N9K-C9372PX-E LEAF-1 Hardware Overview N9K-C9372PX-E LEAF-2 Hardware Overview APIC-SERVER-M1 apic1 This configuration is perfectly valid for a Lab but it is not valid for a production environment. The minimum physical fabric hardware for a production environment includes two spines, two leafs, and three APICs. Fabric Turn Up \u00b6 It is important to know that the initial turn up and device discovery and registration has already ocurred. You are accessing the lab after this step has been completed. A fabric turn up is typically performed on site. Physical Connectivity \u00b6 The management network is up and configured All the management interfaces of the spines and leafs are connected to the management network Optionally, all the console interfaces of the spines and leafs are connected to a terminal server Each leaf switch has a fabric uplink to each spine Each APIC has a CIMC connection to the management network a Managmenet connection to the management network Redundant 10G fabric uplinks Note: Out of the box (before discoverying and registering the swith in ACI), you can connect to an ACI switch via the console port. In this state the password for the admin account is blank. Logical Parameters \u00b6 Paramenter Use Lab Value Pod Number Numeric identifier for each ACI Pod Default: 1 1 TEP Pool Default: 10.0.0.0/16 10.0.0.0/16 TEP Vlan Default: None Management Subnet/Mask Default: 192.168.10.0/24 192.168.10.0/24 Management Network Gateway Default: None 192.168.10.254/24 2019 Melbourne Cisco Live How to Setup an ACI Fabric from Scratch - BRKACI-2004 - 2019 Melbourne Cisco Live Explore the Cisco ACI GUI \u00b6 Now that you are familiar with the physical components of the Lab, lets investigate the APIC GUI and the topology from the APIC controller. Step 1 - Connect to Student PC \u00b6 Connect to your Student PC. See the Getting Around section for details. Step 2 - Login to the APIC \u00b6 From your Student PC, open a browser. Google Chrome is recommended for managing the APIC. https://192.168.10.1 or https://apic.dc.local Accept the security warning or create a security exception to access the GUI with the self signed certificate. Note that Secure HTTP (https) is required to access the APIC GUI by default. Insecure HTTP (http) must be explicitly enabled and is not recommended in a production environment. Login to the APIC. Note the warnings which will flash in the uppler right corner. You will see a Critical warning that the cluster does not contain 3 controllers. You may also see a Major warning regarding Licensing. This is expected in the Lab environment. Should you see these warning in a production environment, they must be corrected. You will see the \"What's New\" dialog and the main APIC Dashboard behding the dialog. Skim through the What's New dialog and close it. Step 3 - Areas of the APIC GUI \u00b6 Examine the the top-most section of the GUI interface. This top ribbon containing the main functional areas of the fabric (System, Tenants, Fabric, Virtual Networking, L4-L7 Services, Admin, Operations, Apps) is known as the Menu Bar . Menu Bar \u00b6 You will use it to navigate to the area of the ACI Fabric you need to view or update. Notice the shading and highlighting to help orient you in the GUI. The Menu bar shows that we are in the Dashboard section of the System menu. ACI GUI Menu Options Menu Headings/Tabs Description System Upon login, the GUI defaults to the System Menu Dashboard which provides the health status of the system. From the System menu tab other settings and licensing options are available along with events and faults. Tenants The Tenants Menu provides access to all tenants configured in the fabric and their logical configuration objects. Fabric The Fabric Menu provides access to inventory details, Fabric Policies, and Access Policies. Virtual Networking The Virtual Networking Menu displays and configures the fabric Virtual Machine Managers (VMMs). L4-L7 Services The L4-L7 Services Menu displays and configures the fabric Virtual Machine Managers (VMMs). Admin The Admin Menu displays and configures administrative functions such as authentication, authorization, and accounting functions, scheduling policies, retaining and purging records, upgrading firmware, and controlling features such as syslog, Call Home, and SNMP. Operations The Operations Menu provides access to operational functions including:<br>- Visibility & Troubleshooting<br>- Capacity Dashboard<br>- EP Tracker<br>- Visualiztion Apps The Apps tab displays all the applications installed or uploaded to APIC. The tab allows an APIC administrator to upload, enable, upgrade, install, or uninstall a packaged application in APIC. Areas of the APIC GUI \u00b6 The APIC or controller GUI has 4 main areas: - Menu Bar - Submenu Bar - Navigation Pane - Work Pane Select the Tenants menu. You will see a list of the default or pre-defined tenants which come with ACI \"out of the box\". Select the common tenant. You will see the standard tenants options listed collapsed in the Navigation Pane on the left side. If you select an option in the Navigation Pane, the objects pertaining to that selection are shows in the Work Pane to the right of the Navigation Pane. The Work Pane displays details about the option selected in the Navigation Pane. Fabric configuration via the GUI is typically performed in the Work Pane. Step 4 - Menu Bar and Navigation Conventions \u00b6 Take some time to select each Menu Bar option and get comfortable moving around in the GUI. For the remainder of the Lab the following convention will be used to guide the Student in navigating the GUI: Menu Bar Option > Submenu Option > Navigation Pane Option(s) > Work Pane Tabs From where you are in the Tenants menu navigate to: System > Dashboard to get back to the Health Dashboard of the Fabric. Notice that when you select the * System Menu option, you will automatically go to the Dashboard by default. GUI Tips \u00b6 Wherever there is a submit button and you are trying to make a change - click it. Some changes won\u2019t require it so the inconsistency sometimes calls that into question. Refresh - sometimes your changes won\u2019t appear until you do, you will see the little circular refresh button on most screens Hover over icons with your mouse for a few seconds to view the icon description Exploring Fabric Inventory, Nodes, and Fabric Topology \u00b6 Now that there is some familiarity with the GUI, lets validate the topology of the fabric. Step 1 - View and Explore the Toplogy \u00b6 Navigate to Fabric > Inventory > Topology . The Work Pane opens into the Summary tab. To view the topology diagram, click on the Topology tab in the Work Pane. Note that the full path would be shown as: Fabric > Inventory > Topology > Topology Navigation paths like this are not uncommon in the ACI GUI. Recall that the format we will follow throughout the lab is: Menu Bar Option > Submenu Option > Navigation Pane Option(s) > Work Pane Tabs From the Topology tab in the work pan verify that the displayed topology reflects the lab design. - One Spine - Two Leaf switches - One APIC server dual-homed to both leaf switches Note: You will see the same topology view if you go to Fabric > Inventory > Pod 1 > Topology Device Summary via hover over device icon \u00b6 Hover over each device icon for a very useful physical summary of the device. Device Connectivity \u00b6 Double click each device icon to view a list of connections. Step 2 - Fabric Membership \u00b6 Navigate to Fabric > Inventory > Fabric Membership . Here you will see the fabric inventory including serial number, Pod, Node ID, Model, Role, Fabric IP, and Status. The Pod, Node ID, and Role are defined during fabric discovery. You will notice that the IP comes from the TEP Pool that was provided during the apic intitial confirguration. Notice the additional tabs including Nodes Pending Registration . This tab is used to register new devices to the fabric. Double clicking on one of the device rows will display a dialog with device details. Examine each device and note the details that are available including certificate information. Step 3 - Pod View \u00b6 Navigate to Fabric > Inventory > Pod1 . Expand Pod1 by clicking on the \">\" symbol. Each device in the fabric will be listed. Expand one of the devices and review the sections available. Click on one of the devices (LEAF-1 is showd in the section below). The Summary tab for the device will appear in the Work Pane. Select the General tab in the Work Pane to view additional information about the device. Navigate to Fabric > Inventory > Pod1 > LEAF-1 (Node-102) > Interfaces > Physical Interfaces . Review the interfaces for the devince and note their operational status, Usage and other useful information. Step 4 - CLI \u00b6 Use PUTTY on the Student PC Desktop to connect to the APIC via SSH. Run the acidiag -h command to view the available ACI diagnotics options of the acidiag command. apic1# acidiag -h usage: acidiag [-h] [-v] {avread,fnvread,fnvreadex,rvread,rvreadle,crashsuspecttracker,bootother,bootcurr,journal,logs,oob,scheduler,cleanup,hwcheck,dbgtoken,validateimage,validatenginxconf,version,preservelogs,platform,verifyapic,bond0test,linkflap,touch,run,installer,start,stop,restart,dmestack,dmecore,reboot,drrmode,vapicjoin} ... positional arguments: {avread,fnvread,fnvreadex,rvread,rvreadle,crashsuspecttracker,bootother,bootcurr,journal,logs,oob,scheduler,cleanup,hwcheck,dbgtoken,validateimage,validatenginxconf,version,preservelogs,platform,verifyapic,bond0test,linkflap,touch,run,installer,start,stop,restart,dmestack,dmecore,reboot,drrmode,vapicjoin} sub-command help avread read appliance vector fnvread read fabric node vector fnvreadex read fabric node vector (extended mode) rvread read replica vector rvreadle read replica leader summary crashsuspecttracker read crash suspect tracker state bootother on next boot, boot other Linux Partition, and display updated /etc/grub.conf bootcurr on next boot, boot current Linux Partition, and display updated /etc/grub.conf journal Contents of journal logs logs show log history oob oob options cleanup fs cleanup utility hwcheck Quick check of APIC Hardware dbgtoken show debug token validateimage validate image validatenginxconf validate nginx conf version show ISO version preservelogs stash away logs in preparation for hard reboot platform show platform verifyapic run apic installation verify command bond0test ==SUPPRESS== linkflap flap a link touch touch special files run run specific commands and capture output installer installer start start a service stop stop a service restart restart a service reboot reboot drrmode drrmode options vapicjoin join existing vapic cluster optional arguments: -h, --help show this help message and exit -v, --verbose verbose apic1# Use acidiag fnvread CLI command to view the fabric devices (nodes). apic1# acidiag fnvread ID Pod ID Name Serial Number IP Address Role State LastUpdMsgId -------------------------------------------------------------------------------------------------------------- 101 1 Spine-1 SAL1948TWWP 10.0.72.97/32 spine active 0 102 1 LEAF-1 SAL1948U33K 10.0.72.98/32 leaf active 0 103 1 LEAF-2 SAL1948U35D 10.0.72.96/32 leaf active 0 Total 3 nodes Use the acidiag verifyapic CLI command to view the APIC status. apic1# acidiag verifyapic openssl_check: certificate details subject= CN=FCH1830V38S,serialNumber=PID:APIC-SERVER-M1 SN:FCH1830V38S issuer= CN=Cisco Manufacturing CA,O=Cisco Systems notBefore=Oct 11 08:42:21 2014 GMT notAfter=Oct 11 08:52:21 2024 GMT openssl_check: passed ssh_check: passed all_checks: passed apic1# Verify that you can enter configuration mode. apic1# config apic1(config)# exit apic1# config t apic1(config)# exit apic1# Verify that you can view the configuration using the usual show commands. apic1# sh run # Command: show running-config # Time: Sun May 31 21:56:08 2020 aaa banner 'Application Policy Infrastructure Controller' aaa authentication login console exit aaa authentication login default exit aaa authentication login domain fallback exit bgp-fabric exit coop-fabric exit no password pwd-strength-check crypto aes exit crypto webtoken session-record-flags login,logout,refresh exit rbac security-domain \"all\" exit rbac security-domain \"mgmt\" exit --More-- Because the APIC serves as the controller for the entire fabric, it is often simpler to log on to the APIC and execute show commands across the fabric. In some cases, you may want to log in to a particular leaf or spine. Node ID Management IP 101 192.168.10.101 102 192.168.10.102 103 192.168.10.103 Establish an SSH connection to each device in the table above and execute some common show commands. You can start with the \"show lldp neighbor\" command. Spine-1# show lldp nei Capability codes: (R) Router, (B) Bridge, (T) Telephone, (C) DOCSIS Cable Device (W) WLAN Access Point, (P) Repeater, (S) Station, (O) Other Device ID Local Intf Hold-time Capability Port ID LEAF-1 Eth1/1 120 BR Eth1/49 LEAF-2 Eth1/2 120 BR Eth1/49 Total entries displayed: 2 Spine-1# Spine-1# sh lldp nei sh: lldp: No such file or directory Spine-1# Note that the \"sh lldp nei\" command failed. Remember that many of the common abbreviations for commands are not accepted by ACI. CLI Tips \u00b6 Use or for command completion Not all command shortcuts are accepted in the ACI CLI. More and more are accepted with every new version of ACI but its a good idea to get into the habit of typing out the full commands. Many ways to interact with ACI \u00b6 As you are beginning to see, there is more than one way to interact with the APIC Controller. via the GUI via the CLI via the API ACI configuration, as you will see throughout these labs, within the GUI can be done in different ways as well. Configuration Wizards Configuration of objects individually and creating the relationships between objects in differing orders These labs will focus on individual object and relationship creation so that you have a fundamental understanding of the process. At that point, you can feel free to use Wizards, the CLI, or the API. You will see examples of all of these in these Labs. If a configuration setting is not specifically called out please leave the default values throughout these labs. Skills you should have after completing this lab \u00b6 After completing this labs you should: - be familiar with the hardware components of the lab - be familiar with the APIC GUI, its high level menu options, and how to navigate through the GUI expanding and selecting options - be able to explore the fabric inventory and determine model, status, and connectivity information - understand where to go to add new devices to the fabric - explore fabric inventory via the CLI Supplemental Information \u00b6 Configuring Out of Band Management \u00b6 This is not a lab but you can follow along. In Step 4 of Exploring the Fabric Inventory, you established an SSH connection to each leaf and spine and executed the \"show lldp command\". You were able to do this because the managment interfaces had already benn configured. This section details how that is done. Configuring out of band management for the fabric is only done once as part of fabric turn up. This activity effectivley configures an IP address on the device management interface so that the device is reachable via SSH. Because it is an essential step in the turn up of an ACI fabric, instructions are provided here for completeness but please do not perform these actions on your student fabric. Tenant mgmt \u00b6 Go to the Tenants menu and select the mgmt tenant from the subment or from the list in the expanded Work Pane. From the mgmt Tenant Navigation Pane, from the tenant navigation Pane navigate to Tenant mgmt > Node Management Addresses > Static Node Management Addresses Form here you have two options to get to the configuration dialog for a node management address. 1. Right click on the Static Node Management Addresses option in the Navigation Pane 2. Click on the tool icon drop down in the Work Pane. Both of these actions are equivalent and will present you with a dialog to create a static node management address. In ACI, this is the equivalent of assigning a management IP to the management port of the device. In ACI you often use the Node ID to identify the device and that is the case here. You can enter a range, as shown below with a starting IP in the management subnet and generally the IPs will be assigned sequentially. Note: The device hostname, credentials, and other settings are configured on the switch by the APIC when it is first discovered. While the range option is handy, there may be unintended consequences or if you have an IP addressing convention (Node ID mapped to last octet is a good practice) the assignments many not adhere to your convention. Its a good practice to do the IP assignment individually on each node. Note the range from and to values are for a single node. The Lab uses the default OOB (Out of Band) Management EPG, however in a production data center it is a good pracitce to configure and explicit management EPG.","title":"Lab 01 - Fabric Discovery"},{"location":"01_lab_fabric_discovery/#lab-01-fabric-inventory-and-discovery","text":"","title":"Lab 01 - Fabric Inventory and Discovery"},{"location":"01_lab_fabric_discovery/#physical-build","text":"Its important to be familiar with the physical configuration of each node in your ACI fabric. A leaf, spine, or apic is a \"Node\" in your ACI fabric and the node numbering is important. In our lab we have a Nexus 9336 Spine, two Nexus 9396 Leafs, and a single APIC-Server-M1.","title":"Physical Build"},{"location":"01_lab_fabric_discovery/#hardware-overview","text":"Model Name View Information N9K-C9336PQ Spine-1 Hardware Overview N9K-C9372PX-E LEAF-1 Hardware Overview N9K-C9372PX-E LEAF-2 Hardware Overview APIC-SERVER-M1 apic1 This configuration is perfectly valid for a Lab but it is not valid for a production environment. The minimum physical fabric hardware for a production environment includes two spines, two leafs, and three APICs.","title":"Hardware Overview"},{"location":"01_lab_fabric_discovery/#fabric-turn-up","text":"It is important to know that the initial turn up and device discovery and registration has already ocurred. You are accessing the lab after this step has been completed. A fabric turn up is typically performed on site.","title":"Fabric Turn Up"},{"location":"01_lab_fabric_discovery/#physical-connectivity","text":"The management network is up and configured All the management interfaces of the spines and leafs are connected to the management network Optionally, all the console interfaces of the spines and leafs are connected to a terminal server Each leaf switch has a fabric uplink to each spine Each APIC has a CIMC connection to the management network a Managmenet connection to the management network Redundant 10G fabric uplinks Note: Out of the box (before discoverying and registering the swith in ACI), you can connect to an ACI switch via the console port. In this state the password for the admin account is blank.","title":"Physical Connectivity"},{"location":"01_lab_fabric_discovery/#logical-parameters","text":"Paramenter Use Lab Value Pod Number Numeric identifier for each ACI Pod Default: 1 1 TEP Pool Default: 10.0.0.0/16 10.0.0.0/16 TEP Vlan Default: None Management Subnet/Mask Default: 192.168.10.0/24 192.168.10.0/24 Management Network Gateway Default: None 192.168.10.254/24 2019 Melbourne Cisco Live How to Setup an ACI Fabric from Scratch - BRKACI-2004 - 2019 Melbourne Cisco Live","title":"Logical Parameters"},{"location":"01_lab_fabric_discovery/#explore-the-cisco-aci-gui","text":"Now that you are familiar with the physical components of the Lab, lets investigate the APIC GUI and the topology from the APIC controller.","title":"Explore the Cisco ACI GUI"},{"location":"01_lab_fabric_discovery/#step-1-connect-to-student-pc","text":"Connect to your Student PC. See the Getting Around section for details.","title":"Step 1 - Connect to Student PC"},{"location":"01_lab_fabric_discovery/#step-2-login-to-the-apic","text":"From your Student PC, open a browser. Google Chrome is recommended for managing the APIC. https://192.168.10.1 or https://apic.dc.local Accept the security warning or create a security exception to access the GUI with the self signed certificate. Note that Secure HTTP (https) is required to access the APIC GUI by default. Insecure HTTP (http) must be explicitly enabled and is not recommended in a production environment. Login to the APIC. Note the warnings which will flash in the uppler right corner. You will see a Critical warning that the cluster does not contain 3 controllers. You may also see a Major warning regarding Licensing. This is expected in the Lab environment. Should you see these warning in a production environment, they must be corrected. You will see the \"What's New\" dialog and the main APIC Dashboard behding the dialog. Skim through the What's New dialog and close it.","title":"Step 2 - Login to the APIC"},{"location":"01_lab_fabric_discovery/#step-3-areas-of-the-apic-gui","text":"Examine the the top-most section of the GUI interface. This top ribbon containing the main functional areas of the fabric (System, Tenants, Fabric, Virtual Networking, L4-L7 Services, Admin, Operations, Apps) is known as the Menu Bar .","title":"Step 3 - Areas of the APIC GUI"},{"location":"01_lab_fabric_discovery/#menu-bar","text":"You will use it to navigate to the area of the ACI Fabric you need to view or update. Notice the shading and highlighting to help orient you in the GUI. The Menu bar shows that we are in the Dashboard section of the System menu. ACI GUI Menu Options Menu Headings/Tabs Description System Upon login, the GUI defaults to the System Menu Dashboard which provides the health status of the system. From the System menu tab other settings and licensing options are available along with events and faults. Tenants The Tenants Menu provides access to all tenants configured in the fabric and their logical configuration objects. Fabric The Fabric Menu provides access to inventory details, Fabric Policies, and Access Policies. Virtual Networking The Virtual Networking Menu displays and configures the fabric Virtual Machine Managers (VMMs). L4-L7 Services The L4-L7 Services Menu displays and configures the fabric Virtual Machine Managers (VMMs). Admin The Admin Menu displays and configures administrative functions such as authentication, authorization, and accounting functions, scheduling policies, retaining and purging records, upgrading firmware, and controlling features such as syslog, Call Home, and SNMP. Operations The Operations Menu provides access to operational functions including:<br>- Visibility & Troubleshooting<br>- Capacity Dashboard<br>- EP Tracker<br>- Visualiztion Apps The Apps tab displays all the applications installed or uploaded to APIC. The tab allows an APIC administrator to upload, enable, upgrade, install, or uninstall a packaged application in APIC.","title":"Menu Bar"},{"location":"01_lab_fabric_discovery/#areas-of-the-apic-gui","text":"The APIC or controller GUI has 4 main areas: - Menu Bar - Submenu Bar - Navigation Pane - Work Pane Select the Tenants menu. You will see a list of the default or pre-defined tenants which come with ACI \"out of the box\". Select the common tenant. You will see the standard tenants options listed collapsed in the Navigation Pane on the left side. If you select an option in the Navigation Pane, the objects pertaining to that selection are shows in the Work Pane to the right of the Navigation Pane. The Work Pane displays details about the option selected in the Navigation Pane. Fabric configuration via the GUI is typically performed in the Work Pane.","title":"Areas of the APIC GUI"},{"location":"01_lab_fabric_discovery/#step-4-menu-bar-and-navigation-conventions","text":"Take some time to select each Menu Bar option and get comfortable moving around in the GUI. For the remainder of the Lab the following convention will be used to guide the Student in navigating the GUI: Menu Bar Option > Submenu Option > Navigation Pane Option(s) > Work Pane Tabs From where you are in the Tenants menu navigate to: System > Dashboard to get back to the Health Dashboard of the Fabric. Notice that when you select the * System Menu option, you will automatically go to the Dashboard by default.","title":"Step 4 - Menu Bar and Navigation Conventions"},{"location":"01_lab_fabric_discovery/#gui-tips","text":"Wherever there is a submit button and you are trying to make a change - click it. Some changes won\u2019t require it so the inconsistency sometimes calls that into question. Refresh - sometimes your changes won\u2019t appear until you do, you will see the little circular refresh button on most screens Hover over icons with your mouse for a few seconds to view the icon description","title":"GUI Tips"},{"location":"01_lab_fabric_discovery/#exploring-fabric-inventory-nodes-and-fabric-topology","text":"Now that there is some familiarity with the GUI, lets validate the topology of the fabric.","title":"Exploring Fabric Inventory, Nodes, and Fabric Topology"},{"location":"01_lab_fabric_discovery/#step-1-view-and-explore-the-toplogy","text":"Navigate to Fabric > Inventory > Topology . The Work Pane opens into the Summary tab. To view the topology diagram, click on the Topology tab in the Work Pane. Note that the full path would be shown as: Fabric > Inventory > Topology > Topology Navigation paths like this are not uncommon in the ACI GUI. Recall that the format we will follow throughout the lab is: Menu Bar Option > Submenu Option > Navigation Pane Option(s) > Work Pane Tabs From the Topology tab in the work pan verify that the displayed topology reflects the lab design. - One Spine - Two Leaf switches - One APIC server dual-homed to both leaf switches Note: You will see the same topology view if you go to Fabric > Inventory > Pod 1 > Topology","title":"Step 1 - View and Explore the Toplogy"},{"location":"01_lab_fabric_discovery/#device-summary-via-hover-over-device-icon","text":"Hover over each device icon for a very useful physical summary of the device.","title":"Device Summary via hover over device icon"},{"location":"01_lab_fabric_discovery/#device-connectivity","text":"Double click each device icon to view a list of connections.","title":"Device Connectivity"},{"location":"01_lab_fabric_discovery/#step-2-fabric-membership","text":"Navigate to Fabric > Inventory > Fabric Membership . Here you will see the fabric inventory including serial number, Pod, Node ID, Model, Role, Fabric IP, and Status. The Pod, Node ID, and Role are defined during fabric discovery. You will notice that the IP comes from the TEP Pool that was provided during the apic intitial confirguration. Notice the additional tabs including Nodes Pending Registration . This tab is used to register new devices to the fabric. Double clicking on one of the device rows will display a dialog with device details. Examine each device and note the details that are available including certificate information.","title":"Step 2 - Fabric Membership"},{"location":"01_lab_fabric_discovery/#step-3-pod-view","text":"Navigate to Fabric > Inventory > Pod1 . Expand Pod1 by clicking on the \">\" symbol. Each device in the fabric will be listed. Expand one of the devices and review the sections available. Click on one of the devices (LEAF-1 is showd in the section below). The Summary tab for the device will appear in the Work Pane. Select the General tab in the Work Pane to view additional information about the device. Navigate to Fabric > Inventory > Pod1 > LEAF-1 (Node-102) > Interfaces > Physical Interfaces . Review the interfaces for the devince and note their operational status, Usage and other useful information.","title":"Step 3 - Pod View"},{"location":"01_lab_fabric_discovery/#step-4-cli","text":"Use PUTTY on the Student PC Desktop to connect to the APIC via SSH. Run the acidiag -h command to view the available ACI diagnotics options of the acidiag command. apic1# acidiag -h usage: acidiag [-h] [-v] {avread,fnvread,fnvreadex,rvread,rvreadle,crashsuspecttracker,bootother,bootcurr,journal,logs,oob,scheduler,cleanup,hwcheck,dbgtoken,validateimage,validatenginxconf,version,preservelogs,platform,verifyapic,bond0test,linkflap,touch,run,installer,start,stop,restart,dmestack,dmecore,reboot,drrmode,vapicjoin} ... positional arguments: {avread,fnvread,fnvreadex,rvread,rvreadle,crashsuspecttracker,bootother,bootcurr,journal,logs,oob,scheduler,cleanup,hwcheck,dbgtoken,validateimage,validatenginxconf,version,preservelogs,platform,verifyapic,bond0test,linkflap,touch,run,installer,start,stop,restart,dmestack,dmecore,reboot,drrmode,vapicjoin} sub-command help avread read appliance vector fnvread read fabric node vector fnvreadex read fabric node vector (extended mode) rvread read replica vector rvreadle read replica leader summary crashsuspecttracker read crash suspect tracker state bootother on next boot, boot other Linux Partition, and display updated /etc/grub.conf bootcurr on next boot, boot current Linux Partition, and display updated /etc/grub.conf journal Contents of journal logs logs show log history oob oob options cleanup fs cleanup utility hwcheck Quick check of APIC Hardware dbgtoken show debug token validateimage validate image validatenginxconf validate nginx conf version show ISO version preservelogs stash away logs in preparation for hard reboot platform show platform verifyapic run apic installation verify command bond0test ==SUPPRESS== linkflap flap a link touch touch special files run run specific commands and capture output installer installer start start a service stop stop a service restart restart a service reboot reboot drrmode drrmode options vapicjoin join existing vapic cluster optional arguments: -h, --help show this help message and exit -v, --verbose verbose apic1# Use acidiag fnvread CLI command to view the fabric devices (nodes). apic1# acidiag fnvread ID Pod ID Name Serial Number IP Address Role State LastUpdMsgId -------------------------------------------------------------------------------------------------------------- 101 1 Spine-1 SAL1948TWWP 10.0.72.97/32 spine active 0 102 1 LEAF-1 SAL1948U33K 10.0.72.98/32 leaf active 0 103 1 LEAF-2 SAL1948U35D 10.0.72.96/32 leaf active 0 Total 3 nodes Use the acidiag verifyapic CLI command to view the APIC status. apic1# acidiag verifyapic openssl_check: certificate details subject= CN=FCH1830V38S,serialNumber=PID:APIC-SERVER-M1 SN:FCH1830V38S issuer= CN=Cisco Manufacturing CA,O=Cisco Systems notBefore=Oct 11 08:42:21 2014 GMT notAfter=Oct 11 08:52:21 2024 GMT openssl_check: passed ssh_check: passed all_checks: passed apic1# Verify that you can enter configuration mode. apic1# config apic1(config)# exit apic1# config t apic1(config)# exit apic1# Verify that you can view the configuration using the usual show commands. apic1# sh run # Command: show running-config # Time: Sun May 31 21:56:08 2020 aaa banner 'Application Policy Infrastructure Controller' aaa authentication login console exit aaa authentication login default exit aaa authentication login domain fallback exit bgp-fabric exit coop-fabric exit no password pwd-strength-check crypto aes exit crypto webtoken session-record-flags login,logout,refresh exit rbac security-domain \"all\" exit rbac security-domain \"mgmt\" exit --More-- Because the APIC serves as the controller for the entire fabric, it is often simpler to log on to the APIC and execute show commands across the fabric. In some cases, you may want to log in to a particular leaf or spine. Node ID Management IP 101 192.168.10.101 102 192.168.10.102 103 192.168.10.103 Establish an SSH connection to each device in the table above and execute some common show commands. You can start with the \"show lldp neighbor\" command. Spine-1# show lldp nei Capability codes: (R) Router, (B) Bridge, (T) Telephone, (C) DOCSIS Cable Device (W) WLAN Access Point, (P) Repeater, (S) Station, (O) Other Device ID Local Intf Hold-time Capability Port ID LEAF-1 Eth1/1 120 BR Eth1/49 LEAF-2 Eth1/2 120 BR Eth1/49 Total entries displayed: 2 Spine-1# Spine-1# sh lldp nei sh: lldp: No such file or directory Spine-1# Note that the \"sh lldp nei\" command failed. Remember that many of the common abbreviations for commands are not accepted by ACI.","title":"Step 4 - CLI"},{"location":"01_lab_fabric_discovery/#cli-tips","text":"Use or for command completion Not all command shortcuts are accepted in the ACI CLI. More and more are accepted with every new version of ACI but its a good idea to get into the habit of typing out the full commands.","title":"CLI Tips"},{"location":"01_lab_fabric_discovery/#many-ways-to-interact-with-aci","text":"As you are beginning to see, there is more than one way to interact with the APIC Controller. via the GUI via the CLI via the API ACI configuration, as you will see throughout these labs, within the GUI can be done in different ways as well. Configuration Wizards Configuration of objects individually and creating the relationships between objects in differing orders These labs will focus on individual object and relationship creation so that you have a fundamental understanding of the process. At that point, you can feel free to use Wizards, the CLI, or the API. You will see examples of all of these in these Labs. If a configuration setting is not specifically called out please leave the default values throughout these labs.","title":"Many ways to interact with ACI"},{"location":"01_lab_fabric_discovery/#skills-you-should-have-after-completing-this-lab","text":"After completing this labs you should: - be familiar with the hardware components of the lab - be familiar with the APIC GUI, its high level menu options, and how to navigate through the GUI expanding and selecting options - be able to explore the fabric inventory and determine model, status, and connectivity information - understand where to go to add new devices to the fabric - explore fabric inventory via the CLI","title":"Skills you should have after completing this lab"},{"location":"01_lab_fabric_discovery/#supplemental-information","text":"","title":"Supplemental Information"},{"location":"01_lab_fabric_discovery/#configuring-out-of-band-management","text":"This is not a lab but you can follow along. In Step 4 of Exploring the Fabric Inventory, you established an SSH connection to each leaf and spine and executed the \"show lldp command\". You were able to do this because the managment interfaces had already benn configured. This section details how that is done. Configuring out of band management for the fabric is only done once as part of fabric turn up. This activity effectivley configures an IP address on the device management interface so that the device is reachable via SSH. Because it is an essential step in the turn up of an ACI fabric, instructions are provided here for completeness but please do not perform these actions on your student fabric.","title":"Configuring Out of Band Management"},{"location":"01_lab_fabric_discovery/#tenant-mgmt","text":"Go to the Tenants menu and select the mgmt tenant from the subment or from the list in the expanded Work Pane. From the mgmt Tenant Navigation Pane, from the tenant navigation Pane navigate to Tenant mgmt > Node Management Addresses > Static Node Management Addresses Form here you have two options to get to the configuration dialog for a node management address. 1. Right click on the Static Node Management Addresses option in the Navigation Pane 2. Click on the tool icon drop down in the Work Pane. Both of these actions are equivalent and will present you with a dialog to create a static node management address. In ACI, this is the equivalent of assigning a management IP to the management port of the device. In ACI you often use the Node ID to identify the device and that is the case here. You can enter a range, as shown below with a starting IP in the management subnet and generally the IPs will be assigned sequentially. Note: The device hostname, credentials, and other settings are configured on the switch by the APIC when it is first discovered. While the range option is handy, there may be unintended consequences or if you have an IP addressing convention (Node ID mapped to last octet is a good practice) the assignments many not adhere to your convention. Its a good practice to do the IP assignment individually on each node. Note the range from and to values are for a single node. The Lab uses the default OOB (Out of Band) Management EPG, however in a production data center it is a good pracitce to configure and explicit management EPG.","title":"Tenant mgmt"},{"location":"02_lab_fabric_access_policies/","text":"Lab 02 - Configure Fabric & Access Constructs \u00b6 Context \u00b6 This lab represents the implementation of networking at the physical layer across the entire fabric as well as on a switch and interface or port level. Typically in an ACI fabric, the Fabric Policies are configured during initial fabric turn up and then largely left alone. The Access Policies represetn that \"day to day\" configuration of the fabric as they define how the fabric connects to hosts and other non-ACI network devices. You an think of Access Policies as the hardware configuration of interfaces. The logical portion of the configuration is implemented when Access Policies are associated with EPGs and \"Vlans\" or Encapsulations. Lab Goals \u00b6 In this lab we will review how to configure basic fabric functionality via Fabric Policies . These are configurations steps which are peformed at the fabric level and which have already been configured for the lab but the guide will detail the steps required to complete this step. We will then look at Access Policies which configure the physical (vs logical) aspects of fabric interfaces which connect the leafs (and spines) to compute, storage, appliances, and other non-ACI network devices. Fabric Configuration \u00b6 Configuration in ACI is largely based on policies which abstract the characteristics of traditional network connectivity. Consider the diagram below and the steps you normally take to configure a layer 2 trunk interface like the one shows on LEAF-2 e1/11. In the classical Ethernet environment this configuration is done on LEAF-2 itself so the switch is understood. The configuration is performed on the interface itself, e1/11 in this case, so that is understood. Because these objects are abstract and all of the configuration is performed by the APIC controller, each of those \"undersood\" items are configued discreetly via policy and then put together to achieve the desired functionality. We will go through that workflow in this lab. Initially it may seem like \"extra\" work to do this but once you start taking advantage of the re-usability of these policy objects and see the ease of scalability, the \"extra\" work will be well worth it. Implementing Cisco ACI Fabric Connectivity \u00b6 The Fabric menu has three submenu items. You should be familiar with the Inventory submenu from Lab 01. The other two Submenu items configure the fabric itself and the physical connectivity. Configuring ACI connectivity is generally done in two steps: 1. Configure the Physical Layer Characteristics 2. Configure the Logical Layer Characteristics Fabric Policies \u00b6 Fabric policies control the configuration of the fabric itself. The most common activity in this section is to define Policy Groups such as the fabrics: Data and Time policy (NTP) Management Access policy SNMP Policy (for the fabric) BGP Route Reflector Policy BGP Route Reflector Policy \u00b6 The BGP Route Reflect policy one of the first fabric configuration items which needs to be completed for a functioning fabric. In a production fabric, it is best practice to make 2 or 4 spines route reflectors. Note: In recent versions of ACI configuring the route reflectors is done from System > System Settings > BGP Route Reflector This activity is only done once and the route reflector policy has already been configured in the the lab for all students. Access Policies \u00b6 Unlike the Fabric Policies which are often configured at turn up and then largely left alone, Access Policies are used often to configure new vlans, domains, switches, and interfaces. Fabric Policies are analogous to configuring two switches in VSS mode or in a vPC domain or VDC. Access Policies are analogous to configuring vlans, access or trunk interfaces, and interface settings. Access Policies are the \"configuration bits\" that you will fashion together to obtain the connectivity you need for your fabric. You build Fabric Access Policies with multiple configuration objects: Object Description & Use Pool Individual or a Range of VLANs Physical Domain A logical construct tying a vlan pool object to an AAEP Attachable Access Entity Profile (AAEP or AEP) An ACI object which groups physical and virtual domains for scalability. Interface Policy Describes an individual behavior.<br>For example: CDP enabled, 10Gig Interface, 1Gig Interface Interface Policy Group Bundles interface policies together for a specific interface behavior and ties that to an AAEP which now adds valid vlans on the interface Interface Profile Ties an Interface Policy Group to a specific interface number Switch Profile Defines a fabric switch or a pair of fabric switches to which Interface Profiles can be associated ACI Object (Construct) Relationship Overview \u00b6 It is vital that you understand the relationship between objects. Tips for naming objects in ACI \u00b6 A naming standard is critical to sucessful operation of ACI. It is important to define your naming conventions before configuring anything in ACI. Objects themselves cannot be renamed. If you have a typo in an object name it must be deleted and re-created. Object names are typically up to 64 alphanumeric characters in length (maximum). Use underscores (spaces are not allowed) Keep names short Already Configured \u00b6 As with the Route Reflector configuration, there are other configuration activities which are done once at the fabric level (rather than the tenant level). Because this configuration is done once per fabric, you will find them already configured in the Lab. This section will review what has already been configured and how to perform the configuration for completeness in your training. Please do not perform these configuration activities on the lab fabric. vPC Domains in ACI - vPC Protection Groups \u00b6 Since one of the first things we are going to do in this lab is configure a port channel (vPC on ACI and a port-channel on switch side), we need to make sure the two leaf switches are configured as a vPC Domain. This step is done onece for each vPC Domain but we will review the required steps here. Step 1 - Confirm the two Leaf switches are configured as a vPC Pair \u00b6 Navigate to Fabric > Access Policis > Policies > Policies > Switch > VPC Domain . Review the configuration of the default vPC domain Dead Peer timer. Even in a production environment the defaults are rarely changed. The object name can be misleading as this is not where the vPC domain is configured. Select the Virtual Port Channel default object at the top level of Fabric > Access Policies > Policies > Switch > VPC Domain In the Work Pane you can see that the two fabric leaf switches, Node IDs 102 and 103, are configured as a VPC \"Protection Group\" which is ACI's name for a vPC Domain. The VPC Pair ID is 101 (as a best practice use the lowest Node ID number) and the fabric has automatically assigned the the vPC Domain an IP address from the TEP IP address block. vPC Domain (vPC Protection Group) \u00b6 Configuring the vPC Domain is simple once you know where that configuration is done in the GUI. Configuration Item Configuration Name Configuration Values vPC Protection Group (vPC Domain) Lab_Fabric_102_103_vPCDomain ID: 102 (lowest Node ID by convention) Switch 1: 102 Switch 2: 103 You would then click Submit but remember to not do so in the Lab environment. In the Lab environment this fabric level configuration is complete and the leaf switches are already operating as a vPC pair. Switch Policies \u00b6 Defining switch \"objects\" which can then have policies applied to them is part of the initial fabric configuration and only performed once per switch and switch pair. Navigate to Fabric > Access Policies > Switches > Leaf Switches . Notice the options in the Navigation Pane: - Profiles <-- Leaf Profiles - this is where the switch \"objects\" are configured - Policy Groups <-- - Overrides Navigate to Fabric > Access Policies > Switches > Leaf Switches > Profiles and expand the Profiles option. You will see three switch \"objects\" representing - LEAF-1 - LEAF-2 - LEAF-1 and LEAF-2 Pair When you want to configure a single interface on a specific leaf you will use the specific individual switch object. In our first example, we will configure a single access port on LEAF-2. Since the switches are abstracted, it is also possible to define an object that represents both switches. When configuring a vPC or two access ports using the same port on each specific switch, this pair object can be used. In our second example we will associate the same port, e1/11, to the LEAFS_102_103_LeafProf Leaf Profile. In one step we have configured port e1/11 on both switches. In this exercise port e1/11 will be configured as a vPC. Configuring Vlan Pools, Physical Domains, and AAEPs \u00b6 Before configuring physical interface characteristics, lets configure the objects which will define our Vlans (Vlan Pools), where the vlans can be trunked (AAEPs) and how they are associated to other objects (Domains). Log in to your Student PC, open your browser, and access the APIC GUI. Vlan Pool: POD##-StaticVlanPool [vlan-3000] POD##-PhyDom POD##-AEP Configuring Interface Policies and Profiles \u00b6 Keeping in mind the full connectivity workflow of: Configure the Physical Layer Characteristics <-- You are here Configure the Logical Layer Characteristics Step 1 - Creating Interface Policies - CDP Enabled \u00b6 Navigate to Fabric > Access Policies > Policies > Interface > CDP Interface Expand CDP Interface and note that there is a default policy that comes pre configured. This is the case with many objects and while the controller will use these default policies when needed and an explicit policy is not selected, it is a best practice to explicitly configure policies for the behavior you want. Create an explicit policy which enables CDP. Right Click on CDP Interface and select the Create CDP Interface Policy option. Configuration Item Configuration Name Configuration Values Interface Policy to enable CDP POD##_CDP_Enabled Example: POD11_CDP_Enabled (for POD11 Student) Admin State = Enabled You now have a policy to enable CDP on an interface which can be reused. Note that in ACI the preferred discovery protocol is LLDP. That is enabled by default across the fabric and is a key part of the discovery process. CDP is not enabled on the fabric by default after 4.0. In order to enable CDP on an interface you will use this new just created policy. Step 2 - Creating Interface Policies - Interface Speed \u00b6 Navigate to Fabric > Access Policies > Policies > Interface > Link Level Right Click on Link Level and select the Create Link Level Policy option. Configuration Item Configuration Name Configuration Values Interface Policy to configure a 1 GigE Interface POD##_1G_IntPol Example: POD11_1G_IntPol (for POD11 Student) Speed: 1 Gbps Interface Policy to configure a 10 GigE Interface POD##_10G_IntPol Example: POD11_10G_IntPol (for POD11 Student) Speed: 10 Gbps Step 3 - Creating Interface Policies - LLDP Disable \u00b6 Navigate to Fabric > Access Policies > Policies > Interface > LLDP Interface Configuration Item Configuration Name Configuration Values Interface Policy to disable LLDP POD##_LLDP_Disabled Example: POD11_LLDP_Disabled Recieve State = Disabled Transmit State = Disabled Interface Policy to enable LLDP POD##_LLDP_Enabled Example: POD11_LLDP_Enabled Recieve State = Enabled Transmit State = Enabled Step 4 - Creating Interface Policies - Static Port Channel \u00b6 Navigate to Fabric > Access Policies > Policies > Interface > Port Channel Configuration Item Configuration Name Configuration Values Interface Policy for a static port channel POD##_Static_Po Example: POD11_Static_Po Mode: Static Channel - Mode On Interface Policy for an active LACP port channel POD##_LACP_Active_Po Example: POD11_LACP_Active_Po Mode: LACP Active Step 5 - Creating Interface Policy Groups - Access Interface \u00b6 Lets take these individual behaviors defined in each Interface Policy and bundle them into a group of behaviors that can then be associated with an interface. This bundle of configuration items defining the port behavior is called an Interface Policy Group . Lets define the Policy Group that would define a 1Gig interface with CDP enabled, LLD disabled, and a static port channel. Configuration Item Configuration Name Configuration Values Interface Policy Group for generic 1G standard access port POD##_1G_StdAccess_PortPolGrp Example: POD11_1G_StdAccess_PortPolGrp Link Level Policy: POD11_1G_IntPol CDP Policy: POD11_CDP_Enabled LLDP Policy: POD11_LLDP_Enabled Attached Entity Profile: POD11-AEP Interface Policy for an active LACP port channel POD##_LACP_Active_Po Example: POD11_p11_1G_PortPolGrp Link Level Policy: POD11_1G_IntPol CDP Policy: POD11_CDP_Enabled LLDP Policy: default Mode: LACP Active Attached Entity Profile: blank for now 1G_ Step 6 - Creating Interface Profile - Access Interface \u00b6 Check in \u00b6 With these policies, we have now defined \"reusable behaviors\": 1 Gigabit Ethernet Interface Enable CDP Disable LLDP Static Port Channel Active LAC Port Channel1G We have also defined two It important to note that these \"behaviors\" are not associated with an interface or even a switch at this point. Think of them as \"raw ingredients\" that are ready for use in order to implement a specific functionality. We will then configure a 1Gig LACP port channel down to the \"mock-core01\" switch.","title":"Lab 02 - Fabric & Access Policies"},{"location":"02_lab_fabric_access_policies/#lab-02-configure-fabric-access-constructs","text":"","title":"Lab 02 - Configure Fabric &amp; Access Constructs"},{"location":"02_lab_fabric_access_policies/#context","text":"This lab represents the implementation of networking at the physical layer across the entire fabric as well as on a switch and interface or port level. Typically in an ACI fabric, the Fabric Policies are configured during initial fabric turn up and then largely left alone. The Access Policies represetn that \"day to day\" configuration of the fabric as they define how the fabric connects to hosts and other non-ACI network devices. You an think of Access Policies as the hardware configuration of interfaces. The logical portion of the configuration is implemented when Access Policies are associated with EPGs and \"Vlans\" or Encapsulations.","title":"Context"},{"location":"02_lab_fabric_access_policies/#lab-goals","text":"In this lab we will review how to configure basic fabric functionality via Fabric Policies . These are configurations steps which are peformed at the fabric level and which have already been configured for the lab but the guide will detail the steps required to complete this step. We will then look at Access Policies which configure the physical (vs logical) aspects of fabric interfaces which connect the leafs (and spines) to compute, storage, appliances, and other non-ACI network devices.","title":"Lab Goals"},{"location":"02_lab_fabric_access_policies/#fabric-configuration","text":"Configuration in ACI is largely based on policies which abstract the characteristics of traditional network connectivity. Consider the diagram below and the steps you normally take to configure a layer 2 trunk interface like the one shows on LEAF-2 e1/11. In the classical Ethernet environment this configuration is done on LEAF-2 itself so the switch is understood. The configuration is performed on the interface itself, e1/11 in this case, so that is understood. Because these objects are abstract and all of the configuration is performed by the APIC controller, each of those \"undersood\" items are configued discreetly via policy and then put together to achieve the desired functionality. We will go through that workflow in this lab. Initially it may seem like \"extra\" work to do this but once you start taking advantage of the re-usability of these policy objects and see the ease of scalability, the \"extra\" work will be well worth it.","title":"Fabric Configuration"},{"location":"02_lab_fabric_access_policies/#implementing-cisco-aci-fabric-connectivity","text":"The Fabric menu has three submenu items. You should be familiar with the Inventory submenu from Lab 01. The other two Submenu items configure the fabric itself and the physical connectivity. Configuring ACI connectivity is generally done in two steps: 1. Configure the Physical Layer Characteristics 2. Configure the Logical Layer Characteristics","title":"Implementing Cisco ACI Fabric Connectivity"},{"location":"02_lab_fabric_access_policies/#fabric-policies","text":"Fabric policies control the configuration of the fabric itself. The most common activity in this section is to define Policy Groups such as the fabrics: Data and Time policy (NTP) Management Access policy SNMP Policy (for the fabric) BGP Route Reflector Policy","title":"Fabric Policies"},{"location":"02_lab_fabric_access_policies/#bgp-route-reflector-policy","text":"The BGP Route Reflect policy one of the first fabric configuration items which needs to be completed for a functioning fabric. In a production fabric, it is best practice to make 2 or 4 spines route reflectors. Note: In recent versions of ACI configuring the route reflectors is done from System > System Settings > BGP Route Reflector This activity is only done once and the route reflector policy has already been configured in the the lab for all students.","title":"BGP Route Reflector Policy"},{"location":"02_lab_fabric_access_policies/#access-policies","text":"Unlike the Fabric Policies which are often configured at turn up and then largely left alone, Access Policies are used often to configure new vlans, domains, switches, and interfaces. Fabric Policies are analogous to configuring two switches in VSS mode or in a vPC domain or VDC. Access Policies are analogous to configuring vlans, access or trunk interfaces, and interface settings. Access Policies are the \"configuration bits\" that you will fashion together to obtain the connectivity you need for your fabric. You build Fabric Access Policies with multiple configuration objects: Object Description & Use Pool Individual or a Range of VLANs Physical Domain A logical construct tying a vlan pool object to an AAEP Attachable Access Entity Profile (AAEP or AEP) An ACI object which groups physical and virtual domains for scalability. Interface Policy Describes an individual behavior.<br>For example: CDP enabled, 10Gig Interface, 1Gig Interface Interface Policy Group Bundles interface policies together for a specific interface behavior and ties that to an AAEP which now adds valid vlans on the interface Interface Profile Ties an Interface Policy Group to a specific interface number Switch Profile Defines a fabric switch or a pair of fabric switches to which Interface Profiles can be associated","title":"Access Policies"},{"location":"02_lab_fabric_access_policies/#aci-object-construct-relationship-overview","text":"It is vital that you understand the relationship between objects.","title":"ACI Object (Construct) Relationship Overview"},{"location":"02_lab_fabric_access_policies/#tips-for-naming-objects-in-aci","text":"A naming standard is critical to sucessful operation of ACI. It is important to define your naming conventions before configuring anything in ACI. Objects themselves cannot be renamed. If you have a typo in an object name it must be deleted and re-created. Object names are typically up to 64 alphanumeric characters in length (maximum). Use underscores (spaces are not allowed) Keep names short","title":"Tips for naming objects in ACI"},{"location":"02_lab_fabric_access_policies/#already-configured","text":"As with the Route Reflector configuration, there are other configuration activities which are done once at the fabric level (rather than the tenant level). Because this configuration is done once per fabric, you will find them already configured in the Lab. This section will review what has already been configured and how to perform the configuration for completeness in your training. Please do not perform these configuration activities on the lab fabric.","title":"Already Configured"},{"location":"02_lab_fabric_access_policies/#vpc-domains-in-aci-vpc-protection-groups","text":"Since one of the first things we are going to do in this lab is configure a port channel (vPC on ACI and a port-channel on switch side), we need to make sure the two leaf switches are configured as a vPC Domain. This step is done onece for each vPC Domain but we will review the required steps here.","title":"vPC Domains in ACI - vPC Protection Groups"},{"location":"02_lab_fabric_access_policies/#step-1-confirm-the-two-leaf-switches-are-configured-as-a-vpc-pair","text":"Navigate to Fabric > Access Policis > Policies > Policies > Switch > VPC Domain . Review the configuration of the default vPC domain Dead Peer timer. Even in a production environment the defaults are rarely changed. The object name can be misleading as this is not where the vPC domain is configured. Select the Virtual Port Channel default object at the top level of Fabric > Access Policies > Policies > Switch > VPC Domain In the Work Pane you can see that the two fabric leaf switches, Node IDs 102 and 103, are configured as a VPC \"Protection Group\" which is ACI's name for a vPC Domain. The VPC Pair ID is 101 (as a best practice use the lowest Node ID number) and the fabric has automatically assigned the the vPC Domain an IP address from the TEP IP address block.","title":"Step 1 - Confirm the two Leaf switches are configured as a vPC Pair"},{"location":"02_lab_fabric_access_policies/#vpc-domain-vpc-protection-group","text":"Configuring the vPC Domain is simple once you know where that configuration is done in the GUI. Configuration Item Configuration Name Configuration Values vPC Protection Group (vPC Domain) Lab_Fabric_102_103_vPCDomain ID: 102 (lowest Node ID by convention) Switch 1: 102 Switch 2: 103 You would then click Submit but remember to not do so in the Lab environment. In the Lab environment this fabric level configuration is complete and the leaf switches are already operating as a vPC pair.","title":"vPC Domain (vPC Protection Group)"},{"location":"02_lab_fabric_access_policies/#switch-policies","text":"Defining switch \"objects\" which can then have policies applied to them is part of the initial fabric configuration and only performed once per switch and switch pair. Navigate to Fabric > Access Policies > Switches > Leaf Switches . Notice the options in the Navigation Pane: - Profiles <-- Leaf Profiles - this is where the switch \"objects\" are configured - Policy Groups <-- - Overrides Navigate to Fabric > Access Policies > Switches > Leaf Switches > Profiles and expand the Profiles option. You will see three switch \"objects\" representing - LEAF-1 - LEAF-2 - LEAF-1 and LEAF-2 Pair When you want to configure a single interface on a specific leaf you will use the specific individual switch object. In our first example, we will configure a single access port on LEAF-2. Since the switches are abstracted, it is also possible to define an object that represents both switches. When configuring a vPC or two access ports using the same port on each specific switch, this pair object can be used. In our second example we will associate the same port, e1/11, to the LEAFS_102_103_LeafProf Leaf Profile. In one step we have configured port e1/11 on both switches. In this exercise port e1/11 will be configured as a vPC.","title":"Switch Policies"},{"location":"02_lab_fabric_access_policies/#configuring-vlan-pools-physical-domains-and-aaeps","text":"Before configuring physical interface characteristics, lets configure the objects which will define our Vlans (Vlan Pools), where the vlans can be trunked (AAEPs) and how they are associated to other objects (Domains). Log in to your Student PC, open your browser, and access the APIC GUI. Vlan Pool: POD##-StaticVlanPool [vlan-3000] POD##-PhyDom POD##-AEP","title":"Configuring Vlan Pools, Physical Domains, and AAEPs"},{"location":"02_lab_fabric_access_policies/#configuring-interface-policies-and-profiles","text":"Keeping in mind the full connectivity workflow of: Configure the Physical Layer Characteristics <-- You are here Configure the Logical Layer Characteristics","title":"Configuring Interface Policies and Profiles"},{"location":"02_lab_fabric_access_policies/#step-1-creating-interface-policies-cdp-enabled","text":"Navigate to Fabric > Access Policies > Policies > Interface > CDP Interface Expand CDP Interface and note that there is a default policy that comes pre configured. This is the case with many objects and while the controller will use these default policies when needed and an explicit policy is not selected, it is a best practice to explicitly configure policies for the behavior you want. Create an explicit policy which enables CDP. Right Click on CDP Interface and select the Create CDP Interface Policy option. Configuration Item Configuration Name Configuration Values Interface Policy to enable CDP POD##_CDP_Enabled Example: POD11_CDP_Enabled (for POD11 Student) Admin State = Enabled You now have a policy to enable CDP on an interface which can be reused. Note that in ACI the preferred discovery protocol is LLDP. That is enabled by default across the fabric and is a key part of the discovery process. CDP is not enabled on the fabric by default after 4.0. In order to enable CDP on an interface you will use this new just created policy.","title":"Step 1 - Creating Interface Policies - CDP Enabled"},{"location":"02_lab_fabric_access_policies/#step-2-creating-interface-policies-interface-speed","text":"Navigate to Fabric > Access Policies > Policies > Interface > Link Level Right Click on Link Level and select the Create Link Level Policy option. Configuration Item Configuration Name Configuration Values Interface Policy to configure a 1 GigE Interface POD##_1G_IntPol Example: POD11_1G_IntPol (for POD11 Student) Speed: 1 Gbps Interface Policy to configure a 10 GigE Interface POD##_10G_IntPol Example: POD11_10G_IntPol (for POD11 Student) Speed: 10 Gbps","title":"Step 2 - Creating Interface Policies - Interface Speed"},{"location":"02_lab_fabric_access_policies/#step-3-creating-interface-policies-lldp-disable","text":"Navigate to Fabric > Access Policies > Policies > Interface > LLDP Interface Configuration Item Configuration Name Configuration Values Interface Policy to disable LLDP POD##_LLDP_Disabled Example: POD11_LLDP_Disabled Recieve State = Disabled Transmit State = Disabled Interface Policy to enable LLDP POD##_LLDP_Enabled Example: POD11_LLDP_Enabled Recieve State = Enabled Transmit State = Enabled","title":"Step 3 - Creating Interface Policies - LLDP Disable"},{"location":"02_lab_fabric_access_policies/#step-4-creating-interface-policies-static-port-channel","text":"Navigate to Fabric > Access Policies > Policies > Interface > Port Channel Configuration Item Configuration Name Configuration Values Interface Policy for a static port channel POD##_Static_Po Example: POD11_Static_Po Mode: Static Channel - Mode On Interface Policy for an active LACP port channel POD##_LACP_Active_Po Example: POD11_LACP_Active_Po Mode: LACP Active","title":"Step 4 - Creating Interface Policies - Static Port Channel"},{"location":"02_lab_fabric_access_policies/#step-5-creating-interface-policy-groups-access-interface","text":"Lets take these individual behaviors defined in each Interface Policy and bundle them into a group of behaviors that can then be associated with an interface. This bundle of configuration items defining the port behavior is called an Interface Policy Group . Lets define the Policy Group that would define a 1Gig interface with CDP enabled, LLD disabled, and a static port channel. Configuration Item Configuration Name Configuration Values Interface Policy Group for generic 1G standard access port POD##_1G_StdAccess_PortPolGrp Example: POD11_1G_StdAccess_PortPolGrp Link Level Policy: POD11_1G_IntPol CDP Policy: POD11_CDP_Enabled LLDP Policy: POD11_LLDP_Enabled Attached Entity Profile: POD11-AEP Interface Policy for an active LACP port channel POD##_LACP_Active_Po Example: POD11_p11_1G_PortPolGrp Link Level Policy: POD11_1G_IntPol CDP Policy: POD11_CDP_Enabled LLDP Policy: default Mode: LACP Active Attached Entity Profile: blank for now 1G_","title":"Step 5 - Creating Interface Policy Groups - Access Interface"},{"location":"02_lab_fabric_access_policies/#step-6-creating-interface-profile-access-interface","text":"","title":"Step 6 - Creating Interface Profile - Access Interface"},{"location":"02_lab_fabric_access_policies/#check-in","text":"With these policies, we have now defined \"reusable behaviors\": 1 Gigabit Ethernet Interface Enable CDP Disable LLDP Static Port Channel Active LAC Port Channel1G We have also defined two It important to note that these \"behaviors\" are not associated with an interface or even a switch at this point. Think of them as \"raw ingredients\" that are ready for use in order to implement a specific functionality. We will then configure a 1Gig LACP port channel down to the \"mock-core01\" switch.","title":"Check in"},{"location":"03_lab_tenant_constructs/","text":"Lab 03 - Configure Tenants and Tenant Constructs \u00b6 Context \u00b6 Having completed the Fabric Policy configuration so that we have an operational Underlay (confusingly called \"Overlay-1\" as you will see when executing CLI commands) and set our Switch Policies (which define our switches) and Interface Policies, Profiles, and AEPs (which define interface behaviors and allowed encapsulations or Vlans) we now start to define the logical components that will make it very easy to secure hosts and services uniformly at scale without consideration of their IP addresses, rather via their EPG association. This logical separation begins with one or more Tenants. The fabric comes \"out of the box\" with three Tenants. infra A Tenant container for the ACI Fabric Infrastructure mgmt Dedicated tenant for managing the ACI Fabric common A tenant with special properties so that any construct configured within this tenant is automatically available to any other Tenant created in the fabric. This tenant is very useful for shared services and you will often find an L3Out , VRF, and Bridge Domains which are shared across other Tenants configured in the common tenant. Lab Goals \u00b6 Tenant portion of the ACI Management Information Tree (MIT) Understand the relationships between the ACI Logical Constructs Step 1: Create a Tenant and VRF Step 2: Create a Bridge Domain and Subnets Step 3: Create an Application Profile and EPGs Step 4: Create Filters and Contracts Step 5: Apply Contracts to EPGs This is the final \"set up only\" lab. At the completion of this lab, the fabric will be ready for actual traffic and connectivity. The following constructs or objects will be configured in this lab: Object Type Object Name Function Tenant POD##_Tenant Administrative domain containing all subsequent objects (VRFs, BDs, EPGs, Subnets, Application Profiles) VRF POD##_VRF Layer 3 routing and forwarding domain within a tenant Bridge Domain POD##_BD Logical container defining flooding behavior. Always associated with a single VRF and often associated with one ore more subnets. Subnet 10.0.1.254/24 IP Subnet Application Profile Tiered_AppProfile Container for EPGs and the policies which define associations, encapsulations, and interactions EGP - Web - App - DB Logical grouping of endpoints which have similar requirements, often security requirements. Reminder: ## = Your Pod Number. For example, if your Pod number is 11, then replace ## with 11 (POD11_Tenant) It is important to think about how to consistently name your constructs. You have some flexibility in the lab but please keep in mind that the lab is shared and so you should always make sure your constructs can be associated to your tenant and your work. Step 1 - Create a Tenant and VRF \u00b6 A Tenant in ACI represents a management domain. Common tenants in actual deployments include tenants such as: Production Dev QA DMZ As you can see from the MIT diagram, a Tenant contains one or more VRFs and so you often find that the Production tenant has a Production VRF associated with it. A VRF in ACI is a VRF. You cannot have overlapping IP Address space within an VRF and so the same is true for a VRF in ACI. In the lab, each lab participant will create a tenant based on their Pod assignment (Pod number). Navigate to Tenants > Add Tenant The Create Tenant dialog box will appear. The only required field is the Tenant name. Enter your Tenant name in the Name field and click Submit. Notice that here you can also associate a Monitoring Policy as well as Security Domains. If this Tenant had special access requirements, then here is where you can associate a specific Security Domain policy with the Tenant. This can be done after the Tenant is created. Because we left the \"Take me to this tenant when I click finish\" option checked, we will be taken to the new tenant page once Submit is clicked. From here navigate to Networking > VRFs and right click on the VRF Folder icon. Select Create VRF and the Create VRF dialog will appear. In the Create VRF dialog, enter the VRF name, uncheck the \"Create A Bridge Domain\" check box, and Click \"Finish\". Once you uncheck the \"Create A Bridge Domain\" check box the Next option becomes Finish as there is no next step. All other values remain unchanged and at their default settings. As you know, ACI behaves in a default-deny, \"white list\" security model, so that only the required access is defined for connectivity. In a \"black list\" model, only the \"bad\" traffic is denied which assumes you know what the \"bad\" traffic is. The default-deny, only allow what is needed (\"white list\"), is by far the more secure. ACI does give you the option of disabling this behavior in several ways, one of which is here a the VRF level. Note that the Policy Control Enforcement Preference is Enforced by default. Should you have a need to disable this behavior (NOT RECOMMENDED), you can do so by setting Policy Control Enforcement Preference to Unenforced . This is rarely done in production but may be a useful troubleshooting tool. Because this VRF was created within the Tenant context it is now associated with the Tenant. Step 2: Create a Bridge Domain and Subnets \u00b6 In traditional networking a Vlan inherently defines a broadcast domain, an encapsulation, and optionally a Layer 3 subnet. We don't typically think of all of those items as separate functions but in ACI you must. In its basic form, a Bridge Domain (BD) defines a broadcast domain as well as flooding behavior. We don't call this out specifically in traditional network because we don't have any other options for flooding behavior, but in ACI we do. ACI can optimize flooding behavior and reduce broadcasts and so a Bridge Domain allows you to define how you want flooding to behave for a particular Bridge Domain. It is also common to define a subnet within the Bridge Domain thereby creating the foundation for a Layer 3 \"Vlan\". Notice that you don't define the encapsulation within the BD. That takes place within an EPG either statically or dynamically depending on the need and the associated Domains (Physical, VMM). A BD can have multiple subnets and contains settings which instantiate the subnet's gateway on the ACI Fabric. Step 2.1 - Create Bridge Domain \u00b6 Navigate to Tenants > PODXX_Tenant > Networking > Bridge Domains and right click on the Bridge Domain Folder icon. Select Create Bridge Domain and the Create Bridge Domain dialog will appear. Name: PODXX_BD VRF: PODXX_VRF (PODXX_Tenant) Forwarding: Cusom (change from the default Optimize) Select Custom in the Forwarding: drop down Click Next Step 2.2 - Create Subnets \u00b6 As with any data center it is important to consider the subnetting design. With ACI, taking a more network centric approach, each BD can be associated with one subnet, simulating what we are used to today. One Bridge Domain, advertising one subnet and associated with one EPG and one encapsulation mimics an SVI in todays networking. In the Lab, we will model a more powerful approach, to give you a sense of what can be accomplished with the flexibility of ACI. In our design we will have one Bridge Domain, and within that BD we will instantiate the three subnets we require for our data center Virtual Machines. Subnet Design for POD##: Subnet Gateway IP: (Anycast Gateway)* Use 10.0.1.0/24 10.0.1.254/24 10.0.2.0/24 10.0.2.254/24 10.0.3.0/24 10.0.3.254/24 * Note: The Gateway IP: is exactly that, defining both the subnet and mask and the anycast gateway IP. (.254 for all subnets). Also, consider that each student is configuring the same subnets for their VMs. This is possible because each student has their own Tenant and VRF. Click the plus (+) in the Subnets: table to add the three subnets in listed in the table. Leave all other settings at their default values and click OK to advance to the \"Step 3 Troubleshooting\" screen. Note the options for Scope: . Scope: When to use... Private to VRF (Default) Advertised Externally Share between vRF Click OK Make no changes to the \"STEP 3 > Advanced/Troubleshooting\" section and click Finish . Review the list of Bridge Domains. You have configured a single BD in your tenant which contains three subnets. The BD is configured to flood in the \"classical\" way rather than the optimized way ACI can also support. Step 3: Create an Application Profile and EPGs \u00b6 An Application Profile is on organizational or grouping construct only. It is a container or folder for EPGs and their associations. An EndPoint Group (EPG) is a container for endpoints that share some commonality. In many cases, these are endpoints that share a common security profile. This is an important distinction to make because Contracts (think of these as ACLs which act on EPGs rather than subnets and IP addresses) are applied to EPGs. Navigate to Tenants > PODXX_Tenant > Application Profiles and right click on the Application Profiles Folder icon. Select Create Application Profile and the Create Application Profile dialog will appear. Enter the Application Profile Name in the Name: field and click Submit . You now have a container for your EPGs and their associated constructs. From here, expand the Application Profile folder by clicking on the \">\" symbol. Expand your Application Profile Tiered_AppProfile and right click on the Application EPG s folder icon for the \" Create Application EPG \" option. Remember the Tenant portion of the ACI Management Information Tree (MIT). Notice that an EPG must be associated to one Application Profile (which is associated to one Tenant) one Bridge Domain (which is associated to one VRF) At a minimum those objects must be defined in order to create an EPG. Other EPG attributes such as Contracts and VM Domain profiles can be configured at a later time. Enter the EPG name in the Name: field. Select the Bridge Domain in the Bridge Domain: field and click Finish . Accept all the default values. Using this same process, create two additional EPGs within this Application Profile, mapped to your PODXX_BD Bridge Domain. App DB Step 4: Create Filters and Contracts \u00b6 Contracts, and how they behave, make ACI one of the easiest fabrics to secure. Think of them as Access Control Lists which can be applied anywhere across the fabric (across all of your top of rack Leaf switches) and which are independent of a vlan or IP Subnet/Address. A Cisco ACI filter is a TCP/IP header field, such as a Layer 3 protocol type or Layer 4 ports, that are used to allow inbound or outbound communications between EPGs. A Cisco ACI contract defines the rules that specify what and how communication in a network is allowed. In Cisco ACI, contracts specify how communications between EPGs take place. Contract scope can be limited to the EPGs in an application profile, a tenant, a VRF, or the entire fabric. It is important to understand the impact scope can have on access. Contracts are wholly logical constructs and so there is no Physical Layer configuration. Contracts defined in the common tenant can be used across any Tenant like other constructs defined in the common tenant. You will configure three filters and use them as building blocks for defining contracts between EPGs. Basic Filter Allow ICMP and SSH HTTP Filter Allow HTTP FTP Filter Allow FTP A few naming Tips: Start the name with protocol/port as that is what typically appears to reduce ambiguity If you are going to do Taboo (not recommended) or Deny contracts, consider adding the action to the Subject name. For example: TCP_22_Permit_SSH_Subject. If you will not use these then establish the convention that all Subjects are \"Permit\".Consider adding Source or Destination (For example: TCP_9004_DST_Entry) Example Object Names: TCP_9004_Filter TCP_9004_Entry PKI_BIDi_Contract TCP_22_SSH_Subject For the lab we will use a simplified naming convention. Filter Table \u00b6 Filter Name Entry Name EtherType IP Protocol Source Port_Range From - To Destination Port_Range From - To BASIC-POD## ICMP IP icmp Leave blank Leave blank BASIC-POD## (second entry) SSH IP tcp 1024 - 65535 22 - 22 HTTP-POD## HTTP IP tcp 1024 - 65535 80 - 80 (http - http also works) FTP-POD## FTP IP tcp 1024 - 65535 21 - 21 Step 4.1 Basic Filter for Ping (ICMP) and SSH \u00b6 Within your Tenant, navigate to Contracts > Filters Right-click Filters and choose Create Filters Create a filter named BASIC-POD## Click the plus sign ( + ) to add an Entry (again, think of this as a single Access List Entry). Complete the first entry which you will name ICMP per the table above Click the Update button for your entry row. This filter will have two entries, ICMP and SSH Click the plus sign again to add the second SSH entry. Click Update and Submit Step 4.2 HTTP Filter for HTTP \u00b6 Using the procedure above and the information in the Filter Table, create an HTTP filter. Step 4.3 FTP Filter for FTP \u00b6 Using the procedure above and the information in the Filter Table, create an FTP filter. Once you complete all of the filters in the Filter Table, you should be able to expand all the objects in the Filter folder and should resemble this: Step 4.4 Contracts \u00b6 Contracts provide a way to control traffic flow within the ACI fabric between endpoint groups. Because ACI operates by default in a white list mode, without contracts there is no communication between EPGs. These contracts are built using a provider-consumer model where one endpoint group provides the services it wants to offer (servers) and another endpoint group consumes them (servers or clients). Contracts are assigned a scope of Global, Tenant, VRF, or Application Profile, which limit the accessibility of the contract. The default scope is VRF.Exter Contracts consist of: Subjects - One or more subjects can be associated with a Contract. A subject has a name, a filter, and an action (Permit or Deny). A Subject also has a shortcut to support bidirectional (Apply Both Directions and Reverse Filter Ports). Filters - a Subject can have 1 or more filters. Think of the Filter as an ACE or Entry within an ACL. A Contract in ACI will define the permitted (white list) communication between its associated EPGs. Again, think of the Filters as Access Control Entries (ACE) and the Contract as an Access Control List (ACL). The biggest difference you will see, is that there are no vlans or IP Subnets or Host IP Addresses in these configurations. With the Filters you have defined, you have port/protocol objects which you can now bundle together and apply in whichever direction is appropriate to EPGs.POD You will create two Contracts Using these FIlter Objects. Contract Table \u00b6 Contract Name Subject Name Apply Both Directions Checkbox Reverse Filters Ports Checkbox Filter Name (Select Existing form Drop Down) Action WebAccess-POD## WEB-ACCESS Checked (Default) Checked (Default) PODXX/BASIC-PODXX Permit WebAccess-POD## WEB-ACCESS (Second Entry) Checked (Default) Checked (Default) PODXX/HTTP-PODXX Permit FileAccess-POD## FILE-ACCESS Checked (Default) Checked (Default) PODXX/BASIC-PODX Permit FileAccess-POD## FILE-ACCESS (Second Entry) Checked (Default) Checked (Default) PODXX/FTP-PODXX Permit Directives Leave blank for all (Default) Priority- Leave blank for all (Default) Step 4.4.1 Web Access Contract \u00b6 Within your Tenant, navigate to Contracts Right-click Contracts folder icon and choose Create Contract Create a contract named WebAccess-POD## . Accept all the defaults including the Scope (VRF). Click the plus sign ( + ) in the Subjects: section to create a subject and add Filters. Add Filters using the previously created filters per the table above Click the Update button for your entry row. Click OK to return to the Contract dialog Click Submit to complete the contract Step 4.4.2 File Access Contract \u00b6 Using the procedure above and the information in the Contract Table, create the File Access Contract. Upon completion of the two Contracts, your Contracts section should look like below: Step 5: Apply Contracts to EPGs \u00b6 Just as with ACLs its a good idea to make sure you understand the application flows. In ACI the terminology may add confusion at first. Just remember that 'Providing' a Contract means providing a service (like TCP/80 inbound) and 'Consuming' a contract allows an EPG to connect to a provided service. Without contracts the endpoints in our EPGs have no connectivity outside their EPG. When we apply our new Contracts we want the endpoints in the App EPG to provide HTTP, ICMP, and SSH to the endpoints in the Web EPG. We also want the endpoints (databases and file servers) in the DB EPG to provide FTP, ICMP, and SSH services to the endpoints in the App EPG. Step 5.1 Apply the WebAccess-PODXX contract as a Provided Contract to the App EPG \u00b6 Navigate to PODXX_Tenant > and expand Tiered-App > Application EPGs > App > Contracts Right click on the Contracts Folder icon and select Add Provided Contract Select WebAccess-POD## contract. No other changes are necessary Click Submit Examine the visual relationship by clicking on your Tiered-App Application Profile Step 5.2 Apply the FileAccess-PODXX contract as a Provided Contract to the DB EPG \u00b6 Use the procedure above to apply the FileAccess-POD## contract as a Provided Contract of the DB EPG. Check in \u00b6 This is quite a long lab so lets check in to see what we have accomplished and what functionality we should have. Once this Lab is complete you should have: A Tenant and associated VRF A Bridge Domain containing three subnets associated with the VRF An Application Profile \"container\" with three EPGs all mapped to the Bridge Domain (one BD but three subnets) Filters describing ICMP SSH HTTP FTP Two contracts describing the allowed connectivity between the App EPG and the Web EPG utilizing the appropriate Filters the allowed connectivity between the APP EPG and the DB EPG utilizing the appropriate Filters The contracts applied to the appropriate EPGs in the appropriate direction ( Provider - Server and Server Services and Consumer - Clients) The necessary Tenant-level logical components have now been created however a few steps remain to achieve connectivity external to the fabric and to Virtualized Hosts and Virtual Machines.","title":"Lab 03 - Tenants"},{"location":"03_lab_tenant_constructs/#lab-03-configure-tenants-and-tenant-constructs","text":"","title":"Lab 03 - Configure Tenants and Tenant Constructs"},{"location":"03_lab_tenant_constructs/#context","text":"Having completed the Fabric Policy configuration so that we have an operational Underlay (confusingly called \"Overlay-1\" as you will see when executing CLI commands) and set our Switch Policies (which define our switches) and Interface Policies, Profiles, and AEPs (which define interface behaviors and allowed encapsulations or Vlans) we now start to define the logical components that will make it very easy to secure hosts and services uniformly at scale without consideration of their IP addresses, rather via their EPG association. This logical separation begins with one or more Tenants. The fabric comes \"out of the box\" with three Tenants. infra A Tenant container for the ACI Fabric Infrastructure mgmt Dedicated tenant for managing the ACI Fabric common A tenant with special properties so that any construct configured within this tenant is automatically available to any other Tenant created in the fabric. This tenant is very useful for shared services and you will often find an L3Out , VRF, and Bridge Domains which are shared across other Tenants configured in the common tenant.","title":"Context"},{"location":"03_lab_tenant_constructs/#lab-goals","text":"Tenant portion of the ACI Management Information Tree (MIT) Understand the relationships between the ACI Logical Constructs Step 1: Create a Tenant and VRF Step 2: Create a Bridge Domain and Subnets Step 3: Create an Application Profile and EPGs Step 4: Create Filters and Contracts Step 5: Apply Contracts to EPGs This is the final \"set up only\" lab. At the completion of this lab, the fabric will be ready for actual traffic and connectivity. The following constructs or objects will be configured in this lab: Object Type Object Name Function Tenant POD##_Tenant Administrative domain containing all subsequent objects (VRFs, BDs, EPGs, Subnets, Application Profiles) VRF POD##_VRF Layer 3 routing and forwarding domain within a tenant Bridge Domain POD##_BD Logical container defining flooding behavior. Always associated with a single VRF and often associated with one ore more subnets. Subnet 10.0.1.254/24 IP Subnet Application Profile Tiered_AppProfile Container for EPGs and the policies which define associations, encapsulations, and interactions EGP - Web - App - DB Logical grouping of endpoints which have similar requirements, often security requirements. Reminder: ## = Your Pod Number. For example, if your Pod number is 11, then replace ## with 11 (POD11_Tenant) It is important to think about how to consistently name your constructs. You have some flexibility in the lab but please keep in mind that the lab is shared and so you should always make sure your constructs can be associated to your tenant and your work.","title":"Lab Goals"},{"location":"03_lab_tenant_constructs/#step-1-create-a-tenant-and-vrf","text":"A Tenant in ACI represents a management domain. Common tenants in actual deployments include tenants such as: Production Dev QA DMZ As you can see from the MIT diagram, a Tenant contains one or more VRFs and so you often find that the Production tenant has a Production VRF associated with it. A VRF in ACI is a VRF. You cannot have overlapping IP Address space within an VRF and so the same is true for a VRF in ACI. In the lab, each lab participant will create a tenant based on their Pod assignment (Pod number). Navigate to Tenants > Add Tenant The Create Tenant dialog box will appear. The only required field is the Tenant name. Enter your Tenant name in the Name field and click Submit. Notice that here you can also associate a Monitoring Policy as well as Security Domains. If this Tenant had special access requirements, then here is where you can associate a specific Security Domain policy with the Tenant. This can be done after the Tenant is created. Because we left the \"Take me to this tenant when I click finish\" option checked, we will be taken to the new tenant page once Submit is clicked. From here navigate to Networking > VRFs and right click on the VRF Folder icon. Select Create VRF and the Create VRF dialog will appear. In the Create VRF dialog, enter the VRF name, uncheck the \"Create A Bridge Domain\" check box, and Click \"Finish\". Once you uncheck the \"Create A Bridge Domain\" check box the Next option becomes Finish as there is no next step. All other values remain unchanged and at their default settings. As you know, ACI behaves in a default-deny, \"white list\" security model, so that only the required access is defined for connectivity. In a \"black list\" model, only the \"bad\" traffic is denied which assumes you know what the \"bad\" traffic is. The default-deny, only allow what is needed (\"white list\"), is by far the more secure. ACI does give you the option of disabling this behavior in several ways, one of which is here a the VRF level. Note that the Policy Control Enforcement Preference is Enforced by default. Should you have a need to disable this behavior (NOT RECOMMENDED), you can do so by setting Policy Control Enforcement Preference to Unenforced . This is rarely done in production but may be a useful troubleshooting tool. Because this VRF was created within the Tenant context it is now associated with the Tenant.","title":"Step 1 - Create a Tenant and VRF"},{"location":"03_lab_tenant_constructs/#step-2-create-a-bridge-domain-and-subnets","text":"In traditional networking a Vlan inherently defines a broadcast domain, an encapsulation, and optionally a Layer 3 subnet. We don't typically think of all of those items as separate functions but in ACI you must. In its basic form, a Bridge Domain (BD) defines a broadcast domain as well as flooding behavior. We don't call this out specifically in traditional network because we don't have any other options for flooding behavior, but in ACI we do. ACI can optimize flooding behavior and reduce broadcasts and so a Bridge Domain allows you to define how you want flooding to behave for a particular Bridge Domain. It is also common to define a subnet within the Bridge Domain thereby creating the foundation for a Layer 3 \"Vlan\". Notice that you don't define the encapsulation within the BD. That takes place within an EPG either statically or dynamically depending on the need and the associated Domains (Physical, VMM). A BD can have multiple subnets and contains settings which instantiate the subnet's gateway on the ACI Fabric.","title":"Step 2: Create a Bridge Domain and Subnets"},{"location":"03_lab_tenant_constructs/#step-21-create-bridge-domain","text":"Navigate to Tenants > PODXX_Tenant > Networking > Bridge Domains and right click on the Bridge Domain Folder icon. Select Create Bridge Domain and the Create Bridge Domain dialog will appear. Name: PODXX_BD VRF: PODXX_VRF (PODXX_Tenant) Forwarding: Cusom (change from the default Optimize) Select Custom in the Forwarding: drop down Click Next","title":"Step 2.1 - Create Bridge Domain"},{"location":"03_lab_tenant_constructs/#step-22-create-subnets","text":"As with any data center it is important to consider the subnetting design. With ACI, taking a more network centric approach, each BD can be associated with one subnet, simulating what we are used to today. One Bridge Domain, advertising one subnet and associated with one EPG and one encapsulation mimics an SVI in todays networking. In the Lab, we will model a more powerful approach, to give you a sense of what can be accomplished with the flexibility of ACI. In our design we will have one Bridge Domain, and within that BD we will instantiate the three subnets we require for our data center Virtual Machines. Subnet Design for POD##: Subnet Gateway IP: (Anycast Gateway)* Use 10.0.1.0/24 10.0.1.254/24 10.0.2.0/24 10.0.2.254/24 10.0.3.0/24 10.0.3.254/24 * Note: The Gateway IP: is exactly that, defining both the subnet and mask and the anycast gateway IP. (.254 for all subnets). Also, consider that each student is configuring the same subnets for their VMs. This is possible because each student has their own Tenant and VRF. Click the plus (+) in the Subnets: table to add the three subnets in listed in the table. Leave all other settings at their default values and click OK to advance to the \"Step 3 Troubleshooting\" screen. Note the options for Scope: . Scope: When to use... Private to VRF (Default) Advertised Externally Share between vRF Click OK Make no changes to the \"STEP 3 > Advanced/Troubleshooting\" section and click Finish . Review the list of Bridge Domains. You have configured a single BD in your tenant which contains three subnets. The BD is configured to flood in the \"classical\" way rather than the optimized way ACI can also support.","title":"Step 2.2 - Create Subnets"},{"location":"03_lab_tenant_constructs/#step-3-create-an-application-profile-and-epgs","text":"An Application Profile is on organizational or grouping construct only. It is a container or folder for EPGs and their associations. An EndPoint Group (EPG) is a container for endpoints that share some commonality. In many cases, these are endpoints that share a common security profile. This is an important distinction to make because Contracts (think of these as ACLs which act on EPGs rather than subnets and IP addresses) are applied to EPGs. Navigate to Tenants > PODXX_Tenant > Application Profiles and right click on the Application Profiles Folder icon. Select Create Application Profile and the Create Application Profile dialog will appear. Enter the Application Profile Name in the Name: field and click Submit . You now have a container for your EPGs and their associated constructs. From here, expand the Application Profile folder by clicking on the \">\" symbol. Expand your Application Profile Tiered_AppProfile and right click on the Application EPG s folder icon for the \" Create Application EPG \" option. Remember the Tenant portion of the ACI Management Information Tree (MIT). Notice that an EPG must be associated to one Application Profile (which is associated to one Tenant) one Bridge Domain (which is associated to one VRF) At a minimum those objects must be defined in order to create an EPG. Other EPG attributes such as Contracts and VM Domain profiles can be configured at a later time. Enter the EPG name in the Name: field. Select the Bridge Domain in the Bridge Domain: field and click Finish . Accept all the default values. Using this same process, create two additional EPGs within this Application Profile, mapped to your PODXX_BD Bridge Domain. App DB","title":"Step 3: Create an Application Profile and EPGs"},{"location":"03_lab_tenant_constructs/#step-4-create-filters-and-contracts","text":"Contracts, and how they behave, make ACI one of the easiest fabrics to secure. Think of them as Access Control Lists which can be applied anywhere across the fabric (across all of your top of rack Leaf switches) and which are independent of a vlan or IP Subnet/Address. A Cisco ACI filter is a TCP/IP header field, such as a Layer 3 protocol type or Layer 4 ports, that are used to allow inbound or outbound communications between EPGs. A Cisco ACI contract defines the rules that specify what and how communication in a network is allowed. In Cisco ACI, contracts specify how communications between EPGs take place. Contract scope can be limited to the EPGs in an application profile, a tenant, a VRF, or the entire fabric. It is important to understand the impact scope can have on access. Contracts are wholly logical constructs and so there is no Physical Layer configuration. Contracts defined in the common tenant can be used across any Tenant like other constructs defined in the common tenant. You will configure three filters and use them as building blocks for defining contracts between EPGs. Basic Filter Allow ICMP and SSH HTTP Filter Allow HTTP FTP Filter Allow FTP A few naming Tips: Start the name with protocol/port as that is what typically appears to reduce ambiguity If you are going to do Taboo (not recommended) or Deny contracts, consider adding the action to the Subject name. For example: TCP_22_Permit_SSH_Subject. If you will not use these then establish the convention that all Subjects are \"Permit\".Consider adding Source or Destination (For example: TCP_9004_DST_Entry) Example Object Names: TCP_9004_Filter TCP_9004_Entry PKI_BIDi_Contract TCP_22_SSH_Subject For the lab we will use a simplified naming convention.","title":"Step 4: Create Filters and Contracts"},{"location":"03_lab_tenant_constructs/#filter-table","text":"Filter Name Entry Name EtherType IP Protocol Source Port_Range From - To Destination Port_Range From - To BASIC-POD## ICMP IP icmp Leave blank Leave blank BASIC-POD## (second entry) SSH IP tcp 1024 - 65535 22 - 22 HTTP-POD## HTTP IP tcp 1024 - 65535 80 - 80 (http - http also works) FTP-POD## FTP IP tcp 1024 - 65535 21 - 21","title":"Filter Table"},{"location":"03_lab_tenant_constructs/#step-41-basic-filter-for-ping-icmp-and-ssh","text":"Within your Tenant, navigate to Contracts > Filters Right-click Filters and choose Create Filters Create a filter named BASIC-POD## Click the plus sign ( + ) to add an Entry (again, think of this as a single Access List Entry). Complete the first entry which you will name ICMP per the table above Click the Update button for your entry row. This filter will have two entries, ICMP and SSH Click the plus sign again to add the second SSH entry. Click Update and Submit","title":"Step 4.1 Basic Filter for Ping (ICMP) and SSH"},{"location":"03_lab_tenant_constructs/#step-42-http-filter-for-http","text":"Using the procedure above and the information in the Filter Table, create an HTTP filter.","title":"Step 4.2 HTTP Filter for HTTP"},{"location":"03_lab_tenant_constructs/#step-43-ftp-filter-for-ftp","text":"Using the procedure above and the information in the Filter Table, create an FTP filter. Once you complete all of the filters in the Filter Table, you should be able to expand all the objects in the Filter folder and should resemble this:","title":"Step 4.3 FTP Filter for FTP"},{"location":"03_lab_tenant_constructs/#step-44-contracts","text":"Contracts provide a way to control traffic flow within the ACI fabric between endpoint groups. Because ACI operates by default in a white list mode, without contracts there is no communication between EPGs. These contracts are built using a provider-consumer model where one endpoint group provides the services it wants to offer (servers) and another endpoint group consumes them (servers or clients). Contracts are assigned a scope of Global, Tenant, VRF, or Application Profile, which limit the accessibility of the contract. The default scope is VRF.Exter Contracts consist of: Subjects - One or more subjects can be associated with a Contract. A subject has a name, a filter, and an action (Permit or Deny). A Subject also has a shortcut to support bidirectional (Apply Both Directions and Reverse Filter Ports). Filters - a Subject can have 1 or more filters. Think of the Filter as an ACE or Entry within an ACL. A Contract in ACI will define the permitted (white list) communication between its associated EPGs. Again, think of the Filters as Access Control Entries (ACE) and the Contract as an Access Control List (ACL). The biggest difference you will see, is that there are no vlans or IP Subnets or Host IP Addresses in these configurations. With the Filters you have defined, you have port/protocol objects which you can now bundle together and apply in whichever direction is appropriate to EPGs.POD You will create two Contracts Using these FIlter Objects.","title":"Step 4.4 Contracts"},{"location":"03_lab_tenant_constructs/#contract-table","text":"Contract Name Subject Name Apply Both Directions Checkbox Reverse Filters Ports Checkbox Filter Name (Select Existing form Drop Down) Action WebAccess-POD## WEB-ACCESS Checked (Default) Checked (Default) PODXX/BASIC-PODXX Permit WebAccess-POD## WEB-ACCESS (Second Entry) Checked (Default) Checked (Default) PODXX/HTTP-PODXX Permit FileAccess-POD## FILE-ACCESS Checked (Default) Checked (Default) PODXX/BASIC-PODX Permit FileAccess-POD## FILE-ACCESS (Second Entry) Checked (Default) Checked (Default) PODXX/FTP-PODXX Permit Directives Leave blank for all (Default) Priority- Leave blank for all (Default)","title":"Contract Table"},{"location":"03_lab_tenant_constructs/#step-441-web-access-contract","text":"Within your Tenant, navigate to Contracts Right-click Contracts folder icon and choose Create Contract Create a contract named WebAccess-POD## . Accept all the defaults including the Scope (VRF). Click the plus sign ( + ) in the Subjects: section to create a subject and add Filters. Add Filters using the previously created filters per the table above Click the Update button for your entry row. Click OK to return to the Contract dialog Click Submit to complete the contract","title":"Step 4.4.1 Web Access Contract"},{"location":"03_lab_tenant_constructs/#step-442-file-access-contract","text":"Using the procedure above and the information in the Contract Table, create the File Access Contract. Upon completion of the two Contracts, your Contracts section should look like below:","title":"Step 4.4.2 File Access Contract"},{"location":"03_lab_tenant_constructs/#step-5-apply-contracts-to-epgs","text":"Just as with ACLs its a good idea to make sure you understand the application flows. In ACI the terminology may add confusion at first. Just remember that 'Providing' a Contract means providing a service (like TCP/80 inbound) and 'Consuming' a contract allows an EPG to connect to a provided service. Without contracts the endpoints in our EPGs have no connectivity outside their EPG. When we apply our new Contracts we want the endpoints in the App EPG to provide HTTP, ICMP, and SSH to the endpoints in the Web EPG. We also want the endpoints (databases and file servers) in the DB EPG to provide FTP, ICMP, and SSH services to the endpoints in the App EPG.","title":"Step 5: Apply Contracts to EPGs"},{"location":"03_lab_tenant_constructs/#step-51-apply-the-webaccess-podxx-contract-as-a-provided-contract-to-the-app-epg","text":"Navigate to PODXX_Tenant > and expand Tiered-App > Application EPGs > App > Contracts Right click on the Contracts Folder icon and select Add Provided Contract Select WebAccess-POD## contract. No other changes are necessary Click Submit Examine the visual relationship by clicking on your Tiered-App Application Profile","title":"Step 5.1 Apply the WebAccess-PODXX contract as a Provided Contract to the App EPG"},{"location":"03_lab_tenant_constructs/#step-52-apply-the-fileaccess-podxx-contract-as-a-provided-contract-to-the-db-epg","text":"Use the procedure above to apply the FileAccess-POD## contract as a Provided Contract of the DB EPG.","title":"Step 5.2 Apply the FileAccess-PODXX contract as a Provided Contract to the DB EPG"},{"location":"03_lab_tenant_constructs/#check-in","text":"This is quite a long lab so lets check in to see what we have accomplished and what functionality we should have. Once this Lab is complete you should have: A Tenant and associated VRF A Bridge Domain containing three subnets associated with the VRF An Application Profile \"container\" with three EPGs all mapped to the Bridge Domain (one BD but three subnets) Filters describing ICMP SSH HTTP FTP Two contracts describing the allowed connectivity between the App EPG and the Web EPG utilizing the appropriate Filters the allowed connectivity between the APP EPG and the DB EPG utilizing the appropriate Filters The contracts applied to the appropriate EPGs in the appropriate direction ( Provider - Server and Server Services and Consumer - Clients) The necessary Tenant-level logical components have now been created however a few steps remain to achieve connectivity external to the fabric and to Virtualized Hosts and Virtual Machines.","title":"Check in"},{"location":"04_lab_layer2_connectivity/","text":"Lab 04 Configure Host Connectivity Constructs (L2) and Layer 2 Links \u00b6 Context \u00b6 So far we have a working fabric (underlay), working individual interface behaviors and characteristics, and our logical overlay (Tenants, EPGs, etc.). We have no \"Data Plane\" connectivity. That is, while we can manage our fabric and the fabric itself is operational, there is no connectivity to external network devices or hosts. As with any network, this is where you will spend most of your time once the fabric is operational. Lab Goals \u00b6 The primary goal of this lab is to understand the physical and logical steps necessary to achieve connectivity from an ACI fabric to hosts and external networking devices. As with all things in ACI, it is important to understand the relationship between the constructs which abstract the physical layer and the logical. Lets begin with our topology. Understand Interface Constructs and Relationships Step 1: Create an Access Port Step 2: Create a Trunk Port Step 3: Create a L2 vPC Access & Trunk Putting It All Together \u00b6 Access Policies and Tenant objects to achieve connectivity \u00b6 Step 1: Create an Access Port \u00b6 High Level Workflow for Access Port \u00b6 Configure the Physical Layer \u00b6 Configuring Interface to External interface (in this case it will be a switch interface) Configure the port behavior (Port Policy Group) Apply the port behavior to one or more interfaces (Interface Profile and Interface Selector) Associate those interfaces to one or more switches as called for by the design Configure the Logical Layer \u00b6 \u201cConfigure\u201d the EPG Associate the Physical Domain Associate the Static Ports * Confirm default or more specific contracts are in place Step 1.1: Configure the port behavior (Port Policy Group): Fabric > Access Policies > Interfaces > Leaf Interfaces > Policy Groups > Leaf Access Port Create Leaf Access Port Policy Group POD##-PROD-10GAccess-PortPolGrp 10G CDP enabled LLD enabled AEP PODXX Step 1.2: Apply the behavior defined by the port policy group \"ENJ-PROD-10GAccess-PortPolGrp\u201d to one or more interfaces: Fabric > Access Policies > Interfaces > Leaf Interfaces > Profiles Create Leaf Interface Profile POD##-PROD-DB-10GAcc-205-206-P03-IntProf POD##-PROD-DB-10GAcc-205-206-P14-IntProf POD##-PROD-DB-10GAcc-207-208-P16-IntProf POD##-PROD-DB-10GAcc-207-208-P04-IntProf Step 1 - \u00b6 Check in \u00b6","title":"Lab 04 - Host & Layer 2 Connectivity"},{"location":"04_lab_layer2_connectivity/#lab-04-configure-host-connectivity-constructs-l2-and-layer-2-links","text":"","title":"Lab 04 Configure Host Connectivity Constructs (L2) and Layer 2 Links"},{"location":"04_lab_layer2_connectivity/#context","text":"So far we have a working fabric (underlay), working individual interface behaviors and characteristics, and our logical overlay (Tenants, EPGs, etc.). We have no \"Data Plane\" connectivity. That is, while we can manage our fabric and the fabric itself is operational, there is no connectivity to external network devices or hosts. As with any network, this is where you will spend most of your time once the fabric is operational.","title":"Context"},{"location":"04_lab_layer2_connectivity/#lab-goals","text":"The primary goal of this lab is to understand the physical and logical steps necessary to achieve connectivity from an ACI fabric to hosts and external networking devices. As with all things in ACI, it is important to understand the relationship between the constructs which abstract the physical layer and the logical. Lets begin with our topology. Understand Interface Constructs and Relationships Step 1: Create an Access Port Step 2: Create a Trunk Port Step 3: Create a L2 vPC Access & Trunk","title":"Lab Goals"},{"location":"04_lab_layer2_connectivity/#putting-it-all-together","text":"","title":"Putting It All Together"},{"location":"04_lab_layer2_connectivity/#access-policies-and-tenant-objects-to-achieve-connectivity","text":"","title":"Access Policies and Tenant objects to achieve connectivity"},{"location":"04_lab_layer2_connectivity/#step-1-create-an-access-port","text":"","title":"Step 1: Create an Access Port"},{"location":"04_lab_layer2_connectivity/#high-level-workflow-for-access-port","text":"","title":"High Level Workflow for Access Port"},{"location":"04_lab_layer2_connectivity/#configure-the-physical-layer","text":"Configuring Interface to External interface (in this case it will be a switch interface) Configure the port behavior (Port Policy Group) Apply the port behavior to one or more interfaces (Interface Profile and Interface Selector) Associate those interfaces to one or more switches as called for by the design","title":"Configure the Physical Layer"},{"location":"04_lab_layer2_connectivity/#configure-the-logical-layer","text":"\u201cConfigure\u201d the EPG Associate the Physical Domain Associate the Static Ports * Confirm default or more specific contracts are in place Step 1.1: Configure the port behavior (Port Policy Group): Fabric > Access Policies > Interfaces > Leaf Interfaces > Policy Groups > Leaf Access Port Create Leaf Access Port Policy Group POD##-PROD-10GAccess-PortPolGrp 10G CDP enabled LLD enabled AEP PODXX Step 1.2: Apply the behavior defined by the port policy group \"ENJ-PROD-10GAccess-PortPolGrp\u201d to one or more interfaces: Fabric > Access Policies > Interfaces > Leaf Interfaces > Profiles Create Leaf Interface Profile POD##-PROD-DB-10GAcc-205-206-P03-IntProf POD##-PROD-DB-10GAcc-205-206-P14-IntProf POD##-PROD-DB-10GAcc-207-208-P16-IntProf POD##-PROD-DB-10GAcc-207-208-P04-IntProf","title":"Configure the Logical Layer"},{"location":"04_lab_layer2_connectivity/#step-1-","text":"","title":"Step 1 -"},{"location":"04_lab_layer2_connectivity/#check-in","text":"","title":"Check in"},{"location":"05_lab_vmm_integration/","text":"05 Lab Integrate ACI with VMware Using Native Distributed Virtual Switch (DVS) \u00b6 Overview \u00b6 One of the most powerful features of ACI is its native integration with the Virtualization Environment. The steps in this lab will integrate ACI APICs with the VMware environment (already configured) via vCenter so that VMs on ESXi hosts will be able to connect to the Logical ACI environment and take advantage of the security we built in Lab 3. In ACI this configuration construct is called a Virtual Machine Manager (VMM) Domain. This integration allows the ACI APIC to communicate with vCenter as though it were an Administrator and automatically set up the required networking the Virtual Machines will need to communicate securely on the ACI fabric. Required Services and Information Administrator level account on vCenter Admin Username Admin Password The exact name of an existing Data Center configured in vCenter to which we want to integreate In a production environment it is best practice to create a dedicated account for this integration, but in the lab we use the already existing admin account and credentials. This lab specifically calls out the default VMware DVS which is part of ESXi. That is because ACI supports integration with VMware vCenter using: Distributed Virtual Switch (DVS) Native to VMware Cisco Application Virtual Switch AVS Cisco ACI Virtual Edge (AVE) (From ACI 3.1(1) and VMware vCenter 6.0 and later) Next generation virtual switch from Cisco In this lab we will focus on native VDS integration as it is part of VMware. Lab Goals \u00b6 At the completion of this lab, you will have enabled the ACI APIC to communicate to the vCenter managing the ESXi Host in your Lab, configured the apporpriate Fabric Access Objects to allow the correct vlans to present to the ACI managed VMware VDS within vCenter and connect VMs to the correct EPGs via the corresponding Port Groups created by the ACI integration. Step 1 - Configure Access Policies for VMM Domain (Dynamic Vlan Pool and new AAEP) Step 2 - Configure a vCenter VMM Domain Step 3 - Verify Cisco APIC Connection to VMware vCenter Server Step 4 - Verify that the APIC has Provisioned a DVS in vCenter Step 5 - Add ESXi Host to ACI APIC Provisioned DVS Step 6 - Associate VMM Domain to EPGs Step 7 - Test EPG Connectivity Step 1 - Configure Access Policies for VMM Domain (Dynamic Vlan Pool and new AAEP) \u00b6 Step 1.1 - Configure a Dynamic Vlan Pool \u00b6 Navigate to Fabric > Access Policies > VLAN and right-click on the VLAN folder icon and select Create VLAN Pool Allocation Mode: Dynamic Allocation VLAN Range: Value Allocation Mode Range (From) 3##1 Dynamic Range (To) 3##9 Dynamic * Replace ## with your assigned 2-digit Pod number Note: Vlan 3##0 is part of the Static Pool Click the plus sign (+) in the Encap Blocks table to configure the range. Allocation Mode: Inherit allocMode from parent Role: External or On the wire encapsulations You will recall creating a static Vlan Pool in Lab 2. With dynamic allocation, the APIC will automatically assign VLANS as needed within the range you define. This facilitates automation as well as eases the burden of configuration. The role defines the use of the VLAN range. External or On the wire encapsulation is used for allocating VLANS for each EPG associated to the VMM domain. The VLANs are used when packets are sent to or from Leaf switches. The internal role is used for private VLAN allocations in the internal vSwitch by the Cisco ACI Virtual Edge (AVE). With the Intenral role, the VLANS are not seen outside of the ESXi host or on the wire. Click OK to return to the pool configuration dialog and click Submit. Step 1.2 - Configure a new AAEP \u00b6 In Lab 2 we configured Configure AAEP to Selectively Allow Vlan Traffic. Attachable Access Entity Profiles (AAEPs) can be considered the \"where\" of the fabric configuration and are used to group domains with similar requirements. They allow a one to many relationship between the policy groups and domains. AEPs are tied to interface policy groups. One ore more domains are added to an AAEP. By grouping domains into AAEPs and associating them, the fabric knows where the various devices in the domain reside. Cisco APIC can push the Vlans and policy to the required interfaces. Step 1.3 - Host Connectivity to VDS Step 2 - Configure a vCenter VMM Domain \u00b6 Its important to remember that what you are actually configuring with a VMM Domain is a virtual switch on the Hypervisor(s) (ESXi) via the hypervisor manager (vCenter in our case). Step 2.1 - vCenter Domain \u00b6 Navigate to Virtual Networking > VMM Domains > VMware and right-click on the VMware folder and select Create vCenter Domain . A Create vCenter Domain dialog will pop up. Notice the various Hypervisors which can be integrated into ACI. Setting: Value Comments Virtual Switch Name: POD11-vCenter-VDS Virtual Switch: VMware vSphere Distributed Switch Unchanged This is the default value Associated Attachable Entity Profile: Leave Blank Access Mode: Read Write Mode Unchanged Read Write Mode is the default Endpoint Retention Time (seconds) 0 Unchanged This is the default VLAN Pool: POD##-Dynamic-VLAN-Pool Configured in Lab 2 Enter the name: POD##-vCenter-VDS Make sure that VMware vSphere Distributed Switch is selected Leave the Associated Attachable Entity Profile (AAEP) empty. You will define it in a later procedure. Choose your dynamic VLAN pool (POD##-VLANs). Step 2.2 - vCenter Credentials \u00b6 Click the plus sign (+) in the vCenter Credentials: table to define credentials with these settings. Name: vCenter Username Password POD##-vCenter-Credentials administrator@vsphere.local 1234QWer! Click OK Step 2.3 - vCenter Server \u00b6 Click the plus sign (+) in the vCenter: table to define the controller settings (IP, etc.). Name: POD##-vCenter-Server The vCenter controller name does not have to match the name of the vCenter domain. Either the IP or the hostname can be entered. Device Management IP Address Username Password vCenter-P1 192.168.10.202 administrator@vsphere.local 1234QWer! vCenter-P2 192.168.10.204 administrator@vsphere.local 1234QWer! vCenter-P3 192.168.10.206 administrator@vsphere.local 1234QWer! vCenter-P4 192.168.10.208 administrator@vsphere.local 1234QWer! vCenter-P5 192.168.10.210 administrator@vsphere.local 1234QWer! Setting: Value Comments DVS Version: DVS Version 6 If you choose the default DVS version (6.5) you would not be able to add the hypervisor with version DVS 6.5 due to a VMware bug Stats Collection: Disabled This is the default value Data center: DC The data center name must exactly match the data center name as it is defined in vCenter Management EPG: You do not configure any EPG for managing the VMware vCenter because the connection from the Cisco APIC to the vCenter is out-of-band (OOB) Associated Credentials: POD##-vCenter-Credentials Set vSwitch Policy to CDP, leave all other settings at their default values Click Submit Step 3 - Verify Cisco APIC Connection to VMware vCenter Server \u00b6 Step 3.1 Verify that the APIC has discovered vCenter \u00b6 Navigate to Virtual Networking > VMM Domains > VMware and expand your vCenter domain and all of its subelements. Note: The APIC connects to the vCenter and obtains its inventory, including hypervisors, VMs, and uplinks. You will see all the VMs that have been installed on your host. Examine the status of the vmnic interfaces. You can over over them in the Topology page, click them in the naviation pane, or go to the General tab. vmnic 0,2, and 3 should be up. VMNIC Map for POD## VMNIC # Connected To Function 0 Management Switch Management 2 LEAF-1 Data Path 3 Leaf-2 Data Path Step 4 - Verify that the APIC has Provisioned a DVS in vCenter \u00b6 Step 4.1 Accessing vCenter \u00b6 In Google Chrome open another tab and connect to vCenter via https://vcenter. The hostname will resolve to 192.168.10.50 Accept the untrusted certificate security warnings and clikc vSphere Web Client (Flash) If prompted, enable Adobe Flash Playre by clicking its button and choosing Allow Accept any security warnings and log in as administrator@vsphere.local with password 1234QWer! Go to Networking . Expand the folder that has been created under your data center (DC). You should see a DVS wit the name of the configured vCenter domain (POD##-vCenter-vDS), within a folder of the same name. Expand the DVS to see two networks have been automatically created. Click the Summary tab to see details about the DVS. vCenter can take up to 15 minutes upon bootup to be ready. When vCenter vecomes active and reachable you will be able to see the elements.","title":"Lab 05 - Integration with VMware ESXi"},{"location":"05_lab_vmm_integration/#05-lab-integrate-aci-with-vmware-using-native-distributed-virtual-switch-dvs","text":"","title":"05 Lab Integrate ACI with VMware Using Native Distributed Virtual Switch (DVS)"},{"location":"05_lab_vmm_integration/#overview","text":"One of the most powerful features of ACI is its native integration with the Virtualization Environment. The steps in this lab will integrate ACI APICs with the VMware environment (already configured) via vCenter so that VMs on ESXi hosts will be able to connect to the Logical ACI environment and take advantage of the security we built in Lab 3. In ACI this configuration construct is called a Virtual Machine Manager (VMM) Domain. This integration allows the ACI APIC to communicate with vCenter as though it were an Administrator and automatically set up the required networking the Virtual Machines will need to communicate securely on the ACI fabric. Required Services and Information Administrator level account on vCenter Admin Username Admin Password The exact name of an existing Data Center configured in vCenter to which we want to integreate In a production environment it is best practice to create a dedicated account for this integration, but in the lab we use the already existing admin account and credentials. This lab specifically calls out the default VMware DVS which is part of ESXi. That is because ACI supports integration with VMware vCenter using: Distributed Virtual Switch (DVS) Native to VMware Cisco Application Virtual Switch AVS Cisco ACI Virtual Edge (AVE) (From ACI 3.1(1) and VMware vCenter 6.0 and later) Next generation virtual switch from Cisco In this lab we will focus on native VDS integration as it is part of VMware.","title":"Overview"},{"location":"05_lab_vmm_integration/#lab-goals","text":"At the completion of this lab, you will have enabled the ACI APIC to communicate to the vCenter managing the ESXi Host in your Lab, configured the apporpriate Fabric Access Objects to allow the correct vlans to present to the ACI managed VMware VDS within vCenter and connect VMs to the correct EPGs via the corresponding Port Groups created by the ACI integration. Step 1 - Configure Access Policies for VMM Domain (Dynamic Vlan Pool and new AAEP) Step 2 - Configure a vCenter VMM Domain Step 3 - Verify Cisco APIC Connection to VMware vCenter Server Step 4 - Verify that the APIC has Provisioned a DVS in vCenter Step 5 - Add ESXi Host to ACI APIC Provisioned DVS Step 6 - Associate VMM Domain to EPGs Step 7 - Test EPG Connectivity","title":"Lab Goals"},{"location":"05_lab_vmm_integration/#step-1-configure-access-policies-for-vmm-domain-dynamic-vlan-pool-and-new-aaep","text":"","title":"Step 1 - Configure Access Policies for VMM Domain  (Dynamic Vlan Pool and new AAEP)"},{"location":"05_lab_vmm_integration/#step-11-configure-a-dynamic-vlan-pool","text":"Navigate to Fabric > Access Policies > VLAN and right-click on the VLAN folder icon and select Create VLAN Pool Allocation Mode: Dynamic Allocation VLAN Range: Value Allocation Mode Range (From) 3##1 Dynamic Range (To) 3##9 Dynamic * Replace ## with your assigned 2-digit Pod number Note: Vlan 3##0 is part of the Static Pool Click the plus sign (+) in the Encap Blocks table to configure the range. Allocation Mode: Inherit allocMode from parent Role: External or On the wire encapsulations You will recall creating a static Vlan Pool in Lab 2. With dynamic allocation, the APIC will automatically assign VLANS as needed within the range you define. This facilitates automation as well as eases the burden of configuration. The role defines the use of the VLAN range. External or On the wire encapsulation is used for allocating VLANS for each EPG associated to the VMM domain. The VLANs are used when packets are sent to or from Leaf switches. The internal role is used for private VLAN allocations in the internal vSwitch by the Cisco ACI Virtual Edge (AVE). With the Intenral role, the VLANS are not seen outside of the ESXi host or on the wire. Click OK to return to the pool configuration dialog and click Submit.","title":"Step 1.1 - Configure a Dynamic Vlan Pool"},{"location":"05_lab_vmm_integration/#step-12-configure-a-new-aaep","text":"In Lab 2 we configured Configure AAEP to Selectively Allow Vlan Traffic. Attachable Access Entity Profiles (AAEPs) can be considered the \"where\" of the fabric configuration and are used to group domains with similar requirements. They allow a one to many relationship between the policy groups and domains. AEPs are tied to interface policy groups. One ore more domains are added to an AAEP. By grouping domains into AAEPs and associating them, the fabric knows where the various devices in the domain reside. Cisco APIC can push the Vlans and policy to the required interfaces. Step 1.3 - Host Connectivity to VDS","title":"Step 1.2 - Configure a new AAEP"},{"location":"05_lab_vmm_integration/#step-2-configure-a-vcenter-vmm-domain","text":"Its important to remember that what you are actually configuring with a VMM Domain is a virtual switch on the Hypervisor(s) (ESXi) via the hypervisor manager (vCenter in our case).","title":"Step 2 - Configure a vCenter VMM Domain"},{"location":"05_lab_vmm_integration/#step-21-vcenter-domain","text":"Navigate to Virtual Networking > VMM Domains > VMware and right-click on the VMware folder and select Create vCenter Domain . A Create vCenter Domain dialog will pop up. Notice the various Hypervisors which can be integrated into ACI. Setting: Value Comments Virtual Switch Name: POD11-vCenter-VDS Virtual Switch: VMware vSphere Distributed Switch Unchanged This is the default value Associated Attachable Entity Profile: Leave Blank Access Mode: Read Write Mode Unchanged Read Write Mode is the default Endpoint Retention Time (seconds) 0 Unchanged This is the default VLAN Pool: POD##-Dynamic-VLAN-Pool Configured in Lab 2 Enter the name: POD##-vCenter-VDS Make sure that VMware vSphere Distributed Switch is selected Leave the Associated Attachable Entity Profile (AAEP) empty. You will define it in a later procedure. Choose your dynamic VLAN pool (POD##-VLANs).","title":"Step 2.1 - vCenter Domain"},{"location":"05_lab_vmm_integration/#step-22-vcenter-credentials","text":"Click the plus sign (+) in the vCenter Credentials: table to define credentials with these settings. Name: vCenter Username Password POD##-vCenter-Credentials administrator@vsphere.local 1234QWer! Click OK","title":"Step 2.2 - vCenter Credentials"},{"location":"05_lab_vmm_integration/#step-23-vcenter-server","text":"Click the plus sign (+) in the vCenter: table to define the controller settings (IP, etc.). Name: POD##-vCenter-Server The vCenter controller name does not have to match the name of the vCenter domain. Either the IP or the hostname can be entered. Device Management IP Address Username Password vCenter-P1 192.168.10.202 administrator@vsphere.local 1234QWer! vCenter-P2 192.168.10.204 administrator@vsphere.local 1234QWer! vCenter-P3 192.168.10.206 administrator@vsphere.local 1234QWer! vCenter-P4 192.168.10.208 administrator@vsphere.local 1234QWer! vCenter-P5 192.168.10.210 administrator@vsphere.local 1234QWer! Setting: Value Comments DVS Version: DVS Version 6 If you choose the default DVS version (6.5) you would not be able to add the hypervisor with version DVS 6.5 due to a VMware bug Stats Collection: Disabled This is the default value Data center: DC The data center name must exactly match the data center name as it is defined in vCenter Management EPG: You do not configure any EPG for managing the VMware vCenter because the connection from the Cisco APIC to the vCenter is out-of-band (OOB) Associated Credentials: POD##-vCenter-Credentials Set vSwitch Policy to CDP, leave all other settings at their default values Click Submit","title":"Step 2.3 - vCenter Server"},{"location":"05_lab_vmm_integration/#step-3-verify-cisco-apic-connection-to-vmware-vcenter-server","text":"","title":"Step 3 - Verify Cisco APIC Connection to VMware vCenter Server"},{"location":"05_lab_vmm_integration/#step-31-verify-that-the-apic-has-discovered-vcenter","text":"Navigate to Virtual Networking > VMM Domains > VMware and expand your vCenter domain and all of its subelements. Note: The APIC connects to the vCenter and obtains its inventory, including hypervisors, VMs, and uplinks. You will see all the VMs that have been installed on your host. Examine the status of the vmnic interfaces. You can over over them in the Topology page, click them in the naviation pane, or go to the General tab. vmnic 0,2, and 3 should be up. VMNIC Map for POD## VMNIC # Connected To Function 0 Management Switch Management 2 LEAF-1 Data Path 3 Leaf-2 Data Path","title":"Step 3.1 Verify that the APIC has discovered vCenter"},{"location":"05_lab_vmm_integration/#step-4-verify-that-the-apic-has-provisioned-a-dvs-in-vcenter","text":"","title":"Step 4 - Verify that the APIC has Provisioned a DVS in vCenter"},{"location":"05_lab_vmm_integration/#step-41-accessing-vcenter","text":"In Google Chrome open another tab and connect to vCenter via https://vcenter. The hostname will resolve to 192.168.10.50 Accept the untrusted certificate security warnings and clikc vSphere Web Client (Flash) If prompted, enable Adobe Flash Playre by clicking its button and choosing Allow Accept any security warnings and log in as administrator@vsphere.local with password 1234QWer! Go to Networking . Expand the folder that has been created under your data center (DC). You should see a DVS wit the name of the configured vCenter domain (POD##-vCenter-vDS), within a folder of the same name. Expand the DVS to see two networks have been automatically created. Click the Summary tab to see details about the DVS. vCenter can take up to 15 minutes upon bootup to be ready. When vCenter vecomes active and reachable you will be able to see the elements.","title":"Step 4.1 Accessing vCenter"},{"location":"06_lab_layer3_connectivity/","text":"","title":"Lab 06 - External Layer 3 Connectivity"},{"location":"07_lab_operations/","text":"","title":"Lab 07 - Management & Operations"},{"location":"08_lab_fwl_integration/","text":"","title":"Lab 08 - Firewall Integration"},{"location":"09_lab_troubeleshooting/","text":"","title":"Lab 09 - Troubleshooting"},{"location":"10_lab_programmability/","text":"ACI Programmability \u00b6 Visore Postman Arya ACI Toolkit Cobra SDK Python Ansible","title":"Lab 10 - Programmability"},{"location":"10_lab_programmability/#aci-programmability","text":"Visore Postman Arya ACI Toolkit Cobra SDK Python Ansible","title":"ACI Programmability"},{"location":"11_lab_appcenter/","text":"","title":"Lab 11 - App Center and ACI Optimize Feature"},{"location":"12_lab_techsupport/","text":"Lab 12 Export Tech Support and Audit Logs \u00b6 Overview \u00b6 While ACI is now a mature technology, there are occasions when you will need to contact Cisco TAC for support. Its always a good idea to have your current configuration documented and ACI makes that very simple. In Lab 07, we configured a remote location for daily backups and off-fabric snapshots. Feel free to use that remote location for this lab or if you want to practice create a new FTP server to export what is the equivalent of show tech. Lab Goals \u00b6 Collect and export on-demand Tech Support \u00b6 Step 1 - Configure a remote location for the FTP service on your Student PC Step 2 - Review the two types of Tech Support export policies available eon the APIC Step 3 - Verify there are no files in the FTP root directory on your Student PC Step 4 - Enable the FTP Server on StudentPC Step 5 - Enable the Tech Support export Step 6 - Collect Tech Support Report Step 7 - Check Export status Query and Save Audit logs \u00b6 Step 1 - SSH to APIC Step 2 - Redirect Output to a Text File Step 3 - View the saved log Collect and export on-demand Tech Support \u00b6 Generate and export on demand tech support data. Step 1 - Configure a remote location for the FTP service on your Student PC \u00b6 Navigate to Admin > Import/Export > Remote Locations . Right click on the Remote Locations folder icon and select Create Remote Location . Note: You will see some other remote locations that are used for loading the Cisco APIC configurations. Configure the properties of the Remote location. Field Value Name: Remote_TechSupportExports IP Address: 192.168.10.15 Protocol: FTP Credentials: admin/1234QWer Access: out-of-band Click Submit Step 2 - Review the two types of Tech Suport export policies available eon the APIC \u00b6 Navigate to Admin > Import/Export > Export Policies . Expand the Tech Support and On-Demand Tech Support menus. You should see a policy default for each type. You should also see additional menus for specialized Tech Support data. Tech Support allows you to schedule an export periodically. On-demand Tech Support, as you would expect, allows you to export on-demand. Step 3 - Verify there are no files in the FTP root directory on your Student PC \u00b6 View the contents of the C:\\ACI folder to confirm there are no files. Step 4 - Enable the FTP Server on StudentPC \u00b6 Start the 3CDaemon application on your StudentPC. This will enable the FTP server. Step 5 - Enable the Tech Support export \u00b6 Configure the On-demand Tech Support default policy to use the remote location defined in Step 1 and include information from all fabric nodes by leaving the Category check box checked (the default value) for All nodes. Click Submit Step 6 - Collect Tech Support Report \u00b6 Click the tools symbol in the top-right corner, choose Collect Tech Supports and then click Yes to confirm thee action Step 7 - Check Export Status \u00b6 Click on the Operational tab in the main panel to monitor export status. After a few minutes, the Detail STatus should start changing from \"Collecting tech support form nxos\" to \"Preparing the compress logs and export\". Once the status shows a check mark and the Detailed Status is \"Task Completed\" its time to verity the export Step 8 Verify TechSupport Export on StudentPC On your StudentPC, check the contents of the C:\\ACI folder. It should contain a sub-directory (tsod-default) with the compressed exported gzipped tar files Query and Save Audit logs \u00b6 Cisco ACI fabric accounting is handled by the same mechanism that handles faults and evens. These two managed objects (MO) represent fabric accounting: aaaSessionLR This managed object tracks user account sessions (login/logout) on the APIC and switches aaaModLR This managed object tracks the changes made by users to objects. If the AAA server is not pingable, it is marked as unavailable and a fault is seen. Both of these event logs are stored in the APIC shards. Once the data exceeds the pre-set storage allocation size, it overwrites records on a first-in first-out basis The standard syslog, callhome, REST query, and CLI export mechanism are fully supported for queries to these two MOs , however, there is no default policy to export this data. For this the moquery CLI command must be used. Step 1 - SSH to APIC \u00b6 Launch PUTTY from your StudentPC Desktop and connect to your Cisco APIC with the usual credentials. Use the moquery utility to display the aaaModLR audit logs (all objects of the class aaaModLR). The process of retrieving and outputting will take several minutes so you may terminate the PUTTY window with Ctrl+C. apic1#moquery -c aaaModLR apic1#moquery -c aaaSessionLR Step 2 - Redirect Output to a Text File \u00b6 Redirect the audit log output to a text file (/tmp/AuditLogMod.txt). It will take a few minutes to complete. apic1# moquery -c aaaSessionLR > /tmp/AuditLogSession.txt & [ 1 ] 29187 apic1# moquery -c aaaModLR > /tmp/AuditLogMod.txt & [ 2 ] 29605 apic1# [ 1 ] - done _exec_legacy_cmd \"/controller/bin/moquery\" \" $@ \" -c aaaSessionLR > apic1# [ 2 ] + done _exec_legacy_cmd \"/controller/bin/moquery\" \" $@ \" -c aaaModLR > apic1# Note: including the ampersand (&) at the end of the command allows the process to run in the background so you can continue to use the shell. Once the background processes are \"done\", you can check the size of the files apic1# ls -al /tmp/*.txt -rw-r--r-- 1 admin admin 411981 Aug 14 21 :35 /tmp/AuditLogMod.txt -rw-r--r-- 1 admin admin 410710 Aug 14 21 :35 /tmp/AuditLogSession.txt -rw-r--r-- 1 root root 2020 Aug 14 21 :44 /tmp/standby-av-fixup-output.txt apic1# Step 3 - View the saved log \u00b6 In approximately 8 minutes you can view the audit log. Normally, once the output is complete you would copy the file off the APIC to view. Command and example output \u00b6 apic1# more /tmp/AuditLogSession.txt Total Objects shown: 703 # aaa.SessionLR id : 4294967969 affected : uni/userext/user-admin cause : unknown changeSet : childAction : clientTag : code : generic created : 2020 -08-14T18:40:49.634+00:00 descr : From-192.168.10.40-client-type-REST-Success dn : subj- [ uni/userext/user-admin ] /sess-4294967969 ind : special modTs : never rn : sess-4294967969 sessionId : roRTGvO5SbiiorzyJcFT4w == severity : info status : systemId : 1 trig : refresh,session txId : 0 user : admin --More-- ( 0 % )","title":"Lab 12 - Export Tech Support"},{"location":"12_lab_techsupport/#lab-12-export-tech-support-and-audit-logs","text":"","title":"Lab 12 Export Tech Support and Audit Logs"},{"location":"12_lab_techsupport/#overview","text":"While ACI is now a mature technology, there are occasions when you will need to contact Cisco TAC for support. Its always a good idea to have your current configuration documented and ACI makes that very simple. In Lab 07, we configured a remote location for daily backups and off-fabric snapshots. Feel free to use that remote location for this lab or if you want to practice create a new FTP server to export what is the equivalent of show tech.","title":"Overview"},{"location":"12_lab_techsupport/#lab-goals","text":"","title":"Lab Goals"},{"location":"12_lab_techsupport/#collect-and-export-on-demand-tech-support","text":"Step 1 - Configure a remote location for the FTP service on your Student PC Step 2 - Review the two types of Tech Support export policies available eon the APIC Step 3 - Verify there are no files in the FTP root directory on your Student PC Step 4 - Enable the FTP Server on StudentPC Step 5 - Enable the Tech Support export Step 6 - Collect Tech Support Report Step 7 - Check Export status","title":"Collect and export on-demand Tech Support"},{"location":"12_lab_techsupport/#query-and-save-audit-logs","text":"Step 1 - SSH to APIC Step 2 - Redirect Output to a Text File Step 3 - View the saved log","title":"Query and Save Audit logs"},{"location":"12_lab_techsupport/#collect-and-export-on-demand-tech-support_1","text":"Generate and export on demand tech support data.","title":"Collect and export on-demand Tech Support"},{"location":"12_lab_techsupport/#step-1-configure-a-remote-location-for-the-ftp-service-on-your-student-pc","text":"Navigate to Admin > Import/Export > Remote Locations . Right click on the Remote Locations folder icon and select Create Remote Location . Note: You will see some other remote locations that are used for loading the Cisco APIC configurations. Configure the properties of the Remote location. Field Value Name: Remote_TechSupportExports IP Address: 192.168.10.15 Protocol: FTP Credentials: admin/1234QWer Access: out-of-band Click Submit","title":"Step 1 - Configure a remote location for the FTP service on your Student PC"},{"location":"12_lab_techsupport/#step-2-review-the-two-types-of-tech-suport-export-policies-available-eon-the-apic","text":"Navigate to Admin > Import/Export > Export Policies . Expand the Tech Support and On-Demand Tech Support menus. You should see a policy default for each type. You should also see additional menus for specialized Tech Support data. Tech Support allows you to schedule an export periodically. On-demand Tech Support, as you would expect, allows you to export on-demand.","title":"Step 2 - Review the two types of Tech Suport export policies available eon the APIC"},{"location":"12_lab_techsupport/#step-3-verify-there-are-no-files-in-the-ftp-root-directory-on-your-student-pc","text":"View the contents of the C:\\ACI folder to confirm there are no files.","title":"Step 3 - Verify there are no files in the FTP root directory on your Student PC"},{"location":"12_lab_techsupport/#step-4-enable-the-ftp-server-on-studentpc","text":"Start the 3CDaemon application on your StudentPC. This will enable the FTP server.","title":"Step 4 -  Enable the FTP Server on StudentPC"},{"location":"12_lab_techsupport/#step-5-enable-the-tech-support-export","text":"Configure the On-demand Tech Support default policy to use the remote location defined in Step 1 and include information from all fabric nodes by leaving the Category check box checked (the default value) for All nodes. Click Submit","title":"Step 5 - Enable the Tech Support export"},{"location":"12_lab_techsupport/#step-6-collect-tech-support-report","text":"Click the tools symbol in the top-right corner, choose Collect Tech Supports and then click Yes to confirm thee action","title":"Step 6 - Collect Tech Support Report"},{"location":"12_lab_techsupport/#step-7-check-export-status","text":"Click on the Operational tab in the main panel to monitor export status. After a few minutes, the Detail STatus should start changing from \"Collecting tech support form nxos\" to \"Preparing the compress logs and export\". Once the status shows a check mark and the Detailed Status is \"Task Completed\" its time to verity the export Step 8 Verify TechSupport Export on StudentPC On your StudentPC, check the contents of the C:\\ACI folder. It should contain a sub-directory (tsod-default) with the compressed exported gzipped tar files","title":"Step 7 - Check Export Status"},{"location":"12_lab_techsupport/#query-and-save-audit-logs_1","text":"Cisco ACI fabric accounting is handled by the same mechanism that handles faults and evens. These two managed objects (MO) represent fabric accounting: aaaSessionLR This managed object tracks user account sessions (login/logout) on the APIC and switches aaaModLR This managed object tracks the changes made by users to objects. If the AAA server is not pingable, it is marked as unavailable and a fault is seen. Both of these event logs are stored in the APIC shards. Once the data exceeds the pre-set storage allocation size, it overwrites records on a first-in first-out basis The standard syslog, callhome, REST query, and CLI export mechanism are fully supported for queries to these two MOs , however, there is no default policy to export this data. For this the moquery CLI command must be used.","title":"Query and Save Audit logs"},{"location":"12_lab_techsupport/#step-1-ssh-to-apic","text":"Launch PUTTY from your StudentPC Desktop and connect to your Cisco APIC with the usual credentials. Use the moquery utility to display the aaaModLR audit logs (all objects of the class aaaModLR). The process of retrieving and outputting will take several minutes so you may terminate the PUTTY window with Ctrl+C. apic1#moquery -c aaaModLR apic1#moquery -c aaaSessionLR","title":"Step 1 - SSH to APIC"},{"location":"12_lab_techsupport/#step-2-redirect-output-to-a-text-file","text":"Redirect the audit log output to a text file (/tmp/AuditLogMod.txt). It will take a few minutes to complete. apic1# moquery -c aaaSessionLR > /tmp/AuditLogSession.txt & [ 1 ] 29187 apic1# moquery -c aaaModLR > /tmp/AuditLogMod.txt & [ 2 ] 29605 apic1# [ 1 ] - done _exec_legacy_cmd \"/controller/bin/moquery\" \" $@ \" -c aaaSessionLR > apic1# [ 2 ] + done _exec_legacy_cmd \"/controller/bin/moquery\" \" $@ \" -c aaaModLR > apic1# Note: including the ampersand (&) at the end of the command allows the process to run in the background so you can continue to use the shell. Once the background processes are \"done\", you can check the size of the files apic1# ls -al /tmp/*.txt -rw-r--r-- 1 admin admin 411981 Aug 14 21 :35 /tmp/AuditLogMod.txt -rw-r--r-- 1 admin admin 410710 Aug 14 21 :35 /tmp/AuditLogSession.txt -rw-r--r-- 1 root root 2020 Aug 14 21 :44 /tmp/standby-av-fixup-output.txt apic1#","title":"Step 2 - Redirect Output to a Text File"},{"location":"12_lab_techsupport/#step-3-view-the-saved-log","text":"In approximately 8 minutes you can view the audit log. Normally, once the output is complete you would copy the file off the APIC to view.","title":"Step 3 - View the saved log"},{"location":"12_lab_techsupport/#command-and-example-output","text":"apic1# more /tmp/AuditLogSession.txt Total Objects shown: 703 # aaa.SessionLR id : 4294967969 affected : uni/userext/user-admin cause : unknown changeSet : childAction : clientTag : code : generic created : 2020 -08-14T18:40:49.634+00:00 descr : From-192.168.10.40-client-type-REST-Success dn : subj- [ uni/userext/user-admin ] /sess-4294967969 ind : special modTs : never rn : sess-4294967969 sessionId : roRTGvO5SbiiorzyJcFT4w == severity : info status : systemId : 1 trig : refresh,session txId : 0 user : admin --More-- ( 0 % )","title":"Command and example output"},{"location":"lab_access/","text":"Getting Around the Lab \u00b6 How this lab works and things to keep in mind... Lab Access \u00b6 Each student is assigned their own lab \"Pod\" (Tenant) in the fabric. Please refer to the Student Pod Table below for your Remote Desktop (RDP) IP Address and Credentials. Access to the lab environment is via the Remote Desktop Protocol (RDP) and so you will need an RDP client on your system in order to access the lab. Student Pod Table \u00b6 POD Number RDP IP RDP Port Username Password 11 65.49.10.72 7779 Claudia 1234QWer! Step 1 - RDP Access to Student PC \u00b6 Login with your credentials from the Student Pod Table Operating System Details Windows 10 From the Search box Type run and <Enter> In the Run dialog type mstsc /admin <Enter> Macintosh Microsoft Remote Desktop client on the Mac App Store See Microsoft Get started with the macOS client Tip: If you are behind a corporate firewall you may want to use Web RDP Web RDP Details: https://65.49.89.250/#/ POD Number Username Password 11 ACI-POD11 $VGi@IQezt1 Step 2 - Applications \u00b6 Once you sucesfully log in (Tip: Don't forget the RDP port number) you will see a customized Desktop which will have all the tools you need and from where you can access all the devices. Chrome Putty This lab guide is also available on the Desktop. IMPORTANT: Please review before continuing with the lab. \u00b6 LAB PREMISE \u00b6 This Lab has a total of 16 Pods. Some equipment is shared across all the Pods while other equipment is dedicated to each individual Pod. A Pod is a group of devices and resources which make an individual Lab usable for each student. Each Pod is identified by a two digit number. Throghout your lab guide, if you see ##, replace ## with your Pod number. ACI is fundamentally a multi-tenant environment and the lab environment makes full use of that capability. Each Student will create their own Tenant in their Pod and map all the policies, test servers, and equipment dedicated to the Pod to their own tenant. For example: The student assigned to POD11 will create their own POD11 tenant and configure access policies and virtual networking policies to the POD11 tenant. A naming standard is particularly important in an ACI Design. All policies will follow a naming standard based on your POD number. Common Equipment \u00b6 The table below details the shared equipment in the lab. This equipment will be shared by all students. Because these resources are shared, you will see configuration appear that is not your own. Please do not delete or change any configuration item that is not your own. Please be respectful of the other students and use only your POD resources. Table of Common (Shared) Devices and Access Information Device Management/Terminal Server IP Telnet Port Number Credentials APIC (apic.dc.local) 192.168.10.1 admin/1234QWer spine 192.168.250.202 7006 admin/1234QWer leaf-1 192.168.250.202 7007 admin/1234QWer leaf-2 192.168.250.202 7008 admin/1234QWer L2/L3 Switch 192.168.250.202 7009 admin/1234QWer AD/DNS/FTP Server 192.168.10.40 admin/1234QWer NTP Server 192.168.10.40 Dedicated Equipment \u00b6 This equipment is dedicated to every individual student. Each Student Pod has a dedicated ESXi Host, vCenter and a set of Linux based Virtual Machines. POD11 Equipment Device Management IP Fabric IP/ FQDN Username Password Linux VMs WEB 10.0.1.1/24 Root 1234Qwer APP 10.0.2.1/24 Roo 1234Qwer DB 10.0.3.1/24 Root 1234Qwer TRANSACT 10.0.4.1/24 Root 1234Qwer Virtualization Environment ESXi Host 192.168.10.211 ESXp11@dc.local Root 1234QWer vCenter 192.168.10.212 vcenterpod16.dc.local administrator@vsphere.local 1234QWer! Credentials Summary \u00b6 For ease of use, the Lab has minimized the number of credentials and uses a standard pattern for the password with minor variations. The table below summarizes all the credentilas you will need to access all the lab resources. Device Type Username/Password Network Devices (Including APIC) admin/1234QWer Linux Virtual Machines Root/1234Qwer ESXi Host Root/1234QWer vCenter administrator@vsphere.local/1234QWer! Physical Interface Reference \u00b6 ACI is fundamentally a networking technology and so throughout the labs you will need to configure interfaces. Use the Physical Interface Table below as a reference. SPINE-1 \u00b6 POD# PORT Connected To Device Connected to Device Port ALL PODS e1/1 leaf-1 e1/49 ALL PODS e1/2 leaf-2 e1/49 LEAF-1 \u00b6 POD# PORT Connected To Device Connected to Device Port ALL PODS e1/2 APIC VIC1 POD11 e1/3 UCS-SERVER-P11 VIC1 POD11 e1/11 ACI-P2-TOR (Cat 3750) Gi1/0/1 ALL PODS e1/49 SPINE-1 e1/1 LEAF-2 \u00b6 POD# PORT Connected To Device Connected to Device Port ALL PODS e1/2 APIC VIC2 POD11 e1/3 UCS-SERVER-P11 VIC2 POD11 e1/11 ACI-P2-TOR (Cat 3750) Gi1/0/2 ALL PODS e1/49 SPINE-1 e1/2 Layer 2 and Layer 3 Logical Configuration \u00b6 L3Out IP and Vlan Details \u00b6 POD Number OSPF AREA LEAF-1 Interface OSPF VLAN SVI on Layer 3 Switch SVI on APIC VLAN Pool Start VLAN Pool End POD11 11 e1/11 1112 172.16.11.2/30 172.16.11.1/30 1110 1119 Layer 2 Details \u00b6 POD Number SVI on External Layer 3 Device LEAF-2 Interface Layer 2 VLANS VLAN Pool Start VLAN Pool End POD11 10.0.2.99 e1/11 112 110 119 ASA Management Details \u00b6 POD Number ASA Management IP POD11 192.168.10.71 Lets get started with ACI! \u00b6","title":"Getting Around the Lab"},{"location":"lab_access/#getting-around-the-lab","text":"How this lab works and things to keep in mind...","title":"Getting Around the Lab"},{"location":"lab_access/#lab-access","text":"Each student is assigned their own lab \"Pod\" (Tenant) in the fabric. Please refer to the Student Pod Table below for your Remote Desktop (RDP) IP Address and Credentials. Access to the lab environment is via the Remote Desktop Protocol (RDP) and so you will need an RDP client on your system in order to access the lab.","title":"Lab Access"},{"location":"lab_access/#student-pod-table","text":"POD Number RDP IP RDP Port Username Password 11 65.49.10.72 7779 Claudia 1234QWer!","title":"Student Pod Table"},{"location":"lab_access/#step-1-rdp-access-to-student-pc","text":"Login with your credentials from the Student Pod Table Operating System Details Windows 10 From the Search box Type run and <Enter> In the Run dialog type mstsc /admin <Enter> Macintosh Microsoft Remote Desktop client on the Mac App Store See Microsoft Get started with the macOS client Tip: If you are behind a corporate firewall you may want to use Web RDP Web RDP Details: https://65.49.89.250/#/ POD Number Username Password 11 ACI-POD11 $VGi@IQezt1","title":"Step 1 - RDP Access to Student PC"},{"location":"lab_access/#step-2-applications","text":"Once you sucesfully log in (Tip: Don't forget the RDP port number) you will see a customized Desktop which will have all the tools you need and from where you can access all the devices. Chrome Putty This lab guide is also available on the Desktop.","title":"Step 2 - Applications"},{"location":"lab_access/#important-please-review-before-continuing-with-the-lab","text":"","title":"IMPORTANT:  Please review before continuing with the lab."},{"location":"lab_access/#lab-premise","text":"This Lab has a total of 16 Pods. Some equipment is shared across all the Pods while other equipment is dedicated to each individual Pod. A Pod is a group of devices and resources which make an individual Lab usable for each student. Each Pod is identified by a two digit number. Throghout your lab guide, if you see ##, replace ## with your Pod number. ACI is fundamentally a multi-tenant environment and the lab environment makes full use of that capability. Each Student will create their own Tenant in their Pod and map all the policies, test servers, and equipment dedicated to the Pod to their own tenant. For example: The student assigned to POD11 will create their own POD11 tenant and configure access policies and virtual networking policies to the POD11 tenant. A naming standard is particularly important in an ACI Design. All policies will follow a naming standard based on your POD number.","title":"LAB PREMISE"},{"location":"lab_access/#common-equipment","text":"The table below details the shared equipment in the lab. This equipment will be shared by all students. Because these resources are shared, you will see configuration appear that is not your own. Please do not delete or change any configuration item that is not your own. Please be respectful of the other students and use only your POD resources. Table of Common (Shared) Devices and Access Information Device Management/Terminal Server IP Telnet Port Number Credentials APIC (apic.dc.local) 192.168.10.1 admin/1234QWer spine 192.168.250.202 7006 admin/1234QWer leaf-1 192.168.250.202 7007 admin/1234QWer leaf-2 192.168.250.202 7008 admin/1234QWer L2/L3 Switch 192.168.250.202 7009 admin/1234QWer AD/DNS/FTP Server 192.168.10.40 admin/1234QWer NTP Server 192.168.10.40","title":"Common Equipment"},{"location":"lab_access/#dedicated-equipment","text":"This equipment is dedicated to every individual student. Each Student Pod has a dedicated ESXi Host, vCenter and a set of Linux based Virtual Machines. POD11 Equipment Device Management IP Fabric IP/ FQDN Username Password Linux VMs WEB 10.0.1.1/24 Root 1234Qwer APP 10.0.2.1/24 Roo 1234Qwer DB 10.0.3.1/24 Root 1234Qwer TRANSACT 10.0.4.1/24 Root 1234Qwer Virtualization Environment ESXi Host 192.168.10.211 ESXp11@dc.local Root 1234QWer vCenter 192.168.10.212 vcenterpod16.dc.local administrator@vsphere.local 1234QWer!","title":"Dedicated Equipment"},{"location":"lab_access/#credentials-summary","text":"For ease of use, the Lab has minimized the number of credentials and uses a standard pattern for the password with minor variations. The table below summarizes all the credentilas you will need to access all the lab resources. Device Type Username/Password Network Devices (Including APIC) admin/1234QWer Linux Virtual Machines Root/1234Qwer ESXi Host Root/1234QWer vCenter administrator@vsphere.local/1234QWer!","title":"Credentials Summary"},{"location":"lab_access/#physical-interface-reference","text":"ACI is fundamentally a networking technology and so throughout the labs you will need to configure interfaces. Use the Physical Interface Table below as a reference.","title":"Physical Interface Reference"},{"location":"lab_access/#spine-1","text":"POD# PORT Connected To Device Connected to Device Port ALL PODS e1/1 leaf-1 e1/49 ALL PODS e1/2 leaf-2 e1/49","title":"SPINE-1"},{"location":"lab_access/#leaf-1","text":"POD# PORT Connected To Device Connected to Device Port ALL PODS e1/2 APIC VIC1 POD11 e1/3 UCS-SERVER-P11 VIC1 POD11 e1/11 ACI-P2-TOR (Cat 3750) Gi1/0/1 ALL PODS e1/49 SPINE-1 e1/1","title":"LEAF-1"},{"location":"lab_access/#leaf-2","text":"POD# PORT Connected To Device Connected to Device Port ALL PODS e1/2 APIC VIC2 POD11 e1/3 UCS-SERVER-P11 VIC2 POD11 e1/11 ACI-P2-TOR (Cat 3750) Gi1/0/2 ALL PODS e1/49 SPINE-1 e1/2","title":"LEAF-2"},{"location":"lab_access/#layer-2-and-layer-3-logical-configuration","text":"","title":"Layer 2 and Layer 3 Logical Configuration"},{"location":"lab_access/#l3out-ip-and-vlan-details","text":"POD Number OSPF AREA LEAF-1 Interface OSPF VLAN SVI on Layer 3 Switch SVI on APIC VLAN Pool Start VLAN Pool End POD11 11 e1/11 1112 172.16.11.2/30 172.16.11.1/30 1110 1119","title":"L3Out IP and Vlan Details"},{"location":"lab_access/#layer-2-details","text":"POD Number SVI on External Layer 3 Device LEAF-2 Interface Layer 2 VLANS VLAN Pool Start VLAN Pool End POD11 10.0.2.99 e1/11 112 110 119","title":"Layer 2 Details"},{"location":"lab_access/#asa-management-details","text":"POD Number ASA Management IP POD11 192.168.10.71","title":"ASA Management Details"},{"location":"lab_access/#lets-get-started-with-aci","text":"","title":"Lets get started with ACI!"},{"location":"mock-core01-config/","text":"Layer 2/Layer 3 Switch Configuration \u00b6 ! banner login ^CCC +-------------------------------------------------+ | ___ _ _ __ __ _ _ | | | _| |___ _ _ _| | \\ \\_ _| | ___| |_ | | | <_| / . | | / . | | | | |_<_> | . \\ | | `___|_\\___`___\\___|_|_|_`_. |___<___|___/ | | <___' | | | +-------------------------------------------------+ Device: Mock Core Layer 3 Switch ^C ! interface Vlan1 ip address 10.1.10.102 255.255.255.0 no ip route-cache no ip mroute-cache end hostname mock-core01 ip domain-name dc.local crypto key gen rsa line vty 0 4 line console 0 logging synchronous login local enable secret 1234QWer aaa new-model username admin password 1234QWer username admin priv 16","title":"Layer 2/Layer 3 Switch Configuration"},{"location":"mock-core01-config/#layer-2layer-3-switch-configuration","text":"! banner login ^CCC +-------------------------------------------------+ | ___ _ _ __ __ _ _ | | | _| |___ _ _ _| | \\ \\_ _| | ___| |_ | | | <_| / . | | / . | | | | |_<_> | . \\ | | `___|_\\___`___\\___|_|_|_`_. |___<___|___/ | | <___' | | | +-------------------------------------------------+ Device: Mock Core Layer 3 Switch ^C ! interface Vlan1 ip address 10.1.10.102 255.255.255.0 no ip route-cache no ip mroute-cache end hostname mock-core01 ip domain-name dc.local crypto key gen rsa line vty 0 4 line console 0 logging synchronous login local enable secret 1234QWer aaa new-model username admin password 1234QWer username admin priv 16","title":"Layer 2/Layer 3 Switch Configuration"},{"location":"topology/","text":"Lab Topology \u00b6 The lab topology is designed to allow configuration of the most common scenarios. Once the Fabric has been discovered and configured and the Tenant design applied, the following functionality can be configured: Layer 3 Routing Layer 2 Connectivity to a Legacy Network Layer 2 Virtual Port Channel A note about Border Leafs. This designation is common in an ACI fabric along with \"Compute Leafs\" and even \"Storage Leafs\". It important to know that this designation is merely an convention to identify the leaf pair that hosts all external connectivity external to the fabric (Border Leafs) or to identify the leaf pairs that are used for host connectivity (Compute Leafs).","title":"Topology & Design"},{"location":"topology/#lab-topology","text":"The lab topology is designed to allow configuration of the most common scenarios. Once the Fabric has been discovered and configured and the Tenant design applied, the following functionality can be configured: Layer 3 Routing Layer 2 Connectivity to a Legacy Network Layer 2 Virtual Port Channel A note about Border Leafs. This designation is common in an ACI fabric along with \"Compute Leafs\" and even \"Storage Leafs\". It important to know that this designation is merely an convention to identify the leaf pair that hosts all external connectivity external to the fabric (Border Leafs) or to identify the leaf pairs that are used for host connectivity (Compute Leafs).","title":"Lab Topology"}]}