{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to CloudMyLab ACI 4 Lab Guide \u00b6 This is a companion guide to the CloudMyLab Cisco Data Center CCIE Rental Rack. This lab guide will go through a typical ACI deployment workflow.","title":"Home"},{"location":"#welcome-to-cloudmylab-aci-4-lab-guide","text":"This is a companion guide to the CloudMyLab Cisco Data Center CCIE Rental Rack. This lab guide will go through a typical ACI deployment workflow.","title":"Welcome to CloudMyLab ACI 4 Lab Guide"},{"location":"01_lab_fabric_discovery/","text":"Lab 01 - Fabric Inventory and Discovery \u00b6 Physical Build \u00b6 Its important to be familiar with the physical configuration of each node in your ACI fabric. A leaf, spine, or apic is a \"Node\" in your ACI fabric and the node numbering is important. In our lab we have a Nexus 9336 Spine, two Nexus 9396 Leafs, and a single APIC-Server-M1. Hardware Overview \u00b6 Model Name View Information N9K-C9336PQ Spine-1 Hardware Overview N9K-C9372PX-E LEAF-1 Hardware Overview N9K-C9372PX-E LEAF-2 Hardware Overview APIC-SERVER-M1 apic1 This configuration is perfectly valid for a Lab but it is not valid for a production environment. The minimum physical fabric hardware for a production environment includes two spines, two leafs, and three APICs. Fabric Turn Up \u00b6 It is important to know that the initial turn up and device discovery and registration has already ocurred. You are accessing the lab after this step has been completed. A fabric turn up is typically performed on site. Physical Connectivity \u00b6 The management network is up and configured All the management interfaces of the spines and leafs are connected to the management network Optionally, all the console interfaces of the spines and leafs are connected to a terminal server Each leaf switch has a fabric uplink to each spine Each APIC has a CIMC connection to the management network a Managmenet connection to the management network Redundant 10G fabric uplinks Note: Out of the box (before discoverying and registering the swith in ACI), you can connect to an ACI switch via the console port. In this state the password for the admin account is blank. Logical Parameters \u00b6 Paramenter Use Lab Value Pod Number Numeric identifier for each ACI Pod Default: 1 1 TEP Pool Default: 10.0.0.0/16 10.0.0.0/16 TEP Vlan Default: None Management Subnet/Mask Default: 192.168.10.0/24 192.168.10.0/24 Management Network Gateway Default: None 192.168.10.254/24 2019 Melbourne Cisco Live How to Setup an ACI Fabric from Scratch - BRKACI-2004 - 2019 Melbourne Cisco Live Explore the Cisco ACI GUI \u00b6 Now that you are familiar with the physical components of the Lab, lets investigate the APIC GUI and the topology from the APIC controller. Step 1 - Connect to Student PC \u00b6 Connect to your Student PC. See the Getting Around section for details. Step 2 - Login to the APIC \u00b6 From your Student PC, open a browser. Google Chrome is recommended for managing the APIC. https://192.168.10.1 or https://apic.dc.local Accept the security warning or create a security exception to access the GUI with the self signed certificate. Note that Secure HTTP (https) is required to access the APIC GUI by default. Insecure HTTP (http) must be explicitly enabled and is not recommended in a production environment. Login to the APIC. Note the warnings which will flash in the uppler right corner. You will see a Critical warning that the cluster does not contain 3 controllers. You may also see a Major warning regarding Licensing. This is expected in the Lab environment. Should you see these warning in a production environment, they must be corrected. You will see the \"What's New\" dialog and the main APIC Dashboard behding the dialog. Skim through the What's New dialog and close it. Step 3 - Areas of the APIC GUI \u00b6 Examine the the top-most section of the GUI interface. This top ribbon containing the main functional areas of the fabric (System, Tenants, Fabric, Virtual Networking, L4-L7 Services, Admin, Operations, Apps) is known as the Menu Bar . Menu Bar \u00b6 You will use it to navigate to the area of the ACI Fabric you need to view or update. Notice the shading and highlighting to help orient you in the GUI. The Menu bar shows that we are in the Dashboard section of the System menu. ACI GUI Menu Options Menu Headings/Tabs Description System Upon login, the GUI defaults to the System Menu Dashboard which provides the health status of the system. From the System menu tab other settings and licensing options are available along with events and faults. Tenants The Tenants Menu provides access to all tenants configured in the fabric and their logical configuration objects. Fabric The Fabric Menu provides access to inventory details, Fabric Policies, and Access Policies. Virtual Networking The Virtual Networking Menu displays and configures the fabric Virtual Machine Managers (VMMs). L4-L7 Services The L4-L7 Services Menu displays and configures the fabric Virtual Machine Managers (VMMs). Admin The Admin Menu displays and configures administrative functions such as authentication, authorization, and accounting functions, scheduling policies, retaining and purging records, upgrading firmware, and controlling features such as syslog, Call Home, and SNMP. Operations The Operations Menu provides access to operational functions including:<br>- Visibility & Troubleshooting<br>- Capacity Dashboard<br>- EP Tracker<br>- Visualiztion Apps The Apps tab displays all the applications installed or uploaded to APIC. The tab allows an APIC administrator to upload, enable, upgrade, install, or uninstall a packaged application in APIC. Areas of the APIC GUI \u00b6 The APIC or controller GUI has 4 main areas: - Menu Bar - Submenu Bar - Navigation Pane - Work Pane Select the Tenants menu. You will see a list of the default or pre-defined tenants which come with ACI \"out of the box\". Select the common tenant. You will see the standard tenants options listed collapsed in the Navigation Pane on the left side. If you select an option in the Navigation Pane, the objects pertaining to that selection are shows in the Work Pane to the right of the Navigation Pane. The Work Pane displays details about the option selected in the Navigation Pane. Fabric configuration via the GUI is typically performed in the Work Pane. Step 4 - Menu Bar and Navigation Conventions \u00b6 Take some time to select each Menu Bar option and get comfortable moving around in the GUI. For the remainder of the Lab the following convention will be used to guide the Student in navigating the GUI: Menu Bar Option > Submenu Option > Navigation Pane Option(s) > Work Pane Tabs From where you are in the Tenants menu navigate to: System > Dashboard to get back to the Health Dashboard of the Fabric. Notice that when you select the * System Menu option, you will automatically go to the Dashboard by default. GUI Tips \u00b6 Wherever there is a submit button and you are trying to make a change - click it. Some changes won\u2019t require it so the inconsistency sometimes calls that into question. Refresh - sometimes your changes won\u2019t appear until you do, you will see the little circular refresh button on most screens Hover over icons with your mouse for a few seconds to view the icon description Exporing Fabric Inventory, Nodes, and Fabric Topology \u00b6 Now that there is some familiarity with the GUI, lets validate the topology of the fabric. Step 1 - View and Explore the Toplogy \u00b6 Navigate to Fabric > Inventory > Topology . The Work Pane opens into the Summary tab. To view the topology diagram, click on the Topology tab in the Work Pane. Note that the full path would be shown as: Fabric > Inventory > Topology > Topology Navigation paths like this are not uncommon in the ACI GUI. Recall that the format we will follow throughout the lab is: Menu Bar Option > Submenu Option > Navigation Pane Option(s) > Work Pane Tabs From the Topology tab in the work pan verify that the displayed topology reflects the lab design. - One Spine - Two Leaf switches - One APIC server dual-homed to both leaf switches Note: You will see the same topology view if you go to Fabric > Inventory > Pod 1 > Topology Device Summary via hover over device icon \u00b6 Hover over each device icon for a very useful physical summary of the device. Device Connectivity \u00b6 Double click each device icon to view a list of connections. Step 2 - Fabric Membership \u00b6 Navigate to Fabric > Inventory > Fabric Membership . Here you will see the fabric inventory including serial number, Pod, Node ID, Model, Role, Fabric IP, and Status. The Pod, Node ID, and Role are defined during fabric discovery. You will notice that the IP comes from the TEP Pool that was provided during the apic intitial confirguration. Notice the additional tabs including Nodes Pending Registration . This tab is used to register new devices to the fabric. Double clicking on one of the device rows will display a dialog with device details. Examine each device and note the details that are available including certificate information. Step 3 - Pod View \u00b6 Navigate to Fabric > Inventory > Pod1 . Expand Pod1 by clicking on the \">\" symbol. Each device in the fabric will be listed. Expand one of the devices and review the sections available. Click on one of the devices (LEAF-1 is showd in the section below). The Summary tab for the device will appear in the Work Pane. Select the General tab in the Work Pane to view additional information about the device. Navigate to Fabric > Inventory > Pod1 > LEAF-1 (Node-102) > Interfaces > Physical Interfaces . Review the interfaces for the devince and note their operational status, Usage and other useful information. Step 4 - CLI \u00b6 Use PUTTY on the Student PC Desktop to connect to the APIC via SSH. Run the acidiag -h command to view the available ACI diagnotics options of the acidiag command. apic1# acidiag -h usage: acidiag [-h] [-v] {avread,fnvread,fnvreadex,rvread,rvreadle,crashsuspecttracker,bootother,bootcurr,journal,logs,oob,scheduler,cleanup,hwcheck,dbgtoken,validateimage,validatenginxconf,version,preservelogs,platform,verifyapic,bond0test,linkflap,touch,run,installer,start,stop,restart,dmestack,dmecore,reboot,drrmode,vapicjoin} ... positional arguments: {avread,fnvread,fnvreadex,rvread,rvreadle,crashsuspecttracker,bootother,bootcurr,journal,logs,oob,scheduler,cleanup,hwcheck,dbgtoken,validateimage,validatenginxconf,version,preservelogs,platform,verifyapic,bond0test,linkflap,touch,run,installer,start,stop,restart,dmestack,dmecore,reboot,drrmode,vapicjoin} sub-command help avread read appliance vector fnvread read fabric node vector fnvreadex read fabric node vector (extended mode) rvread read replica vector rvreadle read replica leader summary crashsuspecttracker read crash suspect tracker state bootother on next boot, boot other Linux Partition, and display updated /etc/grub.conf bootcurr on next boot, boot current Linux Partition, and display updated /etc/grub.conf journal Contents of journal logs logs show log history oob oob options cleanup fs cleanup utility hwcheck Quick check of APIC Hardware dbgtoken show debug token validateimage validate image validatenginxconf validate nginx conf version show ISO version preservelogs stash away logs in preparation for hard reboot platform show platform verifyapic run apic installation verify command bond0test ==SUPPRESS== linkflap flap a link touch touch special files run run specific commands and capture output installer installer start start a service stop stop a service restart restart a service reboot reboot drrmode drrmode options vapicjoin join existing vapic cluster optional arguments: -h, --help show this help message and exit -v, --verbose verbose apic1# Use acidiag fnvread CLI command to view the fabric devices (nodes). apic1# acidiag fnvread ID Pod ID Name Serial Number IP Address Role State LastUpdMsgId -------------------------------------------------------------------------------------------------------------- 101 1 Spine-1 SAL1948TWWP 10.0.72.97/32 spine active 0 102 1 LEAF-1 SAL1948U33K 10.0.72.98/32 leaf active 0 103 1 LEAF-2 SAL1948U35D 10.0.72.96/32 leaf active 0 Total 3 nodes Use the acidiag verifyapic CLI command to view the APIC status. apic1# acidiag verifyapic openssl_check: certificate details subject= CN=FCH1830V38S,serialNumber=PID:APIC-SERVER-M1 SN:FCH1830V38S issuer= CN=Cisco Manufacturing CA,O=Cisco Systems notBefore=Oct 11 08:42:21 2014 GMT notAfter=Oct 11 08:52:21 2024 GMT openssl_check: passed ssh_check: passed all_checks: passed apic1# Verify that you can enter configuration mode. apic1# config apic1(config)# exit apic1# config t apic1(config)# exit apic1# Verify that you can view the configuration using the usual show commands. apic1# sh run # Command: show running-config # Time: Sun May 31 21:56:08 2020 aaa banner 'Application Policy Infrastructure Controller' aaa authentication login console exit aaa authentication login default exit aaa authentication login domain fallback exit bgp-fabric exit coop-fabric exit no password pwd-strength-check crypto aes exit crypto webtoken session-record-flags login,logout,refresh exit rbac security-domain \"all\" exit rbac security-domain \"mgmt\" exit --More-- Because the APIC serves as the controller for the entire fabric, it is often simpler to log on to the APIC and execute show commands across the fabric. In some cases, you may want to log in to a particular leaf or spine. Node ID Management IP 101 192.168.10.101 102 192.168.10.102 103 192.168.10.103 Establish an SSH connection to each device in the table above and execute some common show commands. You can start with the \"show lldp neighbor\" command. Spine-1# show lldp nei Capability codes: (R) Router, (B) Bridge, (T) Telephone, (C) DOCSIS Cable Device (W) WLAN Access Point, (P) Repeater, (S) Station, (O) Other Device ID Local Intf Hold-time Capability Port ID LEAF-1 Eth1/1 120 BR Eth1/49 LEAF-2 Eth1/2 120 BR Eth1/49 Total entries displayed: 2 Spine-1# Spine-1# sh lldp nei sh: lldp: No such file or directory Spine-1# Note that the \"sh lldp nei\" command failed. Remember that many of the common abbreviations for commands are not accepted by ACI. CLI Tips \u00b6 Use or for command completion Not all command shortcuts are accepted in the ACI CLI. More and more are accepted with every new version of ACI but its a good idea to get into the habit of typing out the full commands. Many ways to interact with ACI \u00b6 As you are beginning to see, there is more than one way to interact with the APIC Controller. via the GUI via the CLI via the API ACI configuration, as you will see throughout these labs, within the GUI can be done in different ways as well. Configuration Wizards Configuration of objects individually and creating the relationships between objects in differing orders These labs will focus on individual object and relationship creation so that you have a fundamental understanding of the process. At that point, you can feel free to use Wizards, the CLI, or the API. You will see examples of all of these in these Labs. If a configuration setting is not specifically called out please leave the default values throughout these labs. Skills you should have after completing this lab \u00b6 After completing this labs you should: - be familiar with the hardware components of the lab - be familiar with the APIC GUI, its high level menu options, and how to navigate through the GUI expanding and selecting options - be able to explore the fabric inventory and determine model, status, and connectivity information - understand where to go to add new devices to the fabric - explore fabric inventory via the CLI Supplemental Information \u00b6 Configuring Out of Band Management \u00b6 This is not a lab but you can follow along. In Step 4 of Exploring the Fabric Inventory, you established an SSH connection to each leaf and spine and executed the \"show lldp command\". You were able to do this because the managment interfaces had already benn configured. This section details how that is done. Configuring out of band management for the fabric is only done once as part of fabric turn up. This activity effectivley configures an IP address on the device management interface so that the device is reachable via SSH. Because it is an essential step in the turn up of an ACI fabric, instructions are provided here for completeness but please do not perform these actions on your student fabric. Tenant mgmt \u00b6 Go to the Tenants menu and select the mgmt tenant from the subment or from the list in the expanded Work Pane. From the mgmt Tenant Navigation Pane, from the tenant navigation Pane navigate to Tenant mgmt > Node Management Addresses > Static Node Management Addresses Form here you have two options to get to the configuration dialog for a node management address. 1. Right click on the Static Node Management Addresses option in the Navigation Pane 2. Click on the tool icon drop down in the Work Pane. Both of these actions are equivalent and will present you with a dialog to create a static node management address. In ACI, this is the equivalent of assigning a management IP to the management port of the device. In ACI you often use the Node ID to identify the device and that is the case here. You can enter a range, as shown below with a starting IP in the management subnet and generally the IPs will be assigned sequentially. Note: The device hostname, credentials, and other settings are configured on the switch by the APIC when it is first discovered. While the range option is handy, there may be unintended consequences or if you have an IP addressing convention (Node ID mapped to last octet is a good practice) the assignments many not adhere to your convention. Its a good practice to do the IP assignment individually on each node. Note the range from and to values are for a single node. The Lab uses the default OOB (Out of Band) Management EPG, however in a production data center it is a good pracitce to configure and explicit management EPG.","title":"Lab 01 - Fabric Discovery"},{"location":"01_lab_fabric_discovery/#lab-01-fabric-inventory-and-discovery","text":"","title":"Lab 01 - Fabric Inventory and Discovery"},{"location":"01_lab_fabric_discovery/#physical-build","text":"Its important to be familiar with the physical configuration of each node in your ACI fabric. A leaf, spine, or apic is a \"Node\" in your ACI fabric and the node numbering is important. In our lab we have a Nexus 9336 Spine, two Nexus 9396 Leafs, and a single APIC-Server-M1.","title":"Physical Build"},{"location":"01_lab_fabric_discovery/#hardware-overview","text":"Model Name View Information N9K-C9336PQ Spine-1 Hardware Overview N9K-C9372PX-E LEAF-1 Hardware Overview N9K-C9372PX-E LEAF-2 Hardware Overview APIC-SERVER-M1 apic1 This configuration is perfectly valid for a Lab but it is not valid for a production environment. The minimum physical fabric hardware for a production environment includes two spines, two leafs, and three APICs.","title":"Hardware Overview"},{"location":"01_lab_fabric_discovery/#fabric-turn-up","text":"It is important to know that the initial turn up and device discovery and registration has already ocurred. You are accessing the lab after this step has been completed. A fabric turn up is typically performed on site.","title":"Fabric Turn Up"},{"location":"01_lab_fabric_discovery/#physical-connectivity","text":"The management network is up and configured All the management interfaces of the spines and leafs are connected to the management network Optionally, all the console interfaces of the spines and leafs are connected to a terminal server Each leaf switch has a fabric uplink to each spine Each APIC has a CIMC connection to the management network a Managmenet connection to the management network Redundant 10G fabric uplinks Note: Out of the box (before discoverying and registering the swith in ACI), you can connect to an ACI switch via the console port. In this state the password for the admin account is blank.","title":"Physical Connectivity"},{"location":"01_lab_fabric_discovery/#logical-parameters","text":"Paramenter Use Lab Value Pod Number Numeric identifier for each ACI Pod Default: 1 1 TEP Pool Default: 10.0.0.0/16 10.0.0.0/16 TEP Vlan Default: None Management Subnet/Mask Default: 192.168.10.0/24 192.168.10.0/24 Management Network Gateway Default: None 192.168.10.254/24 2019 Melbourne Cisco Live How to Setup an ACI Fabric from Scratch - BRKACI-2004 - 2019 Melbourne Cisco Live","title":"Logical Parameters"},{"location":"01_lab_fabric_discovery/#explore-the-cisco-aci-gui","text":"Now that you are familiar with the physical components of the Lab, lets investigate the APIC GUI and the topology from the APIC controller.","title":"Explore the Cisco ACI GUI"},{"location":"01_lab_fabric_discovery/#step-1-connect-to-student-pc","text":"Connect to your Student PC. See the Getting Around section for details.","title":"Step 1 - Connect to Student PC"},{"location":"01_lab_fabric_discovery/#step-2-login-to-the-apic","text":"From your Student PC, open a browser. Google Chrome is recommended for managing the APIC. https://192.168.10.1 or https://apic.dc.local Accept the security warning or create a security exception to access the GUI with the self signed certificate. Note that Secure HTTP (https) is required to access the APIC GUI by default. Insecure HTTP (http) must be explicitly enabled and is not recommended in a production environment. Login to the APIC. Note the warnings which will flash in the uppler right corner. You will see a Critical warning that the cluster does not contain 3 controllers. You may also see a Major warning regarding Licensing. This is expected in the Lab environment. Should you see these warning in a production environment, they must be corrected. You will see the \"What's New\" dialog and the main APIC Dashboard behding the dialog. Skim through the What's New dialog and close it.","title":"Step 2 - Login to the APIC"},{"location":"01_lab_fabric_discovery/#step-3-areas-of-the-apic-gui","text":"Examine the the top-most section of the GUI interface. This top ribbon containing the main functional areas of the fabric (System, Tenants, Fabric, Virtual Networking, L4-L7 Services, Admin, Operations, Apps) is known as the Menu Bar .","title":"Step 3 - Areas of the APIC GUI"},{"location":"01_lab_fabric_discovery/#menu-bar","text":"You will use it to navigate to the area of the ACI Fabric you need to view or update. Notice the shading and highlighting to help orient you in the GUI. The Menu bar shows that we are in the Dashboard section of the System menu. ACI GUI Menu Options Menu Headings/Tabs Description System Upon login, the GUI defaults to the System Menu Dashboard which provides the health status of the system. From the System menu tab other settings and licensing options are available along with events and faults. Tenants The Tenants Menu provides access to all tenants configured in the fabric and their logical configuration objects. Fabric The Fabric Menu provides access to inventory details, Fabric Policies, and Access Policies. Virtual Networking The Virtual Networking Menu displays and configures the fabric Virtual Machine Managers (VMMs). L4-L7 Services The L4-L7 Services Menu displays and configures the fabric Virtual Machine Managers (VMMs). Admin The Admin Menu displays and configures administrative functions such as authentication, authorization, and accounting functions, scheduling policies, retaining and purging records, upgrading firmware, and controlling features such as syslog, Call Home, and SNMP. Operations The Operations Menu provides access to operational functions including:<br>- Visibility & Troubleshooting<br>- Capacity Dashboard<br>- EP Tracker<br>- Visualiztion Apps The Apps tab displays all the applications installed or uploaded to APIC. The tab allows an APIC administrator to upload, enable, upgrade, install, or uninstall a packaged application in APIC.","title":"Menu Bar"},{"location":"01_lab_fabric_discovery/#areas-of-the-apic-gui","text":"The APIC or controller GUI has 4 main areas: - Menu Bar - Submenu Bar - Navigation Pane - Work Pane Select the Tenants menu. You will see a list of the default or pre-defined tenants which come with ACI \"out of the box\". Select the common tenant. You will see the standard tenants options listed collapsed in the Navigation Pane on the left side. If you select an option in the Navigation Pane, the objects pertaining to that selection are shows in the Work Pane to the right of the Navigation Pane. The Work Pane displays details about the option selected in the Navigation Pane. Fabric configuration via the GUI is typically performed in the Work Pane.","title":"Areas of the APIC GUI"},{"location":"01_lab_fabric_discovery/#step-4-menu-bar-and-navigation-conventions","text":"Take some time to select each Menu Bar option and get comfortable moving around in the GUI. For the remainder of the Lab the following convention will be used to guide the Student in navigating the GUI: Menu Bar Option > Submenu Option > Navigation Pane Option(s) > Work Pane Tabs From where you are in the Tenants menu navigate to: System > Dashboard to get back to the Health Dashboard of the Fabric. Notice that when you select the * System Menu option, you will automatically go to the Dashboard by default.","title":"Step 4 - Menu Bar and Navigation Conventions"},{"location":"01_lab_fabric_discovery/#gui-tips","text":"Wherever there is a submit button and you are trying to make a change - click it. Some changes won\u2019t require it so the inconsistency sometimes calls that into question. Refresh - sometimes your changes won\u2019t appear until you do, you will see the little circular refresh button on most screens Hover over icons with your mouse for a few seconds to view the icon description","title":"GUI Tips"},{"location":"01_lab_fabric_discovery/#exporing-fabric-inventory-nodes-and-fabric-topology","text":"Now that there is some familiarity with the GUI, lets validate the topology of the fabric.","title":"Exporing Fabric Inventory, Nodes, and Fabric Topology"},{"location":"01_lab_fabric_discovery/#step-1-view-and-explore-the-toplogy","text":"Navigate to Fabric > Inventory > Topology . The Work Pane opens into the Summary tab. To view the topology diagram, click on the Topology tab in the Work Pane. Note that the full path would be shown as: Fabric > Inventory > Topology > Topology Navigation paths like this are not uncommon in the ACI GUI. Recall that the format we will follow throughout the lab is: Menu Bar Option > Submenu Option > Navigation Pane Option(s) > Work Pane Tabs From the Topology tab in the work pan verify that the displayed topology reflects the lab design. - One Spine - Two Leaf switches - One APIC server dual-homed to both leaf switches Note: You will see the same topology view if you go to Fabric > Inventory > Pod 1 > Topology","title":"Step 1 - View and Explore the Toplogy"},{"location":"01_lab_fabric_discovery/#device-summary-via-hover-over-device-icon","text":"Hover over each device icon for a very useful physical summary of the device.","title":"Device Summary via hover over device icon"},{"location":"01_lab_fabric_discovery/#device-connectivity","text":"Double click each device icon to view a list of connections.","title":"Device Connectivity"},{"location":"01_lab_fabric_discovery/#step-2-fabric-membership","text":"Navigate to Fabric > Inventory > Fabric Membership . Here you will see the fabric inventory including serial number, Pod, Node ID, Model, Role, Fabric IP, and Status. The Pod, Node ID, and Role are defined during fabric discovery. You will notice that the IP comes from the TEP Pool that was provided during the apic intitial confirguration. Notice the additional tabs including Nodes Pending Registration . This tab is used to register new devices to the fabric. Double clicking on one of the device rows will display a dialog with device details. Examine each device and note the details that are available including certificate information.","title":"Step 2 - Fabric Membership"},{"location":"01_lab_fabric_discovery/#step-3-pod-view","text":"Navigate to Fabric > Inventory > Pod1 . Expand Pod1 by clicking on the \">\" symbol. Each device in the fabric will be listed. Expand one of the devices and review the sections available. Click on one of the devices (LEAF-1 is showd in the section below). The Summary tab for the device will appear in the Work Pane. Select the General tab in the Work Pane to view additional information about the device. Navigate to Fabric > Inventory > Pod1 > LEAF-1 (Node-102) > Interfaces > Physical Interfaces . Review the interfaces for the devince and note their operational status, Usage and other useful information.","title":"Step 3 - Pod View"},{"location":"01_lab_fabric_discovery/#step-4-cli","text":"Use PUTTY on the Student PC Desktop to connect to the APIC via SSH. Run the acidiag -h command to view the available ACI diagnotics options of the acidiag command. apic1# acidiag -h usage: acidiag [-h] [-v] {avread,fnvread,fnvreadex,rvread,rvreadle,crashsuspecttracker,bootother,bootcurr,journal,logs,oob,scheduler,cleanup,hwcheck,dbgtoken,validateimage,validatenginxconf,version,preservelogs,platform,verifyapic,bond0test,linkflap,touch,run,installer,start,stop,restart,dmestack,dmecore,reboot,drrmode,vapicjoin} ... positional arguments: {avread,fnvread,fnvreadex,rvread,rvreadle,crashsuspecttracker,bootother,bootcurr,journal,logs,oob,scheduler,cleanup,hwcheck,dbgtoken,validateimage,validatenginxconf,version,preservelogs,platform,verifyapic,bond0test,linkflap,touch,run,installer,start,stop,restart,dmestack,dmecore,reboot,drrmode,vapicjoin} sub-command help avread read appliance vector fnvread read fabric node vector fnvreadex read fabric node vector (extended mode) rvread read replica vector rvreadle read replica leader summary crashsuspecttracker read crash suspect tracker state bootother on next boot, boot other Linux Partition, and display updated /etc/grub.conf bootcurr on next boot, boot current Linux Partition, and display updated /etc/grub.conf journal Contents of journal logs logs show log history oob oob options cleanup fs cleanup utility hwcheck Quick check of APIC Hardware dbgtoken show debug token validateimage validate image validatenginxconf validate nginx conf version show ISO version preservelogs stash away logs in preparation for hard reboot platform show platform verifyapic run apic installation verify command bond0test ==SUPPRESS== linkflap flap a link touch touch special files run run specific commands and capture output installer installer start start a service stop stop a service restart restart a service reboot reboot drrmode drrmode options vapicjoin join existing vapic cluster optional arguments: -h, --help show this help message and exit -v, --verbose verbose apic1# Use acidiag fnvread CLI command to view the fabric devices (nodes). apic1# acidiag fnvread ID Pod ID Name Serial Number IP Address Role State LastUpdMsgId -------------------------------------------------------------------------------------------------------------- 101 1 Spine-1 SAL1948TWWP 10.0.72.97/32 spine active 0 102 1 LEAF-1 SAL1948U33K 10.0.72.98/32 leaf active 0 103 1 LEAF-2 SAL1948U35D 10.0.72.96/32 leaf active 0 Total 3 nodes Use the acidiag verifyapic CLI command to view the APIC status. apic1# acidiag verifyapic openssl_check: certificate details subject= CN=FCH1830V38S,serialNumber=PID:APIC-SERVER-M1 SN:FCH1830V38S issuer= CN=Cisco Manufacturing CA,O=Cisco Systems notBefore=Oct 11 08:42:21 2014 GMT notAfter=Oct 11 08:52:21 2024 GMT openssl_check: passed ssh_check: passed all_checks: passed apic1# Verify that you can enter configuration mode. apic1# config apic1(config)# exit apic1# config t apic1(config)# exit apic1# Verify that you can view the configuration using the usual show commands. apic1# sh run # Command: show running-config # Time: Sun May 31 21:56:08 2020 aaa banner 'Application Policy Infrastructure Controller' aaa authentication login console exit aaa authentication login default exit aaa authentication login domain fallback exit bgp-fabric exit coop-fabric exit no password pwd-strength-check crypto aes exit crypto webtoken session-record-flags login,logout,refresh exit rbac security-domain \"all\" exit rbac security-domain \"mgmt\" exit --More-- Because the APIC serves as the controller for the entire fabric, it is often simpler to log on to the APIC and execute show commands across the fabric. In some cases, you may want to log in to a particular leaf or spine. Node ID Management IP 101 192.168.10.101 102 192.168.10.102 103 192.168.10.103 Establish an SSH connection to each device in the table above and execute some common show commands. You can start with the \"show lldp neighbor\" command. Spine-1# show lldp nei Capability codes: (R) Router, (B) Bridge, (T) Telephone, (C) DOCSIS Cable Device (W) WLAN Access Point, (P) Repeater, (S) Station, (O) Other Device ID Local Intf Hold-time Capability Port ID LEAF-1 Eth1/1 120 BR Eth1/49 LEAF-2 Eth1/2 120 BR Eth1/49 Total entries displayed: 2 Spine-1# Spine-1# sh lldp nei sh: lldp: No such file or directory Spine-1# Note that the \"sh lldp nei\" command failed. Remember that many of the common abbreviations for commands are not accepted by ACI.","title":"Step 4 - CLI"},{"location":"01_lab_fabric_discovery/#cli-tips","text":"Use or for command completion Not all command shortcuts are accepted in the ACI CLI. More and more are accepted with every new version of ACI but its a good idea to get into the habit of typing out the full commands.","title":"CLI Tips"},{"location":"01_lab_fabric_discovery/#many-ways-to-interact-with-aci","text":"As you are beginning to see, there is more than one way to interact with the APIC Controller. via the GUI via the CLI via the API ACI configuration, as you will see throughout these labs, within the GUI can be done in different ways as well. Configuration Wizards Configuration of objects individually and creating the relationships between objects in differing orders These labs will focus on individual object and relationship creation so that you have a fundamental understanding of the process. At that point, you can feel free to use Wizards, the CLI, or the API. You will see examples of all of these in these Labs. If a configuration setting is not specifically called out please leave the default values throughout these labs.","title":"Many ways to interact with ACI"},{"location":"01_lab_fabric_discovery/#skills-you-should-have-after-completing-this-lab","text":"After completing this labs you should: - be familiar with the hardware components of the lab - be familiar with the APIC GUI, its high level menu options, and how to navigate through the GUI expanding and selecting options - be able to explore the fabric inventory and determine model, status, and connectivity information - understand where to go to add new devices to the fabric - explore fabric inventory via the CLI","title":"Skills you should have after completing this lab"},{"location":"01_lab_fabric_discovery/#supplemental-information","text":"","title":"Supplemental Information"},{"location":"01_lab_fabric_discovery/#configuring-out-of-band-management","text":"This is not a lab but you can follow along. In Step 4 of Exploring the Fabric Inventory, you established an SSH connection to each leaf and spine and executed the \"show lldp command\". You were able to do this because the managment interfaces had already benn configured. This section details how that is done. Configuring out of band management for the fabric is only done once as part of fabric turn up. This activity effectivley configures an IP address on the device management interface so that the device is reachable via SSH. Because it is an essential step in the turn up of an ACI fabric, instructions are provided here for completeness but please do not perform these actions on your student fabric.","title":"Configuring Out of Band Management"},{"location":"01_lab_fabric_discovery/#tenant-mgmt","text":"Go to the Tenants menu and select the mgmt tenant from the subment or from the list in the expanded Work Pane. From the mgmt Tenant Navigation Pane, from the tenant navigation Pane navigate to Tenant mgmt > Node Management Addresses > Static Node Management Addresses Form here you have two options to get to the configuration dialog for a node management address. 1. Right click on the Static Node Management Addresses option in the Navigation Pane 2. Click on the tool icon drop down in the Work Pane. Both of these actions are equivalent and will present you with a dialog to create a static node management address. In ACI, this is the equivalent of assigning a management IP to the management port of the device. In ACI you often use the Node ID to identify the device and that is the case here. You can enter a range, as shown below with a starting IP in the management subnet and generally the IPs will be assigned sequentially. Note: The device hostname, credentials, and other settings are configured on the switch by the APIC when it is first discovered. While the range option is handy, there may be unintended consequences or if you have an IP addressing convention (Node ID mapped to last octet is a good practice) the assignments many not adhere to your convention. Its a good practice to do the IP assignment individually on each node. Note the range from and to values are for a single node. The Lab uses the default OOB (Out of Band) Management EPG, however in a production data center it is a good pracitce to configure and explicit management EPG.","title":"Tenant mgmt"},{"location":"02_lab_fabric_access_policies/","text":"Lab 02 - Configure Fabric & Access Constructs \u00b6 Context \u00b6 This lab represents the implementation of networking at the physical layer across the entire fabric as well as on a switch and interface or port level. Typically in an ACI fabric, the Fabric Policies are configured during initial fabric turn up and then largely left alone. The Access Policies represetn that \"day to day\" configuration of the fabric as they define how the fabric connects to hosts and other non-ACI network devices. You an think of Access Policies as the hardware configuration of interfaces. The logical portion of the configuration is implemented when Access Policies are associated with EPGs and \"Vlans\" or Encapsulations. Lab Goals \u00b6 In this lab we will review how to configure basic fabric functionality via Fabric Policies . These are configurations steps which are peformed at the fabric level and which have already been configured for the lab but the guide will detail the steps required to complete this step. We will then look at Access Policies which configure the physical (vs logical) aspects of fabric interfaces which connect the leafs (and spines) to compute, storage, appliances, and other non-ACI network devices. Fabric Configuration \u00b6 Configuration in ACI is largely based on policies which abstract the characteristics of traditional network connectivity. Consider the diagram below and the steps you normally take to configure a layer 2 trunk interface like the one shows on LEAF-2 e1/11. In the classical Ethernet environment this configuration is done on LEAF-2 itself so the switch is understood. The configuration is performed on the interface itself, e1/11 in this case, so that is understood. Because these objects are abstract and all of the configuration is performed by the APIC controller, each of those \"undersood\" items are configued discreetly via policy and then put together to achieve the desired functionality. We will go through that workflow in this lab. Initially it may seem like \"extra\" work to do this but once you start taking advantage of the re-usability of these policy objects and see the ease of scalability, the \"extra\" work will be well worth it. Implementing Cisco ACI Fabric Connectivity \u00b6 The Fabric menu has three submenu items. You should be familiar with the Inventory submenu from Lab 01. The other two Submenu items configure the fabric itself and the physical connectivity. Configuring ACI connectivity is generally done in two steps: 1. Configure the Physical Layer Characteristics 2. Configure the Logical Layer Characteristics Fabric Policies \u00b6 Fabric policies control the configuration of the fabric itself. The most common activity in this section is to define Policy Groups such as the fabrics: Data and Time policy (NTP) Management Access policy SNMP Policy (for the fabric) BGP Route Reflector Policy The BGP Route Reflect policy one of the first fabric configuration items which needs to be completed for a functioning fabric. In a production fabric, it is best practice to make 2 or 4 spines route reflectors. Note: In recent versions of ACI configuring the route reflectors is done from System > System Settings > BGP Route Reflector This activity is only done once and the route reflector has already been configured in the the lab for all students. Access Policies \u00b6 Unlike the Fabric Policies which are often configured at turn up and then largely left alone, Access Policies are used often to configure new vlans, domains, switches, and interfaces. Fabric Policies are analogous to configuring two switches in VSS mode or in a vPC domain or VDC. Access Policies are analogous to configuring vlans, access or trunk interfaces, and interface settings. Access Policies are the \"configuration bits\" that you will fashion together to obtain the functionality you need for your fabric. You build Fabric Access Policies with multiple configuration objects: Object Description & Use Pool Individual or a Range of VLANs Physical Domain A logical construct tying a vlan pool object to an AAEP Attachable Access Entity Profile (AAEP) An ACI object which groups physical and virtual domains for scalability. Interface Policy Describes an individual behavior.<br>For example: CDP enabled, 10Gig Interface, 1Gig Interface Interface Policy Group Bundles interface policies together for a specific interface behavior and ties that to an AAEP which now adds valid vlans on the interface Interface Profile Ties an Interface Policy Group to a specific interface number Switch Profile Defines a fabric switch to which Interface Profiles can be associated ACI Object (Construct) Relationship Overview \u00b6 It is vital that you understand the relationship between objects. Tips for naming objects in ACI \u00b6 A naming standard is critical to sucessful operation of ACI. It is important to define your naming conventions before configuring anything in ACI. Objects themselves cannot be renamed. If you have a typo in an object name it must be deleted and re-created. Object names are typically up to 64 alphanumeric characters in length (maximum). Use underscores (spaces are not allowed) Keep names short Already Configured \u00b6 As with the Route Reflector configuration, there are other configuration activities which are done ones at the fabric level (rather than the tenant level) vPC Domains in ACI - vPC Protection Groups \u00b6 Since one of the first things we are going to do in this lab is configure a port channel (vPC on ACI and a port-channel on switch side), we need to make sure the two leaf switches are configured as a vPC Domain. This step is done onece for each vPC Domain but we will review the required steps here. Step 1 - Confirm the two Leaf switches are configured as a vPC Pair \u00b6 Navigate to Fabric > Access Policis > Policies > Policies > Switch > VPC Domain . Review the configuration of the default vPC domain Dead Peer timer. Even in a production environment the defaults are rarely changed. The object name can be misleading as this is not where the vPC domain is configured. Select the Virtual Port Channel default object at the top level of Fabric > Access Policis > Policies > Policies > Switch In the Work Pane you can see that the two fabric leaf switches, Node IDs 102 and 103, are configured as a VPC \"Protection Group\" aka vPC Domain. The VPC Pair ID is 101 (as a best pracitce use the lowest Node ID number) and the fabric has automatically assigned the the vPC Domain an IP address from the TEP IP address block. Configuring the vPC Domain is simple once you know where that configuration is done in the GUI. Configuration Item Configuration Name Configuration Values vPC Protection Group (vPC Domain) Lab_Fabric_102_103_vPCDomain ID: 102 (lowest Node ID by convention) Switch 1: 102 Switch 2: 103 You would then click Submit but remember to not do so in the Lab environment. In the Lab environment this fabric level configuration is complete and the leaf switches are already operating as a vPC pair. Switch Policies \u00b6 Defining switch \"objects\" which can then have policies applied to them is part of the inital fabric configuration and only performed once per switch and switch pair. Navigate to Fabric > Access Policies > Switches > Leaf Switches . Notice the options in the Navigation Pane: - Profiles <-- Leaf Profiles - this is where the switch \"objects\" are configured - Policy Groups <-- - Overrides Navigate to Fabric > Access Policies > Switches > Leaf Switches > Profiles and expand the Profiles option. You will see three switch \"objects\" representing - LEAF-1 - LEAF-2 - LEAF-1 and LEAF-2 Pair When you want to configure a single interface on a specific leaf you will use the specific individual switch object. In our first example, we will configure a single access port on LEAF-2. Since the switches are abstracted, it is also possible to define an object that represents both switches. When configuring a vPC or two access ports using the same port on each specific switch, this pair object can be used. In our second example we will associate the same port, e1/11, to the LEAFS_102_103_LeafProf Leaf Profile. In one step or associate we have configured port e1/11 on both switches. In this exercise port e1/11 will be configured as a vPC. Configuring Interface Policies and Profiles \u00b6 Keeping in mind the full connectivity workflow of: Configure the Physical Layer Characteristics <-- You are here Configure the Logical Layer Characteristics Log in to your Student PC, open your browser, and access the APIC GUI. Step 1 - Creating Interface Policies - CDP Enabled \u00b6 Navigate to Fabric > Access Policies > Policies > Interface > CDP Interface Expand CDP Interface and note that there is a default policy that comes pre configured. This is the case with many objects and while the controller will use these default policies when needed and an explicit policy is not selected, it is a best practice to explicitly configure policies for the behavior you want. Create an explicit policy which enables CDP. Right Click on CDP Interface and select the Create CDP Interface Policy option. Configuration Item Configuration Name Configuration Values Interface Policy to enable CDP POD##_CDP_Enabled Example: POD11_CDP_Enabled (for POD11 Student) Admin State = Enabled You now have a policy to enable CDP on an interface which can be reused. Note that in ACI the preferred discovery protocol is LLDP. That is enabled by default across the fabric and is a key part of the discovery process. CDP is not enabled on the fabric by default after 4.0. In order to enable CDP on an interface you will use this new just created policy. Step 2 - Creating Interface Policies - Interface Speed \u00b6 Navigate to Fabric > Access Policies > Policies > Interface > Link Level Right Click on Link Level and select the Create Link Level Policy option. Configuration Item Configuration Name Configuration Values Interface Policy to configure a 1 GigE Interface POD##_1G_IntPol Example: POD11_1G_IntPol (for POD11 Student) Speed: 1 Gbps Interface Policy to configure a 10 GigE Interface POD##_10G_IntPol Example: POD11_10G_IntPol (for POD11 Student) Speed: 10 Gbps Step 3 - Creating Interface Policies - LLDP Disable \u00b6 Navigate to Fabric > Access Policies > Policies > Interface > LLDP Interface Configuration Item Configuration Name Configuration Values Interface Policy to disable LLDP POD##_LLDP_Disabled Example: POD11_LLDP_Disabled Recieve State = Disabled Transmit State = Disabled Step 4 - Creating Interface Policies - Static Port Channel \u00b6 Navigate to Fabric > Access Policies > Policies > Interface > Port Channel Configuration Item Configuration Name Configuration Values Interface Policy for a static port channel POD##_Static_Po Example: POD11_Static_Po Mode: Static Channel - Mode On Interface Policy for an active LACP port channel POD##_LACP_Active_Po Example: POD11_LACP_Active_Po Mode: LACP Active Step 5 - Creating Interface Policy Groups - Access Interface \u00b6 Lets take these individual behaviors defined in each Interface Policy and bundle them into a group of behaviors that can then be associated with an interface. This bundle of configuration items defining the port behavior is called an Interface Policy Group . Lets define the Policy Group that would define a 1Gig interface with CDP enabled, LLD disabled, and a static port channel. Step 6 - Creating Interface Policy Groups - Access Interface \u00b6 Check in \u00b6 With these policies, we have now defined \"reusable behaviors\": 1 Gigabit Ethernet Interface Enable CDP Disable LLDP Static Port Channel Active LAC Port Channel We have also defined two It important to note that these \"behaviors\" are not associated with an interface or even a switch at this point. Think of them as \"raw ingredients\" that are ready for use in order to implement a specific functionality. We will then configure a 1Gig LACP port channel down to the \"mock-core01\" switch.","title":"Lab 02 - Fabric & Access Policies"},{"location":"02_lab_fabric_access_policies/#lab-02-configure-fabric-access-constructs","text":"","title":"Lab 02 - Configure Fabric &amp; Access Constructs"},{"location":"02_lab_fabric_access_policies/#context","text":"This lab represents the implementation of networking at the physical layer across the entire fabric as well as on a switch and interface or port level. Typically in an ACI fabric, the Fabric Policies are configured during initial fabric turn up and then largely left alone. The Access Policies represetn that \"day to day\" configuration of the fabric as they define how the fabric connects to hosts and other non-ACI network devices. You an think of Access Policies as the hardware configuration of interfaces. The logical portion of the configuration is implemented when Access Policies are associated with EPGs and \"Vlans\" or Encapsulations.","title":"Context"},{"location":"02_lab_fabric_access_policies/#lab-goals","text":"In this lab we will review how to configure basic fabric functionality via Fabric Policies . These are configurations steps which are peformed at the fabric level and which have already been configured for the lab but the guide will detail the steps required to complete this step. We will then look at Access Policies which configure the physical (vs logical) aspects of fabric interfaces which connect the leafs (and spines) to compute, storage, appliances, and other non-ACI network devices.","title":"Lab Goals"},{"location":"02_lab_fabric_access_policies/#fabric-configuration","text":"Configuration in ACI is largely based on policies which abstract the characteristics of traditional network connectivity. Consider the diagram below and the steps you normally take to configure a layer 2 trunk interface like the one shows on LEAF-2 e1/11. In the classical Ethernet environment this configuration is done on LEAF-2 itself so the switch is understood. The configuration is performed on the interface itself, e1/11 in this case, so that is understood. Because these objects are abstract and all of the configuration is performed by the APIC controller, each of those \"undersood\" items are configued discreetly via policy and then put together to achieve the desired functionality. We will go through that workflow in this lab. Initially it may seem like \"extra\" work to do this but once you start taking advantage of the re-usability of these policy objects and see the ease of scalability, the \"extra\" work will be well worth it.","title":"Fabric Configuration"},{"location":"02_lab_fabric_access_policies/#implementing-cisco-aci-fabric-connectivity","text":"The Fabric menu has three submenu items. You should be familiar with the Inventory submenu from Lab 01. The other two Submenu items configure the fabric itself and the physical connectivity. Configuring ACI connectivity is generally done in two steps: 1. Configure the Physical Layer Characteristics 2. Configure the Logical Layer Characteristics","title":"Implementing Cisco ACI Fabric Connectivity"},{"location":"02_lab_fabric_access_policies/#fabric-policies","text":"Fabric policies control the configuration of the fabric itself. The most common activity in this section is to define Policy Groups such as the fabrics: Data and Time policy (NTP) Management Access policy SNMP Policy (for the fabric) BGP Route Reflector Policy The BGP Route Reflect policy one of the first fabric configuration items which needs to be completed for a functioning fabric. In a production fabric, it is best practice to make 2 or 4 spines route reflectors. Note: In recent versions of ACI configuring the route reflectors is done from System > System Settings > BGP Route Reflector This activity is only done once and the route reflector has already been configured in the the lab for all students.","title":"Fabric Policies"},{"location":"02_lab_fabric_access_policies/#access-policies","text":"Unlike the Fabric Policies which are often configured at turn up and then largely left alone, Access Policies are used often to configure new vlans, domains, switches, and interfaces. Fabric Policies are analogous to configuring two switches in VSS mode or in a vPC domain or VDC. Access Policies are analogous to configuring vlans, access or trunk interfaces, and interface settings. Access Policies are the \"configuration bits\" that you will fashion together to obtain the functionality you need for your fabric. You build Fabric Access Policies with multiple configuration objects: Object Description & Use Pool Individual or a Range of VLANs Physical Domain A logical construct tying a vlan pool object to an AAEP Attachable Access Entity Profile (AAEP) An ACI object which groups physical and virtual domains for scalability. Interface Policy Describes an individual behavior.<br>For example: CDP enabled, 10Gig Interface, 1Gig Interface Interface Policy Group Bundles interface policies together for a specific interface behavior and ties that to an AAEP which now adds valid vlans on the interface Interface Profile Ties an Interface Policy Group to a specific interface number Switch Profile Defines a fabric switch to which Interface Profiles can be associated","title":"Access Policies"},{"location":"02_lab_fabric_access_policies/#aci-object-construct-relationship-overview","text":"It is vital that you understand the relationship between objects.","title":"ACI Object (Construct) Relationship Overview"},{"location":"02_lab_fabric_access_policies/#tips-for-naming-objects-in-aci","text":"A naming standard is critical to sucessful operation of ACI. It is important to define your naming conventions before configuring anything in ACI. Objects themselves cannot be renamed. If you have a typo in an object name it must be deleted and re-created. Object names are typically up to 64 alphanumeric characters in length (maximum). Use underscores (spaces are not allowed) Keep names short","title":"Tips for naming objects in ACI"},{"location":"02_lab_fabric_access_policies/#already-configured","text":"As with the Route Reflector configuration, there are other configuration activities which are done ones at the fabric level (rather than the tenant level)","title":"Already Configured"},{"location":"02_lab_fabric_access_policies/#vpc-domains-in-aci-vpc-protection-groups","text":"Since one of the first things we are going to do in this lab is configure a port channel (vPC on ACI and a port-channel on switch side), we need to make sure the two leaf switches are configured as a vPC Domain. This step is done onece for each vPC Domain but we will review the required steps here.","title":"vPC Domains in ACI - vPC Protection Groups"},{"location":"02_lab_fabric_access_policies/#step-1-confirm-the-two-leaf-switches-are-configured-as-a-vpc-pair","text":"Navigate to Fabric > Access Policis > Policies > Policies > Switch > VPC Domain . Review the configuration of the default vPC domain Dead Peer timer. Even in a production environment the defaults are rarely changed. The object name can be misleading as this is not where the vPC domain is configured. Select the Virtual Port Channel default object at the top level of Fabric > Access Policis > Policies > Policies > Switch In the Work Pane you can see that the two fabric leaf switches, Node IDs 102 and 103, are configured as a VPC \"Protection Group\" aka vPC Domain. The VPC Pair ID is 101 (as a best pracitce use the lowest Node ID number) and the fabric has automatically assigned the the vPC Domain an IP address from the TEP IP address block. Configuring the vPC Domain is simple once you know where that configuration is done in the GUI. Configuration Item Configuration Name Configuration Values vPC Protection Group (vPC Domain) Lab_Fabric_102_103_vPCDomain ID: 102 (lowest Node ID by convention) Switch 1: 102 Switch 2: 103 You would then click Submit but remember to not do so in the Lab environment. In the Lab environment this fabric level configuration is complete and the leaf switches are already operating as a vPC pair.","title":"Step 1 - Confirm the two Leaf switches are configured as a vPC Pair"},{"location":"02_lab_fabric_access_policies/#switch-policies","text":"Defining switch \"objects\" which can then have policies applied to them is part of the inital fabric configuration and only performed once per switch and switch pair. Navigate to Fabric > Access Policies > Switches > Leaf Switches . Notice the options in the Navigation Pane: - Profiles <-- Leaf Profiles - this is where the switch \"objects\" are configured - Policy Groups <-- - Overrides Navigate to Fabric > Access Policies > Switches > Leaf Switches > Profiles and expand the Profiles option. You will see three switch \"objects\" representing - LEAF-1 - LEAF-2 - LEAF-1 and LEAF-2 Pair When you want to configure a single interface on a specific leaf you will use the specific individual switch object. In our first example, we will configure a single access port on LEAF-2. Since the switches are abstracted, it is also possible to define an object that represents both switches. When configuring a vPC or two access ports using the same port on each specific switch, this pair object can be used. In our second example we will associate the same port, e1/11, to the LEAFS_102_103_LeafProf Leaf Profile. In one step or associate we have configured port e1/11 on both switches. In this exercise port e1/11 will be configured as a vPC.","title":"Switch Policies"},{"location":"02_lab_fabric_access_policies/#configuring-interface-policies-and-profiles","text":"Keeping in mind the full connectivity workflow of: Configure the Physical Layer Characteristics <-- You are here Configure the Logical Layer Characteristics Log in to your Student PC, open your browser, and access the APIC GUI.","title":"Configuring Interface Policies and Profiles"},{"location":"02_lab_fabric_access_policies/#step-1-creating-interface-policies-cdp-enabled","text":"Navigate to Fabric > Access Policies > Policies > Interface > CDP Interface Expand CDP Interface and note that there is a default policy that comes pre configured. This is the case with many objects and while the controller will use these default policies when needed and an explicit policy is not selected, it is a best practice to explicitly configure policies for the behavior you want. Create an explicit policy which enables CDP. Right Click on CDP Interface and select the Create CDP Interface Policy option. Configuration Item Configuration Name Configuration Values Interface Policy to enable CDP POD##_CDP_Enabled Example: POD11_CDP_Enabled (for POD11 Student) Admin State = Enabled You now have a policy to enable CDP on an interface which can be reused. Note that in ACI the preferred discovery protocol is LLDP. That is enabled by default across the fabric and is a key part of the discovery process. CDP is not enabled on the fabric by default after 4.0. In order to enable CDP on an interface you will use this new just created policy.","title":"Step 1 - Creating Interface Policies - CDP Enabled"},{"location":"02_lab_fabric_access_policies/#step-2-creating-interface-policies-interface-speed","text":"Navigate to Fabric > Access Policies > Policies > Interface > Link Level Right Click on Link Level and select the Create Link Level Policy option. Configuration Item Configuration Name Configuration Values Interface Policy to configure a 1 GigE Interface POD##_1G_IntPol Example: POD11_1G_IntPol (for POD11 Student) Speed: 1 Gbps Interface Policy to configure a 10 GigE Interface POD##_10G_IntPol Example: POD11_10G_IntPol (for POD11 Student) Speed: 10 Gbps","title":"Step 2 - Creating Interface Policies - Interface Speed"},{"location":"02_lab_fabric_access_policies/#step-3-creating-interface-policies-lldp-disable","text":"Navigate to Fabric > Access Policies > Policies > Interface > LLDP Interface Configuration Item Configuration Name Configuration Values Interface Policy to disable LLDP POD##_LLDP_Disabled Example: POD11_LLDP_Disabled Recieve State = Disabled Transmit State = Disabled","title":"Step 3 - Creating Interface Policies - LLDP Disable"},{"location":"02_lab_fabric_access_policies/#step-4-creating-interface-policies-static-port-channel","text":"Navigate to Fabric > Access Policies > Policies > Interface > Port Channel Configuration Item Configuration Name Configuration Values Interface Policy for a static port channel POD##_Static_Po Example: POD11_Static_Po Mode: Static Channel - Mode On Interface Policy for an active LACP port channel POD##_LACP_Active_Po Example: POD11_LACP_Active_Po Mode: LACP Active","title":"Step 4 - Creating Interface Policies - Static Port Channel"},{"location":"02_lab_fabric_access_policies/#step-5-creating-interface-policy-groups-access-interface","text":"Lets take these individual behaviors defined in each Interface Policy and bundle them into a group of behaviors that can then be associated with an interface. This bundle of configuration items defining the port behavior is called an Interface Policy Group . Lets define the Policy Group that would define a 1Gig interface with CDP enabled, LLD disabled, and a static port channel.","title":"Step 5 - Creating Interface Policy Groups - Access Interface"},{"location":"02_lab_fabric_access_policies/#step-6-creating-interface-policy-groups-access-interface","text":"","title":"Step 6 - Creating Interface Policy Groups - Access Interface"},{"location":"02_lab_fabric_access_policies/#check-in","text":"With these policies, we have now defined \"reusable behaviors\": 1 Gigabit Ethernet Interface Enable CDP Disable LLDP Static Port Channel Active LAC Port Channel We have also defined two It important to note that these \"behaviors\" are not associated with an interface or even a switch at this point. Think of them as \"raw ingredients\" that are ready for use in order to implement a specific functionality. We will then configure a 1Gig LACP port channel down to the \"mock-core01\" switch.","title":"Check in"},{"location":"03_lab_tenant_constructs/","text":"Lab 03 - Configure Tenants and Tenant Constructs \u00b6 Context \u00b6 Having completed the Fabric Policy configuration so that we have an operational Underlay (confusingly called \"Overlay-1\" as you will see when executing CLI commands) and set our Switch Policies (which define our switches) and Interface Policies, Profiles, and AEPs (which define interface behaviors and allowed encapsulations or Vlans) we now start to define the logical components that will make it very easy to secure hosts and services uniformly at scale without consideration of their IP addresses, rather via their EPG association. This logical separation begins with one or more Tenants. The fabric comes \"out of the box\" with three Tenants. infra A Tenant container for the ACI Fabric Infrastructure mgmt Dedicated tenant for managing the ACI Fabric common A tenant with special properties so that any construct configured within this tenant is automatically available to any other Tenant created in the fabric. This tenant is very useful for shared services and you will often find an L3Out , VRF, and Bridge Domains which are shared across other Tenants configured in the common tenant. Lab Goals \u00b6 Tenant portion of the ACI Management Information Tree (MIT) Understand the relationships between the ACI Logical Constructs Step 1: Create a Tenant and VRF Step 2: Create a Bridge Domain and Subnets Step 3: Create an Application Profile and EPGs Step 4: Create Filters and Contracts Step 5: Apply Contracts to EPGs This is the final \"set up only\" lab. At the completion of this lab, the fabric will be ready for actual traffic and connectivity. The following constructs or objects will be configured in this lab: Object Type Object Name Function Tenant POD##_Tenant Administrative domain containing all subsequent objects (VRFs, BDs, EPGs, Subnets, Application Profiles) VRF POD##_VRF Layer 3 routing and forwarding domain within a tenant Bridge Domain POD##_BD Logical container defining flooding behavior. Always associated with a single VRF and often associated with one ore more subnets. Subnet 10.0.1.254/24 IP Subnet Application Profile Tiered_AppProfile Container for EPGs and the policies which define associations, encapsulations, and interactions EGP - Web - App - DB Logical grouping of endpoints which have similar requirements, often security requirements. Reminder: ## = Your Pod Number. For example, if your Pod number is 11, then replace ## with 11 (POD11_Tenant) It is important to think about how to consistently name your constructs. You have some flexibility in the lab but please keep in mind that the lab is shared and so you should always make sure your constructs can be associated to your tenant and your work. Step 1 - Create a Tenant and VRF \u00b6 A Tenant in ACI represents a management domain. Common tenants in actual deployments include tenants such as: Production Dev QA DMZ As you can see from the MIT diagram, a Tenant contains one or more VRFs and so you often find that the Production tenant has a Production VRF associated with it. A VRF in ACI is a VRF. You cannot have overlapping IP Address space within an VRF and so the same is true for a VRF in ACI. In the lab, each lab participant will create a tenant based on their Pod assignment (Pod number). Navigate to Tenants > Add Tenant The Create Tenant dialog box will appear. The only required field is the Tenant name. Enter your Tenant name in the Name field and click Submit. Notice that here you can also associate a Monitoring Policy as well as Security Domains. If this Tenant had special access requirements, then here is where you can associate a specific Security Domain policy with the Tenant. This can be done after the Tenant is created. Because we left the \"Take me to this tenant when I click finish\" option checked, we will be taken to the new tenant page once Submit is clicked. From here navigate to Networking > VRFs and right click on the VRF Folder icon. Select Create VRF and the Create VRF dialog will appear. Step 2: Create a Bridge Domain and Subnets \u00b6 In traditional networking a Vlan inherently defines a broadcast domain, an encapsulation, and optionally a subnet. We don't typically think of those items as separate functions but in ACI you must. In its basic form, a Bridge Domain defines a broadcast domain as well as flooding behavior (we don't call this out specifically in traditional network because we dont' have any other options but in ACI we do). ACI can optimize flooding behavior and reduce broadcasts and so a Bridge Domain allows you to define how you want flooding to behave for a particular Bridge Domain (BD). It is also common to define a subnet within the Bridge Domain thereby creating the foundation for a Layer 3 \"Vlan\". Notice that you don't define the encapsulation within the Bridge Domain. That takes place within an EPG either statically or dynamically depending on the need. A Bridge Domain can have multiple subnets and contains settings which instantiate the subnet's gateway on the ACI Fabric. Step 3: Create an Application Profile and EPGs \u00b6 An Application Profile is on organizational or grouping construct only. It is a container or folder for EPGs and their associations. An EndPoint Group (EPG) is a container for endpoints that share some commonality. In many cases, these are endpoints that share a common security profile. This is an important distinction to make because Contracts (think of these as ACLs which act on EPGs rather than subnets and IP addresses) are applied to EPGs. Step 4: Create Filters and Contracts \u00b6 Step 5: Apply Contracts to EPGs \u00b6 Check in \u00b6","title":"Lab 03 - Tenants"},{"location":"03_lab_tenant_constructs/#lab-03-configure-tenants-and-tenant-constructs","text":"","title":"Lab 03 - Configure Tenants and Tenant Constructs"},{"location":"03_lab_tenant_constructs/#context","text":"Having completed the Fabric Policy configuration so that we have an operational Underlay (confusingly called \"Overlay-1\" as you will see when executing CLI commands) and set our Switch Policies (which define our switches) and Interface Policies, Profiles, and AEPs (which define interface behaviors and allowed encapsulations or Vlans) we now start to define the logical components that will make it very easy to secure hosts and services uniformly at scale without consideration of their IP addresses, rather via their EPG association. This logical separation begins with one or more Tenants. The fabric comes \"out of the box\" with three Tenants. infra A Tenant container for the ACI Fabric Infrastructure mgmt Dedicated tenant for managing the ACI Fabric common A tenant with special properties so that any construct configured within this tenant is automatically available to any other Tenant created in the fabric. This tenant is very useful for shared services and you will often find an L3Out , VRF, and Bridge Domains which are shared across other Tenants configured in the common tenant.","title":"Context"},{"location":"03_lab_tenant_constructs/#lab-goals","text":"Tenant portion of the ACI Management Information Tree (MIT) Understand the relationships between the ACI Logical Constructs Step 1: Create a Tenant and VRF Step 2: Create a Bridge Domain and Subnets Step 3: Create an Application Profile and EPGs Step 4: Create Filters and Contracts Step 5: Apply Contracts to EPGs This is the final \"set up only\" lab. At the completion of this lab, the fabric will be ready for actual traffic and connectivity. The following constructs or objects will be configured in this lab: Object Type Object Name Function Tenant POD##_Tenant Administrative domain containing all subsequent objects (VRFs, BDs, EPGs, Subnets, Application Profiles) VRF POD##_VRF Layer 3 routing and forwarding domain within a tenant Bridge Domain POD##_BD Logical container defining flooding behavior. Always associated with a single VRF and often associated with one ore more subnets. Subnet 10.0.1.254/24 IP Subnet Application Profile Tiered_AppProfile Container for EPGs and the policies which define associations, encapsulations, and interactions EGP - Web - App - DB Logical grouping of endpoints which have similar requirements, often security requirements. Reminder: ## = Your Pod Number. For example, if your Pod number is 11, then replace ## with 11 (POD11_Tenant) It is important to think about how to consistently name your constructs. You have some flexibility in the lab but please keep in mind that the lab is shared and so you should always make sure your constructs can be associated to your tenant and your work.","title":"Lab Goals"},{"location":"03_lab_tenant_constructs/#step-1-create-a-tenant-and-vrf","text":"A Tenant in ACI represents a management domain. Common tenants in actual deployments include tenants such as: Production Dev QA DMZ As you can see from the MIT diagram, a Tenant contains one or more VRFs and so you often find that the Production tenant has a Production VRF associated with it. A VRF in ACI is a VRF. You cannot have overlapping IP Address space within an VRF and so the same is true for a VRF in ACI. In the lab, each lab participant will create a tenant based on their Pod assignment (Pod number). Navigate to Tenants > Add Tenant The Create Tenant dialog box will appear. The only required field is the Tenant name. Enter your Tenant name in the Name field and click Submit. Notice that here you can also associate a Monitoring Policy as well as Security Domains. If this Tenant had special access requirements, then here is where you can associate a specific Security Domain policy with the Tenant. This can be done after the Tenant is created. Because we left the \"Take me to this tenant when I click finish\" option checked, we will be taken to the new tenant page once Submit is clicked. From here navigate to Networking > VRFs and right click on the VRF Folder icon. Select Create VRF and the Create VRF dialog will appear.","title":"Step 1 - Create a Tenant and VRF"},{"location":"03_lab_tenant_constructs/#step-2-create-a-bridge-domain-and-subnets","text":"In traditional networking a Vlan inherently defines a broadcast domain, an encapsulation, and optionally a subnet. We don't typically think of those items as separate functions but in ACI you must. In its basic form, a Bridge Domain defines a broadcast domain as well as flooding behavior (we don't call this out specifically in traditional network because we dont' have any other options but in ACI we do). ACI can optimize flooding behavior and reduce broadcasts and so a Bridge Domain allows you to define how you want flooding to behave for a particular Bridge Domain (BD). It is also common to define a subnet within the Bridge Domain thereby creating the foundation for a Layer 3 \"Vlan\". Notice that you don't define the encapsulation within the Bridge Domain. That takes place within an EPG either statically or dynamically depending on the need. A Bridge Domain can have multiple subnets and contains settings which instantiate the subnet's gateway on the ACI Fabric.","title":"Step 2: Create a Bridge Domain and Subnets"},{"location":"03_lab_tenant_constructs/#step-3-create-an-application-profile-and-epgs","text":"An Application Profile is on organizational or grouping construct only. It is a container or folder for EPGs and their associations. An EndPoint Group (EPG) is a container for endpoints that share some commonality. In many cases, these are endpoints that share a common security profile. This is an important distinction to make because Contracts (think of these as ACLs which act on EPGs rather than subnets and IP addresses) are applied to EPGs.","title":"Step 3: Create an Application Profile and EPGs"},{"location":"03_lab_tenant_constructs/#step-4-create-filters-and-contracts","text":"","title":"Step 4: Create Filters and Contracts"},{"location":"03_lab_tenant_constructs/#step-5-apply-contracts-to-epgs","text":"","title":"Step 5: Apply Contracts to EPGs"},{"location":"03_lab_tenant_constructs/#check-in","text":"","title":"Check in"},{"location":"04_lab_layer2_connectivity/","text":"Step4 Configure Host Connectivity Constructs (L2) and Layer 2 Links \u00b6 Context \u00b6 So far we have a working fabric, working interface behaviors and characteristics, and our logical overlay (Tenants, EPGs, etc.) Lab Goals \u00b6 Step 1: Create a Tenant and VRF Step 2: Create a Bridge Domain and Subnets Step 3: Create an Application Profile and EPGs Step 4: Create Filters and Contracts Step 5: Apply Contracts to EPGs Step 1 - \u00b6 Check in \u00b6","title":"Lab 04 - Host & Layer 2 Connectivity"},{"location":"04_lab_layer2_connectivity/#step4-configure-host-connectivity-constructs-l2-and-layer-2-links","text":"","title":"Step4 Configure Host Connectivity Constructs (L2) and Layer 2 Links"},{"location":"04_lab_layer2_connectivity/#context","text":"So far we have a working fabric, working interface behaviors and characteristics, and our logical overlay (Tenants, EPGs, etc.)","title":"Context"},{"location":"04_lab_layer2_connectivity/#lab-goals","text":"Step 1: Create a Tenant and VRF Step 2: Create a Bridge Domain and Subnets Step 3: Create an Application Profile and EPGs Step 4: Create Filters and Contracts Step 5: Apply Contracts to EPGs","title":"Lab Goals"},{"location":"04_lab_layer2_connectivity/#step-1-","text":"","title":"Step 1 -"},{"location":"04_lab_layer2_connectivity/#check-in","text":"","title":"Check in"},{"location":"05_lab_vmm_integration/","text":"","title":"Lab 05 - Integration with VMware ESXi"},{"location":"06_lab_layer3_connectivity/","text":"","title":"Lab 06 - External Layer 3 Connectivity"},{"location":"07_lab_operations/","text":"","title":"Lab 07 - Management & Operations"},{"location":"08_lab_fwl_integration/","text":"","title":"Lab 08 - Firewall Integration"},{"location":"09_lab_troubeleshooting/","text":"","title":"Lab 09 - Troubleshooting"},{"location":"10_lab_programmability/","text":"ACI Programmability \u00b6 Visore Postman Arya ACI Toolkit Cobra SDK Python Ansible","title":"Lab 10 - Programmability"},{"location":"10_lab_programmability/#aci-programmability","text":"Visore Postman Arya ACI Toolkit Cobra SDK Python Ansible","title":"ACI Programmability"},{"location":"11_lab_appcenter/","text":"","title":"Lab 11 - App Center and ACI Optimize Feature"},{"location":"12_lab_techsupport/","text":"Export Tech Support \u00b6 While ACI is now a mature technology, there are occaisions when you will need to contact Cisco TAC for support. Its always a good idea to have your current configuration documented and ACI makes that very simple. In Lab 07, we configured a remote location for daily backups and off-fabric snapshots. We will use thei same remote FTP server to export what is the equivalent of show tech.","title":"Lab 12 - Export Tech Support"},{"location":"12_lab_techsupport/#export-tech-support","text":"While ACI is now a mature technology, there are occaisions when you will need to contact Cisco TAC for support. Its always a good idea to have your current configuration documented and ACI makes that very simple. In Lab 07, we configured a remote location for daily backups and off-fabric snapshots. We will use thei same remote FTP server to export what is the equivalent of show tech.","title":"Export Tech Support"},{"location":"lab_access/","text":"Getting Around the Lab \u00b6 How this lab works and things to keep in mind... Lab Access \u00b6 Each student is assigned their own lab \"Pod\" (Tenant) in the fabric. Please refer to the Student Pod Table below for your Remote Desktop (RDP) IP Address and Credentials. Access to the lab environment is via the Remote Desktop Protocol (RDP) and so you will need an RDP client on your system in order to access the lab. Student Pod Table \u00b6 POD Number RDP IP RDP Port Username Password 11 65.49.10.72 7779 Claudia 1234QWer! Step 1 - RDP Access to Student PC \u00b6 Login with your credentials from the Student Pod Table Operating System Details Windows 10 From the Search box Type run and <Enter> In the Run dialog type mstsc /admin <Enter> Macintosh Microsoft Remote Desktop client on the Mac App Store See Microsoft Get started with the macOS client Tip: If you are behind a corporate firewall you may want to use Web RDP Web RDP Details: https://65.49.89.250/#/ POD Number Username Password 11 ACI-POD11 $VGi@IQezt1 Step 2 - Applications \u00b6 Once you sucesfully log in (Tip: Don't forget the RDP port number) you will see a customized Desktop which will have all the tools you need and from where you can access all the devices. Chrome Putty This lab guide is also available on the Desktop. IMPORTANT: Please review before continuing with the lab. \u00b6 LAB PREMISE \u00b6 This Lab has a total of 16 Pods. Some equipment is shared across all the Pods while other equipment is dedicated to each individual Pod. A Pod is a group of devices and resources which make an individual Lab usable for each student. Each Pod is identified by a two digit number. Throghout your lab guide, if you see ##, replace ## with your Pod number. ACI is fundamentally a multi-tenant environment and the lab environment makes full use of that capability. Each Student will create their own Tenant in their Pod and map all the policies, test servers, and equipment dedicated to the Pod to their own tenant. For example: The student assigned to POD11 will create their own POD11 tenant and configure access policies and virtual networking policies to the POD11 tenant. A naming standard is particularly important in an ACI Design. All policies will follow a naming standard based on your POD number. Common Equipment \u00b6 The table below details the shared equipment in the lab. This equipment will be shared by all students. Because these resources are shared, you will see configuration appear that is not your own. Please do not delete or change any configuration item that is not your own. Please be respectful of the other students and use only your POD resources. Table of Common (Shared) Devices and Access Information Device Management/Terminal Server IP Telnet Port Number Credentials APIC (apic.dc.local) 192.168.10.1 admin/1234QWer spine 192.168.250.202 7006 admin/1234QWer leaf-1 192.168.250.202 7007 admin/1234QWer leaf-2 192.168.250.202 7008 admin/1234QWer L2/L3 Switch 192.168.250.202 7009 admin/1234QWer AD/DNS/FTP Server 192.168.10.40 admin/1234QWer NTP Server 192.168.10.40 Dedicated Equipment \u00b6 This equipment is dedicated to every individual student. Each Student Pod has a dedicated ESXi Host, vCenter and a set of Linux based Virtual Machines. POD11 Equipment Device Management IP Fabric IP/ FQDN Username Password Linux VMs WEB 10.0.1.1/24 Root 1234Qwer APP 10.0.2.1/24 Roo 1234Qwer DB 10.0.3.1/24 Root 1234Qwer TRANSACT 10.0.4.1/24 Root 1234Qwer Virtualization Environment ESXi Host 192.168.10.211 ESXp11@dc.local Root 1234QWer vCenter 192.168.10.212 vcenterpod16.dc.local administrator@vsphere.local 1234QWer! Credentials Summary \u00b6 For ease of use, the Lab has minimized the number of credentials and uses a standard pattern for the password with minor variations. The table below summarizes all the credentilas you will need to access all the lab resources. Device Type Username/Password Network Devices (Including APIC) admin/1234QWer Linux Virtual Machines Root/1234Qwer ESXi Host Root/1234QWer vCenter administrator@vsphere.local/1234QWer! Physical Interface Reference \u00b6 ACI is fundamentally a networking technology and so throughout the labs you will need to configure interfaces. Use the Physical Interface Table below as a reference. SPINE-1 \u00b6 POD# PORT Connected To Device Connected to Device Port ALL PODS e1/1 leaf-1 e1/49 ALL PODS e1/2 leaf-2 e1/49 LEAF-1 \u00b6 POD# PORT Connected To Device Connected to Device Port ALL PODS e1/2 APIC VIC1 POD11 e1/3 UCS-SERVER-P11 VIC1 POD11 e1/11 ACI-P2-TOR (Cat 3750) Gi1/0/1 ALL PODS e1/49 SPINE-1 e1/1 LEAF-2 \u00b6 POD# PORT Connected To Device Connected to Device Port ALL PODS e1/2 APIC VIC2 POD11 e1/3 UCS-SERVER-P11 VIC2 POD11 e1/11 ACI-P2-TOR (Cat 3750) Gi1/0/2 ALL PODS e1/49 SPINE-1 e1/2 Layer 2 and Layer 3 Logical Configuration \u00b6 L3Out IP and Vlan Details \u00b6 POD Number OSPF AREA LEAF-1 Interface OSPF VLAN SVI on Layer 3 Switch SVI on APIC VLAN Pool Start VLAN Pool End POD11 11 e1/11 1112 172.16.11.2/30 172.16.11.1/30 1110 1119 Layer 2 Details \u00b6 POD Number SVI on External Layer 3 Device LEAF-2 Interface Layer 2 VLANS VLAN Pool Start VLAN Pool End POD11 10.0.2.99 e1/11 112 110 119 ASA Management Details \u00b6 POD Number ASA Management IP POD11 192.168.10.71 Lets get started with ACI! \u00b6","title":"Getting Around the Lab"},{"location":"lab_access/#getting-around-the-lab","text":"How this lab works and things to keep in mind...","title":"Getting Around the Lab"},{"location":"lab_access/#lab-access","text":"Each student is assigned their own lab \"Pod\" (Tenant) in the fabric. Please refer to the Student Pod Table below for your Remote Desktop (RDP) IP Address and Credentials. Access to the lab environment is via the Remote Desktop Protocol (RDP) and so you will need an RDP client on your system in order to access the lab.","title":"Lab Access"},{"location":"lab_access/#student-pod-table","text":"POD Number RDP IP RDP Port Username Password 11 65.49.10.72 7779 Claudia 1234QWer!","title":"Student Pod Table"},{"location":"lab_access/#step-1-rdp-access-to-student-pc","text":"Login with your credentials from the Student Pod Table Operating System Details Windows 10 From the Search box Type run and <Enter> In the Run dialog type mstsc /admin <Enter> Macintosh Microsoft Remote Desktop client on the Mac App Store See Microsoft Get started with the macOS client Tip: If you are behind a corporate firewall you may want to use Web RDP Web RDP Details: https://65.49.89.250/#/ POD Number Username Password 11 ACI-POD11 $VGi@IQezt1","title":"Step 1 - RDP Access to Student PC"},{"location":"lab_access/#step-2-applications","text":"Once you sucesfully log in (Tip: Don't forget the RDP port number) you will see a customized Desktop which will have all the tools you need and from where you can access all the devices. Chrome Putty This lab guide is also available on the Desktop.","title":"Step 2 - Applications"},{"location":"lab_access/#important-please-review-before-continuing-with-the-lab","text":"","title":"IMPORTANT:  Please review before continuing with the lab."},{"location":"lab_access/#lab-premise","text":"This Lab has a total of 16 Pods. Some equipment is shared across all the Pods while other equipment is dedicated to each individual Pod. A Pod is a group of devices and resources which make an individual Lab usable for each student. Each Pod is identified by a two digit number. Throghout your lab guide, if you see ##, replace ## with your Pod number. ACI is fundamentally a multi-tenant environment and the lab environment makes full use of that capability. Each Student will create their own Tenant in their Pod and map all the policies, test servers, and equipment dedicated to the Pod to their own tenant. For example: The student assigned to POD11 will create their own POD11 tenant and configure access policies and virtual networking policies to the POD11 tenant. A naming standard is particularly important in an ACI Design. All policies will follow a naming standard based on your POD number.","title":"LAB PREMISE"},{"location":"lab_access/#common-equipment","text":"The table below details the shared equipment in the lab. This equipment will be shared by all students. Because these resources are shared, you will see configuration appear that is not your own. Please do not delete or change any configuration item that is not your own. Please be respectful of the other students and use only your POD resources. Table of Common (Shared) Devices and Access Information Device Management/Terminal Server IP Telnet Port Number Credentials APIC (apic.dc.local) 192.168.10.1 admin/1234QWer spine 192.168.250.202 7006 admin/1234QWer leaf-1 192.168.250.202 7007 admin/1234QWer leaf-2 192.168.250.202 7008 admin/1234QWer L2/L3 Switch 192.168.250.202 7009 admin/1234QWer AD/DNS/FTP Server 192.168.10.40 admin/1234QWer NTP Server 192.168.10.40","title":"Common Equipment"},{"location":"lab_access/#dedicated-equipment","text":"This equipment is dedicated to every individual student. Each Student Pod has a dedicated ESXi Host, vCenter and a set of Linux based Virtual Machines. POD11 Equipment Device Management IP Fabric IP/ FQDN Username Password Linux VMs WEB 10.0.1.1/24 Root 1234Qwer APP 10.0.2.1/24 Roo 1234Qwer DB 10.0.3.1/24 Root 1234Qwer TRANSACT 10.0.4.1/24 Root 1234Qwer Virtualization Environment ESXi Host 192.168.10.211 ESXp11@dc.local Root 1234QWer vCenter 192.168.10.212 vcenterpod16.dc.local administrator@vsphere.local 1234QWer!","title":"Dedicated Equipment"},{"location":"lab_access/#credentials-summary","text":"For ease of use, the Lab has minimized the number of credentials and uses a standard pattern for the password with minor variations. The table below summarizes all the credentilas you will need to access all the lab resources. Device Type Username/Password Network Devices (Including APIC) admin/1234QWer Linux Virtual Machines Root/1234Qwer ESXi Host Root/1234QWer vCenter administrator@vsphere.local/1234QWer!","title":"Credentials Summary"},{"location":"lab_access/#physical-interface-reference","text":"ACI is fundamentally a networking technology and so throughout the labs you will need to configure interfaces. Use the Physical Interface Table below as a reference.","title":"Physical Interface Reference"},{"location":"lab_access/#spine-1","text":"POD# PORT Connected To Device Connected to Device Port ALL PODS e1/1 leaf-1 e1/49 ALL PODS e1/2 leaf-2 e1/49","title":"SPINE-1"},{"location":"lab_access/#leaf-1","text":"POD# PORT Connected To Device Connected to Device Port ALL PODS e1/2 APIC VIC1 POD11 e1/3 UCS-SERVER-P11 VIC1 POD11 e1/11 ACI-P2-TOR (Cat 3750) Gi1/0/1 ALL PODS e1/49 SPINE-1 e1/1","title":"LEAF-1"},{"location":"lab_access/#leaf-2","text":"POD# PORT Connected To Device Connected to Device Port ALL PODS e1/2 APIC VIC2 POD11 e1/3 UCS-SERVER-P11 VIC2 POD11 e1/11 ACI-P2-TOR (Cat 3750) Gi1/0/2 ALL PODS e1/49 SPINE-1 e1/2","title":"LEAF-2"},{"location":"lab_access/#layer-2-and-layer-3-logical-configuration","text":"","title":"Layer 2 and Layer 3 Logical Configuration"},{"location":"lab_access/#l3out-ip-and-vlan-details","text":"POD Number OSPF AREA LEAF-1 Interface OSPF VLAN SVI on Layer 3 Switch SVI on APIC VLAN Pool Start VLAN Pool End POD11 11 e1/11 1112 172.16.11.2/30 172.16.11.1/30 1110 1119","title":"L3Out IP and Vlan Details"},{"location":"lab_access/#layer-2-details","text":"POD Number SVI on External Layer 3 Device LEAF-2 Interface Layer 2 VLANS VLAN Pool Start VLAN Pool End POD11 10.0.2.99 e1/11 112 110 119","title":"Layer 2 Details"},{"location":"lab_access/#asa-management-details","text":"POD Number ASA Management IP POD11 192.168.10.71","title":"ASA Management Details"},{"location":"lab_access/#lets-get-started-with-aci","text":"","title":"Lets get started with ACI!"},{"location":"mock-core01-config/","text":"Layer 2/Layer 3 Switch Configuration \u00b6 ! banner login ^CCC +-------------------------------------------------+ | ___ _ _ __ __ _ _ | | | _| |___ _ _ _| | \\ \\_ _| | ___| |_ | | | <_| / . | | / . | | | | |_<_> | . \\ | | `___|_\\___`___\\___|_|_|_`_. |___<___|___/ | | <___' | | | +-------------------------------------------------+ Device: Mock Core Layer 3 Switch ^C ! interface Vlan1 ip address 10.1.10.102 255.255.255.0 no ip route-cache no ip mroute-cache end hostname mock-core01 ip domain-name dc.local crypto key gen rsa line vty 0 4 line console 0 logging synchronous login local enable secret 1234QWer aaa new-model username admin password 1234QWer username admin priv 16","title":"Layer 2/Layer 3 Switch Configuration"},{"location":"mock-core01-config/#layer-2layer-3-switch-configuration","text":"! banner login ^CCC +-------------------------------------------------+ | ___ _ _ __ __ _ _ | | | _| |___ _ _ _| | \\ \\_ _| | ___| |_ | | | <_| / . | | / . | | | | |_<_> | . \\ | | `___|_\\___`___\\___|_|_|_`_. |___<___|___/ | | <___' | | | +-------------------------------------------------+ Device: Mock Core Layer 3 Switch ^C ! interface Vlan1 ip address 10.1.10.102 255.255.255.0 no ip route-cache no ip mroute-cache end hostname mock-core01 ip domain-name dc.local crypto key gen rsa line vty 0 4 line console 0 logging synchronous login local enable secret 1234QWer aaa new-model username admin password 1234QWer username admin priv 16","title":"Layer 2/Layer 3 Switch Configuration"},{"location":"topology/","text":"Lab Topology \u00b6 The lab topology is designed to allow configuration of the most common scenarios. Once the Fabric has been discovered and configured and the Tenant design applied, the following functionality can be configured: Layer 3 Routing Layer 2 Connectivity to a Legacy Network Layer 2 Virtual Port Channel A note about Border Leafs. This designation is common in an ACI fabric along with \"Compute Leafs\" and even \"Storage Leafs\". It important to know that this designation is merely an convention to identify the leaf pair that hosts all external connectivity external to the fabric (Border Leafs) or to identify the leaf pairs that are used for host connectivity (Compute Leafs).","title":"Topology & Design"},{"location":"topology/#lab-topology","text":"The lab topology is designed to allow configuration of the most common scenarios. Once the Fabric has been discovered and configured and the Tenant design applied, the following functionality can be configured: Layer 3 Routing Layer 2 Connectivity to a Legacy Network Layer 2 Virtual Port Channel A note about Border Leafs. This designation is common in an ACI fabric along with \"Compute Leafs\" and even \"Storage Leafs\". It important to know that this designation is merely an convention to identify the leaf pair that hosts all external connectivity external to the fabric (Border Leafs) or to identify the leaf pairs that are used for host connectivity (Compute Leafs).","title":"Lab Topology"}]}