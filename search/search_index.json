{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to CloudMyLab ACI 4 Lab Guide \u00b6 This is a companion guide to the CloudMyLab Cisco Data Center CCIE Rental Rack. This lab guide will go through a typical ACI deployment workflow.","title":"Home"},{"location":"#welcome-to-cloudmylab-aci-4-lab-guide","text":"This is a companion guide to the CloudMyLab Cisco Data Center CCIE Rental Rack. This lab guide will go through a typical ACI deployment workflow.","title":"Welcome to CloudMyLab ACI 4 Lab Guide"},{"location":"01_lab_fabric_discovery/","text":"Lab 01 - Fabric Inventory and Discovery \u00b6 Physical Build \u00b6 Its important to be familiar with the physical configuration of each node in your ACI fabric. A leaf, spine, or apic is a \"Node\" in your ACI fabric and the node numbering is important. In our lab we have a Nexus 9336 Spine, two Nexus 9396 Leafs, and a single APIC-Server-M1. Hardware Overview \u00b6 Model Name View Information N9K-C9336PQ Spine-1 Hardware Overview N9K-C9372PX-E LEAF-1 Hardware Overview N9K-C9372PX-E LEAF-2 Hardware Overview APIC-SERVER-M1 apic1 This configuration is perfectly valid for a Lab but it is not valid for a production environment. The minimum physical fabric hardware for a production environment includes two spines, two leafs, and three APICs. Fabric Turn Up \u00b6 It is important to know that the initial turn up and device discovery and registration has already ocurred. You are accessing the lab after this step has been completed. A fabric turn up is typically performed on site. Physical Connectivity \u00b6 The management network is up and configured All the management interfaces of the spines and leafs are connected to the management network Optionally, all the console interfaces of the spines and leafs are connected to a terminal server Each leaf switch has a fabric uplink to each spine Each APIC has a CIMC connection to the management network a Managmenet connection to the management network Redundant 10G fabric uplinks Note: Out of the box (before discoverying and registering the swith in ACI), you can connect to an ACI switch via the console port. In this state the password for the admin account is blank. Logical Parameters \u00b6 Paramenter Use Lab Value Pod Number Numeric identifier for each ACI Pod&lt;br&gt;Default: 1 1 TEP Pool Default: 10.0.0.0/16 10.0.0.0/16 TEP Vlan Default: None Management Subnet/Mask Default: 192.168.10.0/24 192.168.10.0/24 Management Network Gateway Default: None 192.168.10.254/24 2019 Melbourne Cisco Live How to Setup an ACI Fabric from Scratch - BRKACI-2004 - 2019 Melbourne Cisco Live Explore the Cisco ACI GUI \u00b6 Now that you are familiar with the physical components of the Lab, lets investigate the APIC GUI and the topology from the APIC controller. Step 1 - Connect to Student PC \u00b6 Connect to your Student PC. See the Getting Around section for details. Step 2 - Login to the APIC \u00b6 From your Student PC, open a browser. Google Chrome is recommended for managing the APIC. https://192.168.10.1 or https://apic.dc.local Accept the security warning or create a security exception to access the GUI with the self signed certificate. Note that Secure HTTP (https) is required to access the APIC GUI by default. Insecure HTTP (http) must be explicitly enabled and is not recommended in a production environment. Login to the APIC. Note the warnings which will flash in the uppler right corner. You will see a Critical warning that the cluster does not contain 3 controllers. You may also see a Major warning regarding Licensing. This is expected in the Lab environment. Should you see these warning in a production environment, they must be corrected. You will see the \"What's New\" dialog and the main APIC Dashboard behding the dialog. Skim through the What's New dialog and close it. Step 3 - Areas of the APIC GUI \u00b6 Examine the the top-most section of the GUI interface. This top ribbon containing the main functional areas of the fabric (System, Tenants, Fabric, Virtual Networking, L4-L7 Services, Admin, Operations, Apps) is known as the Menu Bar . Menu Bar \u00b6 You will use it to navigate to the area of the ACI Fabric you need to view or update. Notice the shading and highlighting to help orient you in the GUI. The Menu bar shows that we are in the Dashboard section of the System menu. ACI GUI Menu Options Menu Headings/Tabs Description System Upon login, the GUI defaults to the System Menu Dashboard which provides the health status of the system. From the System menu tab other settings and licensing options are available along with events and faults. Tenants The Tenants Menu provides access to all tenants configured in the fabric and their logical configuration objects. Fabric The Fabric Menu provides access to inventory details, Fabric Policies, and Access Policies. Virtual Networking The Virtual Networking Menu displays and configures the fabric Virtual Machine Managers (VMMs). L4-L7 Services The L4-L7 Services Menu displays and configures the fabric Virtual Machine Managers (VMMs). Admin The Admin Menu displays and configures administrative functions such as authentication, authorization, and accounting functions, scheduling policies, retaining and purging records, upgrading firmware, and controlling features such as syslog, Call Home, and SNMP. Operations The Operations Menu provides access to operational functions including:<br>- Visibility & Troubleshooting<br>- Capacity Dashboard<br>- EP Tracker<br>- Visualiztion Apps The Apps tab displays all the applications installed or uploaded to APIC. The tab allows an APIC administrator to upload, enable, upgrade, install, or uninstall a packaged application in APIC. Areas of the APIC GUI \u00b6 The APIC or controller GUI has 4 main areas: - Menu Bar - Submenu Bar - Navigation Pane - Work Pane Select the Tenants menu. You will see a list of the default or pre-defined tenants which come with ACI \"out of the box\". Select the common tenant. You will see the standard tenants options listed collapsed in the Navigation Pane on the left side. If you select an option in the Navigation Pane, the objects pertaining to that selection are shows in the Work Pane to the right of the Navigation Pane. The Work Pane displays details about the option selected in the Navigation Pane. Fabric configuration via the GUI is typically performed in the Work Pane. Step 4 - Menu Bar and Navigation Conventions \u00b6 Take some time to select each Menu Bar option and get comfortable moving around in the GUI. For the remainder of the Lab the following convention will be used to guide the Student in navigating the GUI: Menu Bar Option > Submenu Option > Navigation Pane Option(s) > Work Pane Tabs From where you are in the Tenants menu navigate to: System > Dashboard to get back to the Health Dashboard of the Fabric. Notice that when you select the * System Menu option, you will automatically go to the Dashboard by default. GUI Tips \u00b6 Wherever there is a submit button and you are trying to make a change - click it. Some changes won\u2019t require it so the inconsistency sometimes calls that into question. Refresh - sometimes your changes won\u2019t appear until you do, you will see the little circular refresh button on most screens Hover over icons with your mouse for a few seconds to view the icon description Exporing Fabric Inventory, Nodes, and Fabric Topology \u00b6 Now that there is some familiarity with the GUI, lets validate the topology of the fabric. Step 1 - View and Explore the Toplogy \u00b6 Navigate to Fabric > Inventory > Topology . The Work Pane opens into the Summary tab. To view the topology diagram, click on the Topology tab in the Work Pane. Note that the full path would be shown as: Fabric > Inventory > Topology > Topology Navigation paths like this are not uncommon in the ACI GUI. Recall that the format we will follow throughout the lab is: Menu Bar Option > Submenu Option > Navigation Pane Option(s) > Work Pane Tabs From the Topology tab in the work pan verify that the displayed topology reflects the lab design. - One Spine - Two Leaf switches - One APIC server dual-homed to both leaf switches Note: You will see the same topology view if you go to Fabric > Inventory > Pod 1 > Topology Device Summary via hover over device icon \u00b6 Hover over each device icon for a very useful physical summary of the device. Device Connectivity \u00b6 Double click each device icon to view a list of connections. Step 2 - Fabric Membership \u00b6 Navigate to Fabric > Inventory > Fabric Membership . Here you will see the fabric inventory including serial number, Pod, Node ID, Model, Role, Fabric IP, and Status. The Pod, Node ID, and Role are defined during fabric discovery. You will notice that the IP comes from the TEP Pool that was provided during the apic intitial confirguration. Notice the additional tabs including Nodes Pending Registration . This tab is used to register new devices to the fabric. Double clicking on one of the device rows will display a dialog with device details. Examine each device and note the details that are available including certificate information. Step 3 - Pod View \u00b6 Navigate to Fabric > Inventory > Pod1 . Expand Pod1 by clicking on the \">\" symbol. Each device in the fabric will be listed. Expand one of the devices and review the sections available. Click on one of the devices (LEAF-1 is showd in the section below). The Summary tab for the device will appear in the Work Pane. Select the General tab in the Work Pane to view additional information about the device. Navigate to Fabric > Inventory > Pod1 > LEAF-1 (Node-102) > Interfaces > Physical Interfaces . Review the interfaces for the devince and note their operational status, Usage and other useful information. Step 4 - CLI \u00b6 Use PUTTY on the Student PC Desktop to connect to the APIC via SSH. Run the acidiag -h command to view the available ACI diagnotics options of the acidiag command. apic1# acidiag -h usage: acidiag [-h] [-v] {avread,fnvread,fnvreadex,rvread,rvreadle,crashsuspecttracker,bootother,bootcurr,journal,logs,oob,scheduler,cleanup,hwcheck,dbgtoken,validateimage,validatenginxconf,version,preservelogs,platform,verifyapic,bond0test,linkflap,touch,run,installer,start,stop,restart,dmestack,dmecore,reboot,drrmode,vapicjoin} ... positional arguments: {avread,fnvread,fnvreadex,rvread,rvreadle,crashsuspecttracker,bootother,bootcurr,journal,logs,oob,scheduler,cleanup,hwcheck,dbgtoken,validateimage,validatenginxconf,version,preservelogs,platform,verifyapic,bond0test,linkflap,touch,run,installer,start,stop,restart,dmestack,dmecore,reboot,drrmode,vapicjoin} sub-command help avread read appliance vector fnvread read fabric node vector fnvreadex read fabric node vector (extended mode) rvread read replica vector rvreadle read replica leader summary crashsuspecttracker read crash suspect tracker state bootother on next boot, boot other Linux Partition, and display updated /etc/grub.conf bootcurr on next boot, boot current Linux Partition, and display updated /etc/grub.conf journal Contents of journal logs logs show log history oob oob options cleanup fs cleanup utility hwcheck Quick check of APIC Hardware dbgtoken show debug token validateimage validate image validatenginxconf validate nginx conf version show ISO version preservelogs stash away logs in preparation for hard reboot platform show platform verifyapic run apic installation verify command bond0test ==SUPPRESS== linkflap flap a link touch touch special files run run specific commands and capture output installer installer start start a service stop stop a service restart restart a service reboot reboot drrmode drrmode options vapicjoin join existing vapic cluster optional arguments: -h, --help show this help message and exit -v, --verbose verbose apic1# Use acidiag fnvread CLI command to view the fabric devices (nodes). apic1# acidiag fnvread ID Pod ID Name Serial Number IP Address Role State LastUpdMsgId -------------------------------------------------------------------------------------------------------------- 101 1 Spine-1 SAL1948TWWP 10.0.72.97/32 spine active 0 102 1 LEAF-1 SAL1948U33K 10.0.72.98/32 leaf active 0 103 1 LEAF-2 SAL1948U35D 10.0.72.96/32 leaf active 0 Total 3 nodes Use the acidiag verifyapic CLI command to view the APIC status. apic1# acidiag verifyapic openssl_check: certificate details subject= CN=FCH1830V38S,serialNumber=PID:APIC-SERVER-M1 SN:FCH1830V38S issuer= CN=Cisco Manufacturing CA,O=Cisco Systems notBefore=Oct 11 08:42:21 2014 GMT notAfter=Oct 11 08:52:21 2024 GMT openssl_check: passed ssh_check: passed all_checks: passed apic1# Verify that you can enter configuration mode. apic1# config apic1(config)# exit apic1# config t apic1(config)# exit apic1# Verify that you can view the configuration using the usual show commands. apic1# sh run # Command: show running-config # Time: Sun May 31 21:56:08 2020 aaa banner 'Application Policy Infrastructure Controller' aaa authentication login console exit aaa authentication login default exit aaa authentication login domain fallback exit bgp-fabric exit coop-fabric exit no password pwd-strength-check crypto aes exit crypto webtoken session-record-flags login,logout,refresh exit rbac security-domain \"all\" exit rbac security-domain \"mgmt\" exit --More-- Because the APIC serves as the controller for the entire fabric, it is often simpler to log on to the APIC and execute show commands across the fabric. In some cases, you may want to log in to a particular leaf or spine. Node ID Management IP 101 192.168.10.101 102 192.168.10.102 103 192.168.10.103 Establish an SSH connection to each device in the table above and execute some common show commands. You can start with the \"show lldp neighbor\" command. Spine-1# show lldp nei Capability codes: (R) Router, (B) Bridge, (T) Telephone, (C) DOCSIS Cable Device (W) WLAN Access Point, (P) Repeater, (S) Station, (O) Other Device ID Local Intf Hold-time Capability Port ID LEAF-1 Eth1/1 120 BR Eth1/49 LEAF-2 Eth1/2 120 BR Eth1/49 Total entries displayed: 2 Spine-1# Spine-1# sh lldp nei sh: lldp: No such file or directory Spine-1# Note that the \"sh lldp nei\" command failed. Remember that many of the common abbreviations for commands are not accepted by ACI. CLI Tips \u00b6 Use or for command completion Not all command shortcuts are accepted in the ACI CLI. More and more are accepted with every new version of ACI but its a good idea to get into the habit of typing out the full commands. Skills you should have after completing this lab \u00b6 After completing this labs you should: - be familiar with the hardware components of the lab - be familiar with the APIC GUI, its high level menu options, and how to navigate through the GUI expanding and selecting options - be able to explore the fabric inventory and determine model, status, and connectivity information - understand where to go to add new devices to the fabric - explore fabric inventory via the CLI Supplemental Information \u00b6 Configuring Out of Band Management \u00b6 This is not a lab but you can follow along. In Step 4 of Exploring the Fabric Inventory, you established an SSH connection to each leaf and spine and executed the \"show lldp command\". You were able to do this because the managment interfaces had already benn configured. This section details how that is done. Configuring out of band management for the fabric is only done once as part of fabric turn up. This activity effectivley configures an IP address on the device management interface so that the device is reachable via SSH. Because it is an essential step in the turn up of an ACI fabric, instructions are provided here for completeness but please do not perform these actions on your student fabric. Tenant mgmt \u00b6 Go to the Tenants* menu and select the mgmt** tenant from the subment or from the list in the expanded Work Pane. From the mgmt Tenant Navigation Pane, from the tenant navigation Pane navigate to Tenant mgmt > Node Management Addresses > Static Node Management Addresses Form here you have two options to get to the configuration dialog for a node management address. 1. Right click on the Static Node Management Addresses option in the Navigation Pane 2. Click on the tool icon drop down in the Work Pane. Both of these actions are equivalent and will present you with a dialog to create a static node management address. In ACI, this is the equivalent of assigning a management IP to the management port of the device. In ACI you often use the Node ID to identify the device and that is the case here. You can enter a range, as shown below with a starting IP in the management subnet and generally the IPs will be assigned sequentially. Note: The device hostname, credentials, and other settings are configured on the switch by the APIC when it is first discovered. While the range option is handy, there may be unintended consequences or if you have an IP addressing convention (Node ID mapped to last octet is a good practice) the assignments many not adhere to your convention. Its a good practice to do the IP assignment individually on each node. Note the range from and to values are for a single node. The Lab uses the default OOB (Out of Band) Management EPG, however in a production data center it is a good pracitce to configure and explicit management EPG.","title":"Lab 01 - Fabric Discovery"},{"location":"01_lab_fabric_discovery/#lab-01-fabric-inventory-and-discovery","text":"","title":"Lab 01 - Fabric Inventory and Discovery"},{"location":"01_lab_fabric_discovery/#physical-build","text":"Its important to be familiar with the physical configuration of each node in your ACI fabric. A leaf, spine, or apic is a \"Node\" in your ACI fabric and the node numbering is important. In our lab we have a Nexus 9336 Spine, two Nexus 9396 Leafs, and a single APIC-Server-M1.","title":"Physical Build"},{"location":"01_lab_fabric_discovery/#hardware-overview","text":"Model Name View Information N9K-C9336PQ Spine-1 Hardware Overview N9K-C9372PX-E LEAF-1 Hardware Overview N9K-C9372PX-E LEAF-2 Hardware Overview APIC-SERVER-M1 apic1 This configuration is perfectly valid for a Lab but it is not valid for a production environment. The minimum physical fabric hardware for a production environment includes two spines, two leafs, and three APICs.","title":"Hardware Overview"},{"location":"01_lab_fabric_discovery/#fabric-turn-up","text":"It is important to know that the initial turn up and device discovery and registration has already ocurred. You are accessing the lab after this step has been completed. A fabric turn up is typically performed on site.","title":"Fabric Turn Up"},{"location":"01_lab_fabric_discovery/#physical-connectivity","text":"The management network is up and configured All the management interfaces of the spines and leafs are connected to the management network Optionally, all the console interfaces of the spines and leafs are connected to a terminal server Each leaf switch has a fabric uplink to each spine Each APIC has a CIMC connection to the management network a Managmenet connection to the management network Redundant 10G fabric uplinks Note: Out of the box (before discoverying and registering the swith in ACI), you can connect to an ACI switch via the console port. In this state the password for the admin account is blank.","title":"Physical Connectivity"},{"location":"01_lab_fabric_discovery/#logical-parameters","text":"Paramenter Use Lab Value Pod Number Numeric identifier for each ACI Pod&lt;br&gt;Default: 1 1 TEP Pool Default: 10.0.0.0/16 10.0.0.0/16 TEP Vlan Default: None Management Subnet/Mask Default: 192.168.10.0/24 192.168.10.0/24 Management Network Gateway Default: None 192.168.10.254/24 2019 Melbourne Cisco Live How to Setup an ACI Fabric from Scratch - BRKACI-2004 - 2019 Melbourne Cisco Live","title":"Logical Parameters"},{"location":"01_lab_fabric_discovery/#explore-the-cisco-aci-gui","text":"Now that you are familiar with the physical components of the Lab, lets investigate the APIC GUI and the topology from the APIC controller.","title":"Explore the Cisco ACI GUI"},{"location":"01_lab_fabric_discovery/#step-1-connect-to-student-pc","text":"Connect to your Student PC. See the Getting Around section for details.","title":"Step 1 - Connect to Student PC"},{"location":"01_lab_fabric_discovery/#step-2-login-to-the-apic","text":"From your Student PC, open a browser. Google Chrome is recommended for managing the APIC. https://192.168.10.1 or https://apic.dc.local Accept the security warning or create a security exception to access the GUI with the self signed certificate. Note that Secure HTTP (https) is required to access the APIC GUI by default. Insecure HTTP (http) must be explicitly enabled and is not recommended in a production environment. Login to the APIC. Note the warnings which will flash in the uppler right corner. You will see a Critical warning that the cluster does not contain 3 controllers. You may also see a Major warning regarding Licensing. This is expected in the Lab environment. Should you see these warning in a production environment, they must be corrected. You will see the \"What's New\" dialog and the main APIC Dashboard behding the dialog. Skim through the What's New dialog and close it.","title":"Step 2 - Login to the APIC"},{"location":"01_lab_fabric_discovery/#step-3-areas-of-the-apic-gui","text":"Examine the the top-most section of the GUI interface. This top ribbon containing the main functional areas of the fabric (System, Tenants, Fabric, Virtual Networking, L4-L7 Services, Admin, Operations, Apps) is known as the Menu Bar .","title":"Step 3 - Areas of the APIC GUI"},{"location":"01_lab_fabric_discovery/#menu-bar","text":"You will use it to navigate to the area of the ACI Fabric you need to view or update. Notice the shading and highlighting to help orient you in the GUI. The Menu bar shows that we are in the Dashboard section of the System menu. ACI GUI Menu Options Menu Headings/Tabs Description System Upon login, the GUI defaults to the System Menu Dashboard which provides the health status of the system. From the System menu tab other settings and licensing options are available along with events and faults. Tenants The Tenants Menu provides access to all tenants configured in the fabric and their logical configuration objects. Fabric The Fabric Menu provides access to inventory details, Fabric Policies, and Access Policies. Virtual Networking The Virtual Networking Menu displays and configures the fabric Virtual Machine Managers (VMMs). L4-L7 Services The L4-L7 Services Menu displays and configures the fabric Virtual Machine Managers (VMMs). Admin The Admin Menu displays and configures administrative functions such as authentication, authorization, and accounting functions, scheduling policies, retaining and purging records, upgrading firmware, and controlling features such as syslog, Call Home, and SNMP. Operations The Operations Menu provides access to operational functions including:<br>- Visibility & Troubleshooting<br>- Capacity Dashboard<br>- EP Tracker<br>- Visualiztion Apps The Apps tab displays all the applications installed or uploaded to APIC. The tab allows an APIC administrator to upload, enable, upgrade, install, or uninstall a packaged application in APIC.","title":"Menu Bar"},{"location":"01_lab_fabric_discovery/#areas-of-the-apic-gui","text":"The APIC or controller GUI has 4 main areas: - Menu Bar - Submenu Bar - Navigation Pane - Work Pane Select the Tenants menu. You will see a list of the default or pre-defined tenants which come with ACI \"out of the box\". Select the common tenant. You will see the standard tenants options listed collapsed in the Navigation Pane on the left side. If you select an option in the Navigation Pane, the objects pertaining to that selection are shows in the Work Pane to the right of the Navigation Pane. The Work Pane displays details about the option selected in the Navigation Pane. Fabric configuration via the GUI is typically performed in the Work Pane.","title":"Areas of the APIC GUI"},{"location":"01_lab_fabric_discovery/#step-4-menu-bar-and-navigation-conventions","text":"Take some time to select each Menu Bar option and get comfortable moving around in the GUI. For the remainder of the Lab the following convention will be used to guide the Student in navigating the GUI: Menu Bar Option > Submenu Option > Navigation Pane Option(s) > Work Pane Tabs From where you are in the Tenants menu navigate to: System > Dashboard to get back to the Health Dashboard of the Fabric. Notice that when you select the * System Menu option, you will automatically go to the Dashboard by default.","title":"Step 4 - Menu Bar and Navigation Conventions"},{"location":"01_lab_fabric_discovery/#gui-tips","text":"Wherever there is a submit button and you are trying to make a change - click it. Some changes won\u2019t require it so the inconsistency sometimes calls that into question. Refresh - sometimes your changes won\u2019t appear until you do, you will see the little circular refresh button on most screens Hover over icons with your mouse for a few seconds to view the icon description","title":"GUI Tips"},{"location":"01_lab_fabric_discovery/#exporing-fabric-inventory-nodes-and-fabric-topology","text":"Now that there is some familiarity with the GUI, lets validate the topology of the fabric.","title":"Exporing Fabric Inventory, Nodes, and Fabric Topology"},{"location":"01_lab_fabric_discovery/#step-1-view-and-explore-the-toplogy","text":"Navigate to Fabric > Inventory > Topology . The Work Pane opens into the Summary tab. To view the topology diagram, click on the Topology tab in the Work Pane. Note that the full path would be shown as: Fabric > Inventory > Topology > Topology Navigation paths like this are not uncommon in the ACI GUI. Recall that the format we will follow throughout the lab is: Menu Bar Option > Submenu Option > Navigation Pane Option(s) > Work Pane Tabs From the Topology tab in the work pan verify that the displayed topology reflects the lab design. - One Spine - Two Leaf switches - One APIC server dual-homed to both leaf switches Note: You will see the same topology view if you go to Fabric > Inventory > Pod 1 > Topology","title":"Step 1 - View and Explore the Toplogy"},{"location":"01_lab_fabric_discovery/#device-summary-via-hover-over-device-icon","text":"Hover over each device icon for a very useful physical summary of the device.","title":"Device Summary via hover over device icon"},{"location":"01_lab_fabric_discovery/#device-connectivity","text":"Double click each device icon to view a list of connections.","title":"Device Connectivity"},{"location":"01_lab_fabric_discovery/#step-2-fabric-membership","text":"Navigate to Fabric > Inventory > Fabric Membership . Here you will see the fabric inventory including serial number, Pod, Node ID, Model, Role, Fabric IP, and Status. The Pod, Node ID, and Role are defined during fabric discovery. You will notice that the IP comes from the TEP Pool that was provided during the apic intitial confirguration. Notice the additional tabs including Nodes Pending Registration . This tab is used to register new devices to the fabric. Double clicking on one of the device rows will display a dialog with device details. Examine each device and note the details that are available including certificate information.","title":"Step 2 - Fabric Membership"},{"location":"01_lab_fabric_discovery/#step-3-pod-view","text":"Navigate to Fabric > Inventory > Pod1 . Expand Pod1 by clicking on the \">\" symbol. Each device in the fabric will be listed. Expand one of the devices and review the sections available. Click on one of the devices (LEAF-1 is showd in the section below). The Summary tab for the device will appear in the Work Pane. Select the General tab in the Work Pane to view additional information about the device. Navigate to Fabric > Inventory > Pod1 > LEAF-1 (Node-102) > Interfaces > Physical Interfaces . Review the interfaces for the devince and note their operational status, Usage and other useful information.","title":"Step 3 - Pod View"},{"location":"01_lab_fabric_discovery/#step-4-cli","text":"Use PUTTY on the Student PC Desktop to connect to the APIC via SSH. Run the acidiag -h command to view the available ACI diagnotics options of the acidiag command. apic1# acidiag -h usage: acidiag [-h] [-v] {avread,fnvread,fnvreadex,rvread,rvreadle,crashsuspecttracker,bootother,bootcurr,journal,logs,oob,scheduler,cleanup,hwcheck,dbgtoken,validateimage,validatenginxconf,version,preservelogs,platform,verifyapic,bond0test,linkflap,touch,run,installer,start,stop,restart,dmestack,dmecore,reboot,drrmode,vapicjoin} ... positional arguments: {avread,fnvread,fnvreadex,rvread,rvreadle,crashsuspecttracker,bootother,bootcurr,journal,logs,oob,scheduler,cleanup,hwcheck,dbgtoken,validateimage,validatenginxconf,version,preservelogs,platform,verifyapic,bond0test,linkflap,touch,run,installer,start,stop,restart,dmestack,dmecore,reboot,drrmode,vapicjoin} sub-command help avread read appliance vector fnvread read fabric node vector fnvreadex read fabric node vector (extended mode) rvread read replica vector rvreadle read replica leader summary crashsuspecttracker read crash suspect tracker state bootother on next boot, boot other Linux Partition, and display updated /etc/grub.conf bootcurr on next boot, boot current Linux Partition, and display updated /etc/grub.conf journal Contents of journal logs logs show log history oob oob options cleanup fs cleanup utility hwcheck Quick check of APIC Hardware dbgtoken show debug token validateimage validate image validatenginxconf validate nginx conf version show ISO version preservelogs stash away logs in preparation for hard reboot platform show platform verifyapic run apic installation verify command bond0test ==SUPPRESS== linkflap flap a link touch touch special files run run specific commands and capture output installer installer start start a service stop stop a service restart restart a service reboot reboot drrmode drrmode options vapicjoin join existing vapic cluster optional arguments: -h, --help show this help message and exit -v, --verbose verbose apic1# Use acidiag fnvread CLI command to view the fabric devices (nodes). apic1# acidiag fnvread ID Pod ID Name Serial Number IP Address Role State LastUpdMsgId -------------------------------------------------------------------------------------------------------------- 101 1 Spine-1 SAL1948TWWP 10.0.72.97/32 spine active 0 102 1 LEAF-1 SAL1948U33K 10.0.72.98/32 leaf active 0 103 1 LEAF-2 SAL1948U35D 10.0.72.96/32 leaf active 0 Total 3 nodes Use the acidiag verifyapic CLI command to view the APIC status. apic1# acidiag verifyapic openssl_check: certificate details subject= CN=FCH1830V38S,serialNumber=PID:APIC-SERVER-M1 SN:FCH1830V38S issuer= CN=Cisco Manufacturing CA,O=Cisco Systems notBefore=Oct 11 08:42:21 2014 GMT notAfter=Oct 11 08:52:21 2024 GMT openssl_check: passed ssh_check: passed all_checks: passed apic1# Verify that you can enter configuration mode. apic1# config apic1(config)# exit apic1# config t apic1(config)# exit apic1# Verify that you can view the configuration using the usual show commands. apic1# sh run # Command: show running-config # Time: Sun May 31 21:56:08 2020 aaa banner 'Application Policy Infrastructure Controller' aaa authentication login console exit aaa authentication login default exit aaa authentication login domain fallback exit bgp-fabric exit coop-fabric exit no password pwd-strength-check crypto aes exit crypto webtoken session-record-flags login,logout,refresh exit rbac security-domain \"all\" exit rbac security-domain \"mgmt\" exit --More-- Because the APIC serves as the controller for the entire fabric, it is often simpler to log on to the APIC and execute show commands across the fabric. In some cases, you may want to log in to a particular leaf or spine. Node ID Management IP 101 192.168.10.101 102 192.168.10.102 103 192.168.10.103 Establish an SSH connection to each device in the table above and execute some common show commands. You can start with the \"show lldp neighbor\" command. Spine-1# show lldp nei Capability codes: (R) Router, (B) Bridge, (T) Telephone, (C) DOCSIS Cable Device (W) WLAN Access Point, (P) Repeater, (S) Station, (O) Other Device ID Local Intf Hold-time Capability Port ID LEAF-1 Eth1/1 120 BR Eth1/49 LEAF-2 Eth1/2 120 BR Eth1/49 Total entries displayed: 2 Spine-1# Spine-1# sh lldp nei sh: lldp: No such file or directory Spine-1# Note that the \"sh lldp nei\" command failed. Remember that many of the common abbreviations for commands are not accepted by ACI.","title":"Step 4 - CLI"},{"location":"01_lab_fabric_discovery/#cli-tips","text":"Use or for command completion Not all command shortcuts are accepted in the ACI CLI. More and more are accepted with every new version of ACI but its a good idea to get into the habit of typing out the full commands.","title":"CLI Tips"},{"location":"01_lab_fabric_discovery/#skills-you-should-have-after-completing-this-lab","text":"After completing this labs you should: - be familiar with the hardware components of the lab - be familiar with the APIC GUI, its high level menu options, and how to navigate through the GUI expanding and selecting options - be able to explore the fabric inventory and determine model, status, and connectivity information - understand where to go to add new devices to the fabric - explore fabric inventory via the CLI","title":"Skills you should have after completing this lab"},{"location":"01_lab_fabric_discovery/#supplemental-information","text":"","title":"Supplemental Information"},{"location":"01_lab_fabric_discovery/#configuring-out-of-band-management","text":"This is not a lab but you can follow along. In Step 4 of Exploring the Fabric Inventory, you established an SSH connection to each leaf and spine and executed the \"show lldp command\". You were able to do this because the managment interfaces had already benn configured. This section details how that is done. Configuring out of band management for the fabric is only done once as part of fabric turn up. This activity effectivley configures an IP address on the device management interface so that the device is reachable via SSH. Because it is an essential step in the turn up of an ACI fabric, instructions are provided here for completeness but please do not perform these actions on your student fabric.","title":"Configuring Out of Band Management"},{"location":"01_lab_fabric_discovery/#tenant-mgmt","text":"Go to the Tenants* menu and select the mgmt** tenant from the subment or from the list in the expanded Work Pane. From the mgmt Tenant Navigation Pane, from the tenant navigation Pane navigate to Tenant mgmt > Node Management Addresses > Static Node Management Addresses Form here you have two options to get to the configuration dialog for a node management address. 1. Right click on the Static Node Management Addresses option in the Navigation Pane 2. Click on the tool icon drop down in the Work Pane. Both of these actions are equivalent and will present you with a dialog to create a static node management address. In ACI, this is the equivalent of assigning a management IP to the management port of the device. In ACI you often use the Node ID to identify the device and that is the case here. You can enter a range, as shown below with a starting IP in the management subnet and generally the IPs will be assigned sequentially. Note: The device hostname, credentials, and other settings are configured on the switch by the APIC when it is first discovered. While the range option is handy, there may be unintended consequences or if you have an IP addressing convention (Node ID mapped to last octet is a good practice) the assignments many not adhere to your convention. Its a good practice to do the IP assignment individually on each node. Note the range from and to values are for a single node. The Lab uses the default OOB (Out of Band) Management EPG, however in a production data center it is a good pracitce to configure and explicit management EPG.","title":"Tenant mgmt"},{"location":"02_lab_fabric_access_policies/","text":"Lab 02 - Configure Fabric & Access Constructs \u00b6 Fabric Configuration \u00b6 Configuration in ACI is based on policies.","title":"Lab 02 - Fabric & Access Policies"},{"location":"02_lab_fabric_access_policies/#lab-02-configure-fabric-access-constructs","text":"","title":"Lab 02 - Configure Fabric &amp; Access Constructs"},{"location":"02_lab_fabric_access_policies/#fabric-configuration","text":"Configuration in ACI is based on policies.","title":"Fabric Configuration"},{"location":"03_lab_tenant_constructs/","text":"Step 3 - Configure Tenants and Tenant Constructs \u00b6","title":"Lab 03 - Tenants"},{"location":"03_lab_tenant_constructs/#step-3-configure-tenants-and-tenant-constructs","text":"","title":"Step 3 - Configure Tenants and Tenant Constructs"},{"location":"04_lab_layer2_connectivity/","text":"Step4 Configure Host Connectivity Constructs (L2) and Layer 2 Links \u00b6","title":"Lab 04 - Host & Layer 2 Connectivity"},{"location":"04_lab_layer2_connectivity/#step4-configure-host-connectivity-constructs-l2-and-layer-2-links","text":"","title":"Step4 Configure Host Connectivity Constructs (L2) and Layer 2 Links"},{"location":"05_lab_vmm_integration/","text":"","title":"Lab 05 - Integration with VMware ESXi"},{"location":"06_lab_layer3_connectivity/","text":"","title":"Lab 06 - External Layer 3 Connectivity"},{"location":"07_lab_operations/","text":"","title":"Lab 07 - Management & Operations"},{"location":"08_lab_fwl_integration/","text":"","title":"Lab 08 - Firewall Integration"},{"location":"09_lab_troubeleshooting/","text":"","title":"Lab 09 - Troubleshooting"},{"location":"10_lab_programmability/","text":"ACI Programmability \u00b6 Visore Postman Arya ACI Toolkit Cobra SDK Python Ansible","title":"Lab 10 - Programmability"},{"location":"10_lab_programmability/#aci-programmability","text":"Visore Postman Arya ACI Toolkit Cobra SDK Python Ansible","title":"ACI Programmability"},{"location":"11_lab_appcenter/","text":"","title":"Lab 11 - App Center and ACI Optimize Feature"},{"location":"12_lab_techsupport/","text":"Export Tech Support \u00b6 While ACI is now a mature technology, there are occaisions when you will need to contact Cisco TAC for support. Its always a good idea to have your current configuration documented and ACI makes that very simple. In Lab 07, we configured a remote location for daily backups and off-fabric snapshots. We will use thei same remote FTP server to export what is the equivalent of show tech.","title":"Lab 12 - Export Tech Support"},{"location":"12_lab_techsupport/#export-tech-support","text":"While ACI is now a mature technology, there are occaisions when you will need to contact Cisco TAC for support. Its always a good idea to have your current configuration documented and ACI makes that very simple. In Lab 07, we configured a remote location for daily backups and off-fabric snapshots. We will use thei same remote FTP server to export what is the equivalent of show tech.","title":"Export Tech Support"},{"location":"lab_access/","text":"Getting Around the Lab \u00b6 How this lab works and things to keep in mind... Lab Access \u00b6 Each student is assigned their own lab \"Pod\" (Tenant) in the fabric. Please refer to the Student Pod Table below for your Remote Desktop (RDP) IP Address and Credentials. Access to the lab environment is via the Remote Desktop Protocol (RDP) and so you will need an RDP client on your system in order to access the lab. Student Pod Table \u00b6 POD Number RDP IP RDP Port Username Password 11 65.49.10.72 7779 Claudia 1234QWer! >1< Login with your credentials from the Student Pod Table Operating System Details Windows 10 From the Search box Type run and <Enter> In the Run dialog type mstsc /admin <Enter> Macintosh Microsoft Remote Desktop client on the Mac App Store See Microsoft Get started with the macOS client >2< Once you sucesfully log in (Tip: Don't forget the RDP port number) you will see a customized Desktop which will have all the tools you need and from where you can access all the devices. >3< This lab guide is also available on the Desktop. IMPORTANT: Please review before continuing with the lab. \u00b6 LAB PREMISE \u00b6 This Lab has a total of 16 Pods. Some equipment is shared across all the Pods while other equipment is dedicated to each individual Pod. A Pod is a group of devices and resources which make an individual Lab usable for each student. Each Pod is identified by a two digit number. Throghout your lab guide, if you see ##, replace ## with your Pod number. ACI is fundamentally a multi-tenant environment and the lab environment makes full use of that capability. Each Student will create their own Tenant in their Pod and map all the policies, test servers, and equipment dedicated to the Pod to their own tenant. For example: The student assigned to POD11 will create their own POD11 tenant and configure access policies and virtual networking policies to the POD11 tenant. A naming standard is particularly important in an ACI Design. All policies will follow a naming standard based on your POD number. Common Equipment \u00b6 The table below details the shared equipment in the lab. This equipment will be shared by all students. Because these resources are shared, you will see configuration appear that is not your own. Please do not delete or change any configuration item that is not your own. Please be respectful of the other students and use only your POD resources. Table of Common (Shared) Devices and Access Information Device Management/Terminal Server IP Telnet Port Number Credentials APIC (apic.dc.local) 192.168.10.1 admin/1234QWer spine 192.168.250.202 7006 admin/1234QWer leaf-1 192.168.250.202 7007 admin/1234QWer leaf-2 192.168.250.202 7008 admin/1234QWer L2/L3 Switch 192.168.250.202 7009 admin/1234QWer AD/DNS/FTP Server 192.168.10.40 admin/1234QWer NTP Server 192.168.10.40 Dedicated Equipment \u00b6 This equipment is dedicated to every individual student. Each Student Pod has a dedicated ESXi Host, vCenter and a set of Linux based Virtual Machines. POD11 Equipment Device Management IP Fabric IP/ FQDN Username Password Linux VMs WEB 10.0.1.1/24 Root 1234Qwer APP 10.0.2.1/24 Roo 1234Qwer DB 10.0.3.1/24 Root 1234Qwer TRANSACT 10.0.4.1/24 Root 1234Qwer Virtualization Environment ESXi Host 192.168.10.211 ESXp11@dc.local Root 1234QWer vCenter 192.168.10.212 vcenterpod16.dc.local administrator@vsphere.local 1234QWer! Credentials Summary \u00b6 For ease of use, the Lab has minimized the number of credentials and uses a standard pattern for the password with minor variations. The table below summarizes all the credentilas you will need to access all the lab resources. Device Type Username/Password Network Devices (Including APIC) admin/1234QWer Linux Virtual Machines Root/1234Qwer ESXi Host Root/1234QWer vCenter administrator@vsphere.local/1234QWer! Physical Interface Reference \u00b6 ACI is fundamentally a networking technology and so throughout the labs you will need to configure interfaces. Use the Physical Interface Table below as a reference. SPINE-1 \u00b6 POD# PORT Connected To Device Connected to Device Port ALL PODS e1/1 leaf-1 e1/49 ALL PODS e1/2 leaf-2 e1/49 LEAF-1 \u00b6 POD# PORT Connected To Device Connected to Device Port ALL PODS e1/2 APIC VIC1 POD11 e1/3 UCS-SERVER-P11 VIC1 POD11 e1/11 ACI-P2-TOR (Cat 3750) Gi1/0/1 ALL PODS e1/49 SPINE-1 e1/1 LEAF-2 \u00b6 POD# PORT Connected To Device Connected to Device Port ALL PODS e1/2 APIC VIC2 POD11 e1/3 UCS-SERVER-P11 VIC2 POD11 e1/11 ACI-P2-TOR (Cat 3750) Gi1/0/2 ALL PODS e1/49 SPINE-1 e1/2 Layer 2 and Layer 3 Logical Configuration \u00b6 L3Out IP and Vlan Details \u00b6 POD Number OSPF AREA LEAF-1 Interface OSPF VLAN SVI on Layer 3 Switch SVI on APIC VLAN Pool Start VLAN Pool End POD11 11 e1/11 1112 172.16.11.2/30 172.16.11.1/30 1110 1119 Layer 2 Details \u00b6 POD Number SVI on External Layer 3 Device LEAF-2 Interface Layer 2 VLANS VLAN Pool Start VLAN Pool End POD11 10.0.2.99 e1/11 112 110 119 ASA Management Details \u00b6 POD Number ASA Management IP POD11 192.168.10.71 Lets get started with ACI! \u00b6","title":"Getting Around"},{"location":"lab_access/#getting-around-the-lab","text":"How this lab works and things to keep in mind...","title":"Getting Around the Lab"},{"location":"lab_access/#lab-access","text":"Each student is assigned their own lab \"Pod\" (Tenant) in the fabric. Please refer to the Student Pod Table below for your Remote Desktop (RDP) IP Address and Credentials. Access to the lab environment is via the Remote Desktop Protocol (RDP) and so you will need an RDP client on your system in order to access the lab.","title":"Lab Access"},{"location":"lab_access/#student-pod-table","text":"POD Number RDP IP RDP Port Username Password 11 65.49.10.72 7779 Claudia 1234QWer! >1< Login with your credentials from the Student Pod Table Operating System Details Windows 10 From the Search box Type run and <Enter> In the Run dialog type mstsc /admin <Enter> Macintosh Microsoft Remote Desktop client on the Mac App Store See Microsoft Get started with the macOS client >2< Once you sucesfully log in (Tip: Don't forget the RDP port number) you will see a customized Desktop which will have all the tools you need and from where you can access all the devices. >3< This lab guide is also available on the Desktop.","title":"Student Pod Table"},{"location":"lab_access/#important-please-review-before-continuing-with-the-lab","text":"","title":"IMPORTANT:  Please review before continuing with the lab."},{"location":"lab_access/#lab-premise","text":"This Lab has a total of 16 Pods. Some equipment is shared across all the Pods while other equipment is dedicated to each individual Pod. A Pod is a group of devices and resources which make an individual Lab usable for each student. Each Pod is identified by a two digit number. Throghout your lab guide, if you see ##, replace ## with your Pod number. ACI is fundamentally a multi-tenant environment and the lab environment makes full use of that capability. Each Student will create their own Tenant in their Pod and map all the policies, test servers, and equipment dedicated to the Pod to their own tenant. For example: The student assigned to POD11 will create their own POD11 tenant and configure access policies and virtual networking policies to the POD11 tenant. A naming standard is particularly important in an ACI Design. All policies will follow a naming standard based on your POD number.","title":"LAB PREMISE"},{"location":"lab_access/#common-equipment","text":"The table below details the shared equipment in the lab. This equipment will be shared by all students. Because these resources are shared, you will see configuration appear that is not your own. Please do not delete or change any configuration item that is not your own. Please be respectful of the other students and use only your POD resources. Table of Common (Shared) Devices and Access Information Device Management/Terminal Server IP Telnet Port Number Credentials APIC (apic.dc.local) 192.168.10.1 admin/1234QWer spine 192.168.250.202 7006 admin/1234QWer leaf-1 192.168.250.202 7007 admin/1234QWer leaf-2 192.168.250.202 7008 admin/1234QWer L2/L3 Switch 192.168.250.202 7009 admin/1234QWer AD/DNS/FTP Server 192.168.10.40 admin/1234QWer NTP Server 192.168.10.40","title":"Common Equipment"},{"location":"lab_access/#dedicated-equipment","text":"This equipment is dedicated to every individual student. Each Student Pod has a dedicated ESXi Host, vCenter and a set of Linux based Virtual Machines. POD11 Equipment Device Management IP Fabric IP/ FQDN Username Password Linux VMs WEB 10.0.1.1/24 Root 1234Qwer APP 10.0.2.1/24 Roo 1234Qwer DB 10.0.3.1/24 Root 1234Qwer TRANSACT 10.0.4.1/24 Root 1234Qwer Virtualization Environment ESXi Host 192.168.10.211 ESXp11@dc.local Root 1234QWer vCenter 192.168.10.212 vcenterpod16.dc.local administrator@vsphere.local 1234QWer!","title":"Dedicated Equipment"},{"location":"lab_access/#credentials-summary","text":"For ease of use, the Lab has minimized the number of credentials and uses a standard pattern for the password with minor variations. The table below summarizes all the credentilas you will need to access all the lab resources. Device Type Username/Password Network Devices (Including APIC) admin/1234QWer Linux Virtual Machines Root/1234Qwer ESXi Host Root/1234QWer vCenter administrator@vsphere.local/1234QWer!","title":"Credentials Summary"},{"location":"lab_access/#physical-interface-reference","text":"ACI is fundamentally a networking technology and so throughout the labs you will need to configure interfaces. Use the Physical Interface Table below as a reference.","title":"Physical Interface Reference"},{"location":"lab_access/#spine-1","text":"POD# PORT Connected To Device Connected to Device Port ALL PODS e1/1 leaf-1 e1/49 ALL PODS e1/2 leaf-2 e1/49","title":"SPINE-1"},{"location":"lab_access/#leaf-1","text":"POD# PORT Connected To Device Connected to Device Port ALL PODS e1/2 APIC VIC1 POD11 e1/3 UCS-SERVER-P11 VIC1 POD11 e1/11 ACI-P2-TOR (Cat 3750) Gi1/0/1 ALL PODS e1/49 SPINE-1 e1/1","title":"LEAF-1"},{"location":"lab_access/#leaf-2","text":"POD# PORT Connected To Device Connected to Device Port ALL PODS e1/2 APIC VIC2 POD11 e1/3 UCS-SERVER-P11 VIC2 POD11 e1/11 ACI-P2-TOR (Cat 3750) Gi1/0/2 ALL PODS e1/49 SPINE-1 e1/2","title":"LEAF-2"},{"location":"lab_access/#layer-2-and-layer-3-logical-configuration","text":"","title":"Layer 2 and Layer 3 Logical Configuration"},{"location":"lab_access/#l3out-ip-and-vlan-details","text":"POD Number OSPF AREA LEAF-1 Interface OSPF VLAN SVI on Layer 3 Switch SVI on APIC VLAN Pool Start VLAN Pool End POD11 11 e1/11 1112 172.16.11.2/30 172.16.11.1/30 1110 1119","title":"L3Out IP and Vlan Details"},{"location":"lab_access/#layer-2-details","text":"POD Number SVI on External Layer 3 Device LEAF-2 Interface Layer 2 VLANS VLAN Pool Start VLAN Pool End POD11 10.0.2.99 e1/11 112 110 119","title":"Layer 2 Details"},{"location":"lab_access/#asa-management-details","text":"POD Number ASA Management IP POD11 192.168.10.71","title":"ASA Management Details"},{"location":"lab_access/#lets-get-started-with-aci","text":"","title":"Lets get started with ACI!"},{"location":"topology/","text":"Lab Topology \u00b6 The lab topology is designed to allow configuration of the most common scenarios. Once the Fabric has been discovered and configured and the Tenant design applied, the following functionality can be configured: Layer 3 Routing Layer 2 Connectivity to a Legacy Network Layer 2 Virtual Port Channel","title":"Topology & Design"},{"location":"topology/#lab-topology","text":"The lab topology is designed to allow configuration of the most common scenarios. Once the Fabric has been discovered and configured and the Tenant design applied, the following functionality can be configured: Layer 3 Routing Layer 2 Connectivity to a Legacy Network Layer 2 Virtual Port Channel","title":"Lab Topology"}]}